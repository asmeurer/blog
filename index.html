<!DOCTYPE html>
<html prefix="        og: http://ogp.me/ns# article: http://ogp.me/ns/article#     " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="My blog">
<meta name="viewport" content="width=device-width">
<title>Aaron Meurer's Blog</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="https://asmeurer.com/blog/">
<link rel="next" href="index-4.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link rel="prefetch" href="posts/verifying-the-riemann-hypothesis-with-sympy-and-mpmath/" type="text/html">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
        
    <header id="header"><h1 id="brand"><a href="https://asmeurer.com/blog/" title="Aaron Meurer's Blog" rel="home">

        <span id="blog-title">Aaron Meurer's Blog</span>
    </a></h1>

        

        
    <nav id="menu"><ul>
<li class="active"><a href=".">Blog<span class="sr-only"> (active)</span></a></li>
                <li><a href="about">About</a></li>
                <li><a href="work">Work</a></li>
                <li><a href="archive.html">Archives</a></li>
                <li><a href="rss.xml">RSS</a></li>

    

    
    
    </ul></nav></header><main id="content"><div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/verifying-the-riemann-hypothesis-with-sympy-and-mpmath/" class="u-url">Verifying the Riemann Hypothesis with SymPy and mpmath</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Aaron Meurer
            </span></p>
            <p class="dateline">
            <a href="posts/verifying-the-riemann-hypothesis-with-sympy-and-mpmath/" rel="bookmark">
            <time class="published dt-published" datetime="2020-03-31T16:12:54-05:00" itemprop="datePublished" title="2020-03-31 16:12">2020-03-31 16:12</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/verifying-the-riemann-hypothesis-with-sympy-and-mpmath/#disqus_thread" data-disqus-identifier="cache/posts/verifying-the-riemann-hypothesis-with-sympy-and-mpmath.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p>Like most people, I've had a lot of free time recently, and I've spent some of
it watching various YouTube videos about the <a href="https://en.wikipedia.org/wiki/Riemann_hypothesis">Riemann
Hypothesis</a>. I've collected
the videos I've watched into <a href="https://www.youtube.com/playlist?list=PLrFrByaoJbcqKjzgJvLs2-spSmzP7jolT">YouTube
playlist</a>.
The playlist is sorted with the most mathematically approachable videos first,
so even if you haven't studied complex analysis before, you can watch the
first few. If you have studied complex analysis, all the videos will be within
your reach (none of them are highly technical with proofs). Each video
contains parts that aren't in any of the other videos, so you will get
something out of watching each of them.</p>
<p>The <a href="https://www.youtube.com/watch?v=lyf9W2PWm40&amp;list=PLrFrByaoJbcqKjzgJvLs2-spSmzP7jolT&amp;index=8">last video in the
playlist</a>
is a lecture by Keith Conrad. In it, he mentioned a method by which one could
go about verifying the Riemann Hypothesis with a computer. I wanted to see if
I could do this with SymPy and mpmath. It turns out you can.</p>
<h2>Background Mathematics</h2>
<h3>Euler's Product Formula</h3>
<p>Before we get to the computations, let's go over some mathematical background.
As you may know, the Riemann Hypothesis is one of the 7 <a href="https://en.wikipedia.org/wiki/Millennium_Prize_Problems">Millennium Prize
Problems</a> outlined by
the Clay Mathematics Institute in 2000. The problems have gained some fame
because each problem comes with a $1,000,000 prize if solved. One problem, the
<a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_conjecture">Poincar√© conjecture</a>,
has already been solved (Grigori Perelman who solved it turned down the 1
million dollar prize). The remainder remain unsolved.</p>
<p>The Riemann Hypothesis is one of the most famous of these problems. The reason
for this is that the problem is central many open questions in number theory.
There are hundreds of theorems which are only known to be true contingent on
the Riemann Hypothesis, meaning that if the Riemann Hypothesis were proven,
immediately hundreds of theorems would be proven as well. Also, unlike some
other Millennium Prize problems, like P=NP, the Riemann Hypothesis is almost
universally believed to be true by mathematicians. So it's not a question of
whether or not it is true, just one of how to actually prove it. The problem
has been open for over 160 years, and while many advances have been made, no
one has yet come up with a proof of it (crackpot proofs aside).</p>
<p>To understand the statement of the hypothesis, we must first define the zeta
function. Let</p>
<p>$$\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}$$</p>
<p>(that squiggle $\zeta$ is the lowercase Greek letter zeta). This expression
makes sense if $s$ is an integer greater than or equal to 2, $s=2, 3, 4, \ldots$,
since we know from simple arguments from calculus that the summation converges
in those cases (it isn't important for us what those values are, only that the
summation converges). The story begins with Euler, who in 1740 considered the
following infinite product:</p>
<p>$$\prod_{\text{$p$ prime}}\frac{1}{1 -
\frac{1}{p^s}}.$$</p>
<p>The product ranges over all prime numbers, i.e., it is
$$\left(\frac{1}{1 - \frac{1}{2^s}}\right)\cdot\left(\frac{1}{1 -
\frac{1}{3^s}}\right)\cdot\left(\frac{1}{1 - \frac{1}{5^s}}\right)\cdots.$$
The fraction $\frac{1}{1 - \frac{1}{p}}$ may seem odd at first, but consider
the famous geometric series formula, $$\sum_{k=0}^\infty r^k = \frac{1}{1 -
r},$$ which is true for $|r| &lt; 1$. Our fraction is exactly of this form, with
$r = \frac{1}{p^s}$. So substituting, we have</p>
<p>$$\prod_{\text{$p$ prime}}\frac{1}{1 - \frac{1}{p^s}} =
\prod_{\text{$p$ prime}}\sum_{k=0}^\infty \left(\frac{1}{p^s}\right)^k =
\prod_{\text{$p$ prime}}\sum_{k=0}^\infty \left(\frac{1}{p^k}\right)^s.$$</p>
<p>Let's take a closer look at what this is. It is</p>
<p>$$\left(\frac{1}{p_1^s} + \frac{1}{p_1^{2s}} + \frac{1}{p_1^{3s}} +
\cdots\right)\cdot\left(\frac{1}{p_2^s} + \frac{1}{p_2^{2s}} +
\frac{1}{p_2^{3s}} + \cdots\right)\cdot\left(\frac{1}{p_3^s} + \frac{1}{p_3^{2s}} +
\frac{1}{p_3^{3s}} + \cdots\right)\cdots,$$</p>
<p>where $p_1$ is the first prime, $p_2$ is the second prime, and so on. Now
think about how to expand finite products of finite sums, for instance,
$$(x_1 + x_2 + x_3)(y_1 + y_2 + y_3)(z_1 + z_2 + z_3).$$ To expand the above,
you would take a sum of every combination where you pick one $x$ term, one $y$
term, and one $z$ term, giving</p>
<p>$$x_1y_1z_1 + x_1y_1z_2 + \cdots + x_2y_1z_3 + \cdots + x_3y_2z_1 + \cdots + x_3y_3z_3.$$</p>
<p>So to expand the infinite product, we do the same thing. We take every
combination of picking $1/p_i^{ks}$, with one $k$ for each $i$. If we pick
infinitely many non-$1$ powers, the product will be zero, so we only need to
consider terms where there are finitely many primes. The resulting sum will be
something like</p>
<p>$$\frac{1}{1^s} + \frac{1}{p_1^s} + \frac{1}{p_2^s} + \frac{1}{\left(p_1^2\right)^s} +
\frac{1}{p_3^s} + \frac{1}{\left(p_1p_2\right)^s} + \cdots,$$</p>
<p>where each prime power combination is picked exactly once. However, we know by
the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic">Fundamental Theorem of
Arithmetic</a>
that when you take all combinations of products primes that you get each
positive integer exactly once. So the above sum is just</p>
<p>$$\frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots,$$ which is just
$\zeta(s)$ as we defined it above.</p>
<p>In other words,</p>
<p>$$\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \prod_{\text{$p$
prime}}\frac{1}{1 - \frac{1}{p^s}},$$ for $s = 2, 3, 4, \ldots$. This is known
as Euler's product formula for the zeta function. Euler's product formula
gives us our first clue as to why the zeta function can give us insights into
prime numbers.</p>
<h3>Analytic Continuation</h3>
<p>In 1859, Bernhard Riemann wrote a <a href="https://en.wikipedia.org/wiki/On_the_Number_of_Primes_Less_Than_a_Given_Magnitude">short 9 page paper on number theory and the
zeta
function</a>.
It was the only paper Riemann ever wrote on the subject of number theory, but
it is undoubtedly one of the most important papers every written on the
subject.</p>
<p>In the paper, Riemann considered that the zeta function summation,</p>
<p>$$\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s},$$</p>
<p>makes sense not just for integers $s = 2, 3, 4, \ldots$, but for any real
number $s &gt; 1$ (if $s = 1$, the summation is the <a href="https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)">harmonic
series</a>, which
famously diverges). In fact, it is not hard to see that for complex $s$, the
summation makes sense so long as $\mathrm{Re}(s) &gt; 1$ (for more about what it
even means for $s$ to be complex in that formula, and the basic ideas of
analytic continuation, I recommend <a href="https://www.youtube.com/watch?v=sD0NjbwqlYw&amp;list=PLrFrByaoJbcqKjzgJvLs2-spSmzP7jolT&amp;index=3">3Blue1Brown's
video</a>
from my YouTube playlist).</p>
<p>Riemann wanted to extend this function to the entire complex plane, not just
$\mathrm{Re}(s) &gt; 1$. The process of doing this is called <a href="https://en.wikipedia.org/wiki/Analytic_continuation">analytic
continuation</a>. The theory
of complex analysis tells us that if we can find an extension of $\zeta(s)$ to
the whole complex plan that remains differentiable, then that extension is
unique, and we can reasonably say that that <em>is</em> the definition of the
function everywhere.</p>
<p>Riemann used the following approach. Consider what we might call the
"completed zeta function"</p>
<p>$$Z(s) = \pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s).$$</p>
<p>Using Fourier analysis, Riemann gave a formula for $Z(s)$ that is defined
everywhere, allowing us to use it to define $\zeta(s)$ to the left of 1. I
won't repeat Riemann's formula for $Z(s)$ as the exact formula isn't
important, but from it one could also see</p>
<ol>
<li>
<p>$Z(s)$ is defined everywhere in the complex plane, except for simple poles at 0
and 1.</p>
</li>
<li>
<p>$Z(s) = Z(1 - s).$ This means if we have a value for $s$ that is right of
the line $\mathrm{Re}(z) = \frac{1}{2},$ we can get a value to the left of
it by reflecting it over the real-axis and the line at $\frac{1}{2}$ (to
see this, note that the average of $s$ and $1 - s$ is $1/2$, so the
midpoint of a line connecting the two should always go through the point
$1/2$).</p>
</li>
</ol>
<img src="s-and-1-s.svg" alt="Reflection of s and 1 - s" width="608"><p>(Reflection of $s$ and $1 - s$. Created with
<a href="https://www.geogebra.org/graphing/c9rzy9hj">Geogebra</a>)</p>
<h3>Zeros</h3>
<p>Looking at $Z(s)$, it is a product of three parts. So the zeros and poles of
$Z(s)$ correspond to the zeros and poles of these parts, unless they cancel.
$\pi^{-\frac{s}{2}}$ is the easiest: it has no zeros and no poles. The second
part is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
$\Gamma(z)$ has no zeros and has simple poles at nonpositive integers $z=0,
-1, -2, \ldots$.</p>
<p>So taking this, along with the fact that $Z(s)$ is entire except for simple
poles at 0 and 1, we get from $$\zeta(s) =
\frac{Z(s)}{\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)}$$</p>
<ol>
<li>$Z(s)$ has a simple pole at 1, which means that $\zeta(s)$ does as well.
This is not surprising, since we already know the summation formula from
above diverges as $s$ approaches 1.</li>
<li>$Z(s)$ has a simple pole at 0. Since $\Gamma\left(\frac{s}{2}\right)$ also
has a simple pole at 0, they must cancel and $\zeta(s)$ must have neither a
zero nor a pole at 0 (in fact, $\zeta(0) = -1/2$).</li>
<li>Since $\Gamma\left(\frac{s}{2}\right)$ has no zeros, there are no further
poles of $\zeta(s)$. Thus, $\zeta(s)$ is entire everywhere except for a
simple pole at $s=1$.</li>
<li>$\Gamma\left(\frac{s}{2}\right)$ has poles at the remaining negative even
integers. Since $Z(s)$ has no poles there, these must correspond to zeros
of $\zeta(s)$. These are the so-called "trivial" zeros of the zeta
function, at $s=-2, -4, -6, \ldots$. The term "trivial" here is a relative
one. They are trivial to see from the above formula, whereas other zeros of
$\zeta(s)$ are much harder to find.</li>
<li>$\zeta(s) \neq 0$ if $\mathrm{Re}(s) &gt; 1$. One way to see this is from the
Euler product formula. Since each term in the product is not zero, the
function itself cannot be zero (this is a bit hand-wavy, but it can be made
rigorous). This implies that $Z(s) \neq 0$ in this region as well. We can
reflect $\mathrm{Re}(s) &gt; 1$ over the line at $\frac{1}{2}$ by considering
$\zeta(1 - s)$. Using the above formula and the fact that $Z(s) = Z(1 -
s)$, we see that $\zeta(s)$ cannot be zero for $\mathrm{Re}(s) &lt; 0$ either,
with the exception of the aforementioned trivial zeros at $s=-2, -4, -6,
\ldots$.</li>
</ol>
<p>Thus, any non-trivial zeros of $\zeta(s)$ must have real part between 0 and 1.
This is the so-called "critical strip". Riemann hypothesized that these zeros
are not only between 0 and 1, but are in fact on the line dividing the strip
at real part equal to $1/2$. This line is called the "critical line". This is
Riemann's famous hypothesis: that all the non-trivial zeros of $\zeta(s)$ have
real part equal to $1/2$.</p>
<h3>Computational Verification</h3>
<p>Whenever you have a mathematical hypothesis, it is good to check if it is true
numerically. Riemann himself used some methods (not the same ones we use here)
to numerically estimate the first few non-trivial zeros of $\zeta(s)$, and
found that they lied on the critical line, hence the motivation for his
hypotehsis. Here is an <a href="https://www.maths.tcd.ie/pub/HistMath/People/Riemann/Zeta/EZeta.pdf">English
translation</a>
of his original paper if you are interested.</p>
<p>If we verified that all the zeros in the critical strip from, say,
$\mathrm{Im}(s) = 0$ to $\mathrm{Im}(s) = N$ are in fact on the critical line
for some large $N$, then it would give evidence that the Riemann Hypothesis is
true. However, to be sure, this would not constitute a proof.
<a href="https://en.wikipedia.org/wiki/G._H._Hardy">Hardy</a> showed in 1914 that
$\zeta(s)$ has infinitely many zeros on the critical strip, so only finding
finitely many of them would not suffice as a proof. (Although if we were to
find a counter-example, a zero <em>not</em> on the critical line, that WOULD
constitute a proof that the Hypothesis is false. However, there are strong
reasons to believe that the hypothesis is not false, so this would be unlikely
to happen.)</p>
<p>How would we verify that the zeros are all on the line $1/2$. We can find
zeros of $\zeta(s)$ numerically, but how would we know if the real part is
really exactly 0.5 and not 0.500000000000000000000000000000000001? And more
importantly, just because we find some zeros, it doesn't mean that we have all
of them. Maybe we can find a bunch of zeros on the critical line, but how
would we be sure that there aren't other zeros lurking around elsewhere on the
critical strip?</p>
<p>We want to find rigorous answers to these two questions:</p>
<ol>
<li>
<p>How can we count the number of zeros between $\mathrm{Im}(s) = 0$ and
$\mathrm{Im}(s) = N$ of $\zeta(s)$?</p>
</li>
<li>
<p>How can we verify that all those zeros lie on the critical line, that is,
they have real part equal to exactly $1/2$?</p>
</li>
</ol>
<h4>Counting Zeros Part 1</h4>
<p>To answer the first question, we can make use of a powerful theorem from
complex analysis called the <a href="https://en.wikipedia.org/wiki/Argument_principle#Generalized_argument_principle">argument
principle</a>.
The argument principle says that if $f$ is a meromorphic function on some
closed contour $C$, and does not have any zeros or poles on $C$ itself, then</p>
<p>$$\frac{1}{2\pi i}\oint_C \frac{f'(z)}{f(z)}\,ds = \#\left\{\text{zeros of $f$
inside of C}\right\} - \#\left\{\text{poles of $f$
inside of C}\right\},$$ where all zeros and poles are counted with
multiplicity.</p>
<p>In other words, the integral on the left-hand side counts the number of zeros
of $f$ minus the number of poles of $f$ in a region. The argument principle is
quite easy to show given the Cauchy residue theorem (see the above linked
Wikipedia article for a proof). The expression $f'(z)/f(z)$ is called the
"<a href="https://en.wikipedia.org/wiki/Logarithmic_derivative">logarithmic
derivative</a> of $f$",
because it equals $\frac{d}{dz} \log(f(z))$ (although it makes sense even without
defining what "$\log$" means).</p>
<p>One should take a moment to appreciate the beauty of this result. The
left-hand side is an integral, something we generally think of as being a
continuous quantity. But it is always exactly equal to an integer. Results
such as these give us a further glimpse at how analytic functions and complex
analysis can produce theorems about number theory, a field which one would
naively think can only be studied via discrete means. In fact, these methods
are far more powerful than discrete methods. For many results in number
theory, we only know how to prove them using complex analytic means. So-called
<a href="https://en.wikipedia.org/wiki/Elementary_proof">"elementary" proofs</a> for
these results, or proofs that only use discrete methods and do not use complex
analysis, have not yet been found.</p>
<p>Practically speaking, the fact that the above integral is exactly an integer
means that if we compute it numerically and it comes out to something like
0.9999999, we know that it must in fact equal exactly 1. So as long as we get
a result that is near an integer, we can round it to the exact answer.</p>
<p>We can integrate a contour along the critical strip up to some $\mathrm{Im}(s)
= N$ to count the number of zeros up to $N$ (we have to make sure to account
for the poles. I go into more details about this when I actually compute the
integral below).</p>
<h4>Counting Zeros Part 2</h4>
<p>So using the argument principle, we can count the number of zeros in a region.
Now how can we verify that they all lie on the critical line? The answer lies
in the $Z(s)$ function defined above. By the points outlined in the previous
section, we can see that $Z(s)$ is zero exactly where $\zeta(s)$ is zero on
the critical strip, and it is not zero anywhere else. In other words,</p>
<div style="text-align:center"> <b>the zeros of $Z(s)$ are exactly the non-trivial zeros of $\zeta(s)$.</b>
</div>
<p>This helps us because $Z(s)$ has a nice property on the critical line. First
we note that $Z(s)$ commutes with conjugation, that is $\overline{Z(s)} =
Z(\overline{s})$ (this isn't obvious from what I have shown, but it is true).
On the critical line $\frac{1}{2} + it$, we have</p>
<p>$$\overline{Z\left(\frac{1}{2} + it\right)} = Z\left(\overline{\frac{1}{2} +
it}\right) = Z\left(\frac{1}{2} - it\right).$$</p>
<p>However, $Z(s) = Z(1 - s)$, and $1 - \left(\frac{1}{2} - it\right) =
\frac{1}{2} + it$, so</p>
<p>$$\overline{Z\left(\frac{1}{2} + it\right)} = Z\left(\frac{1}{2} +
it\right),$$</p>
<p>which means that $Z\left(\frac{1}{2} + it\right)$ is real valued for real $t$.</p>
<p>This simplifies things a lot, because it is much easier to find zeros of a real
function. In fact, we don't even care about finding the zeros, only counting
them. Since $Z(s)$ is continuous, we can use a simple method: counting sign
changes. If a continuous real function changes signs from negative to positive or from
positive to negative n times in an interval, then it must have at least n
zeros in that interval. It may have more, for instance, if some zeros are
clustered close together, or if a zero has a multiplicity greater than 1, but
we know that there must be at least n.</p>
<p>So our approach to verifying the Riemann Hypothesis is as such:</p>
<ol>
<li>
<p>Integrate $\frac{1}{2\pi i}\oint_C Z'(s)/Z(s)\,ds$ along a contour $C$
that runs along the critical strip up to some $\mathrm{Im}(s) = N$. The
integral will tell us there are exactly $n$ zeros in the contour, counting
multiplicity.</p>
</li>
<li>
<p>Try to find $n$ sign changes of $Z(1/2 + it)$ for $t\in [0, N]$. If we can
find $n$ of them, we are done. We have confirmed all the zeros are on the
critical line.</p>
</li>
</ol>
<p>Step 2 would fail if the Riemann Hypothesis is false, in which case a zero
wouldn't be on the critical line. But it would also fail if a zero has a
multiplicity greater than 1, since the integral would count it more times than
the sign changes. Fortunately, as it turns out, the Riemann Hypothesis has
been verified up to N = 10000000000000, and no one has yet found a zero of the
zeta function yet that has a multiplicity greater than 1, so we should not
expect that to happen (no one has yet found a counterexample to the Riemann
Hypothesis either).</p>
<h2>Verification with SymPy and mpmath</h2>
<p>We now use SymPy and mpmath to compute the above quantities. We use
<a href="https://www.sympy.org/">SymPy</a> to do symbolic manipulation for us, but the
heavy work is done by <a href="http://mpmath.org/doc/current/index.html">mpmath</a>.
mpmath is a pure Python library for arbitrary precision numerics. It is used
by SymPy under the hood, but it will be easier for us to use it directly. It
can do, among other things, numeric integration. When I first tried to do
this, I tried using the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.zeta.html"><code>scipy.special</code> zeta
function</a>,
but unfortunately, it does not support complex arguments.</p>
<p>First we do some basic imports</p>
<pre><code class="language-py">&gt;&gt;&gt; from sympy import *
&gt;&gt;&gt; import mpmath
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; s = symbols('s')
</code></pre>
<p>Define the completed zeta function $Z = \pi^{-s/2}\Gamma(s/2)\zeta(s)$.</p>
<pre><code>&gt;&gt;&gt; Z = pi**(-s/2)*gamma(s/2)*zeta(s)
</code></pre>
<p>We can verify that Z is indeed real for $\frac{1}{2} + it.$</p>
<pre><code class="language-py">&gt;&gt;&gt; Z.subs(s, 1/2 + 0.5j).evalf()
-1.97702795164031 + 5.49690501450151e-17*I
</code></pre>
<p>We get a small imaginary part due to the way floating point arithmetic works.
Since it is below <code>1e-15</code>, we can safely ignore it.</p>
<p><code>D</code> will be the logarithmic derivative of <code>Z</code>.</p>
<pre><code class="language-py">&gt;&gt;&gt; D = simplify(Z.diff(s)/Z)
&gt;&gt;&gt; D
polygamma(0, s/2)/2 - log(pi)/2 + Derivative(zeta(s), s)/zeta(s)
</code></pre>
<p>This is $$\frac{\operatorname{polygamma}{\left(0,\frac{s}{2} \right)}}{2} -
\frac{\log{\left(\pi \right)}}{2} + \frac{
\zeta'\left(s\right)}{\zeta\left(s\right)}.$$</p>
<p>Note that logarithmic derivatives behave similar to logarithms. The
logarithmic derivative of a product is the sum of logarithmic derivatives (the
$\operatorname{polygamma}$ function is the derivative of $\Gamma$).</p>
<p>We now use
<a href="https://docs.sympy.org/latest/modules/utilities/lambdify.html#sympy.utilities.lambdify.lambdify"><code>lambdify</code></a>
to convert the SymPy expressions <code>Z</code> and <code>D</code> into functions that are evaluated
using mpmath. A technical difficulty here is that the derivative of the zeta
function $\zeta'(s)$ does not have a closed-form expression. <a href="http://mpmath.org/doc/current/functions/zeta.html?highlight=zeta#mpmath.zeta">mpmath's <code>zeta</code>
can evaluate
$\zeta'$</a>
but it doesn't yet work with <code>sympy.lambdify</code> (see <a href="https://github.com/sympy/sympy/issues/11802">SymPy issue
11802</a>). So we have to manually
define <code>"Derivative"</code> in lambdify, knowing that it will be the derivative of
<code>zeta</code> when it is called. Beware that this is only correct for this specific
expression where we know that <code>Derivative</code> will be <code>Derivative(zeta(s), s)</code>.</p>
<pre><code class="language-py">&gt;&gt;&gt; Z_func = lambdify(s, Z, 'mpmath')
&gt;&gt;&gt; D_func = lambdify(s, D, modules=['mpmath',
...     {'Derivative': lambda expr, z: mpmath.zeta(z, derivative=1)}])
</code></pre>
<p>Now define a function to use the argument principle to count the number of
zeros up to $Ni$. Due to the symmetry $Z(s) = Z(1 - s)$, it is only necessary
to count zeros in the top half-plane.</p>
<p>We have to be careful about the poles of $Z(s)$ at 0 and 1. We can either
integrate right above them, or expand the contour to include them. I chose to
do the former, starting at $0.1i$. It is known that there $\zeta(s)$ has no
zeros near the real axis on the critical strip. I could have also expanded the
contour to go around 0 and 1, and offset the result by 2 to account for the
integral counting those points as poles.</p>
<p>It has also been shown that there are no zeros on the lines $\mathrm{Re}(s) =
0$ or $\mathrm{Re}(s) = 1$, so we do not need to worry about that. If the
upper point of our contour happens to have zeros exactly on it, we would be
very unlucky, but even if this were to happen we could just adjust it up a
little bit.</p>
<img src="contour-c.svg" alt="Our contour" width="608"><p>(Our contour. Created with <a href="https://www.geogebra.org/graphing/nmnsaywd">Geogebra</a>)</p>
<p><a href="http://mpmath.org/doc/current/calculus/integration.html#mpmath.quad"><code>mpmath.quad</code></a>
can take a list of points to compute a contour. The <code>maxdegree</code> parameter
allows us to increase the degree of the quadrature if it becomes necessary to
get an accurate result.</p>
<pre><code class="language-py">&gt;&gt;&gt; def argument_count(func, N, maxdegree=6):
...     return 1/(2*mpmath.pi*1j)*(mpmath.quad(func,
...         [1 + 0.1j, 1 + N*1j, 0 + N*1j, 0 + 0.1j,  1 + 0.1j],
...         maxdegree=maxdegree))
</code></pre>
<p>Now let's test it. Lets count the zeros of $$s^2 - s + 1/2$$ in the box
bounded by the above rectangle ($N = 10$).</p>
<pre><code class="language-py">&gt;&gt;&gt; expr = s**2 - s + S(1)/2
&gt;&gt;&gt; argument_count(lambdify(s, expr.diff(s)/expr), 10)
mpc(real='1.0', imag='3.4287545414000525e-24')
</code></pre>
<p>The integral is 1. We can confirm there is indeed one
zero in this box, at $\frac{1}{2} + \frac{i}{2}$.</p>
<pre><code class="language-py">&gt;&gt;&gt; solve(s**2 - s + S(1)/2)
[1/2 - I/2, 1/2 + I/2]
</code></pre>
<p>Now define a function to count the number of sign changes in a list of real
values.</p>
<pre><code class="language-py">&gt;&gt;&gt; def sign_changes(L):
...     """
...     Count the number of sign changes in L
...
...     Values of L should all be real.
...     """
...     changes = 0
...     assert im(L[0]) == 0, L[0]
...     s = sign(L[0])
...     for i in L[1:]:
...         assert im(i) == 0, i
...         s_ = sign(i)
...         if s_ == 0:
...             # Assume these got chopped to 0
...             continue
...         if s_ != s:
...             changes += 1
...         s = s_
...     return changes
</code></pre>
<p>For example, for $\sin(s)$ from -10 to 10, there are 7 zeros ($3\pi\approx
9.42$)</p>
<pre><code class="language-py">&gt;&gt;&gt; sign_changes(lambdify(s, sin(s))(np.linspace(-10, 10)))
7
</code></pre>
<p>Now compute sign changes along the critical line. We also make provisions in
case we have to increase the precision of mpmath to get correct results here.</p>
<pre><code class="language-py">&gt;&gt;&gt; def compute_points(Z_func, N, npoints=10000, dps=15):
...     import warnings
...     old_dps = mpmath.mp.dps
...     points = np.linspace(0, N, npoints)
...     try:
...         mpmath.mp.dps = dps
...         L = [mpmath.chop(Z_func(i)) for i in 1/2 + points*1j]
...     finally:
...         mpmath.mp.dps = old_dps
...     if L[-1] == 0:
...         # mpmath will give 0 if the precision is not high enough, since Z
...         # decays rapidly on the critical line.
...         warnings.warn("You may need to increase the precision")
...     return L
</code></pre>
<p>Now we can check how many zeros of $Z(s)$ (and hence non-trivial zeros of
$\zeta(s)$) we can find. According to
<a href="https://en.wikipedia.org/wiki/Riemann_hypothesis">Wikipedia</a>, the first few
non-trivial zeros of $\zeta(s)$ in the upper half-plane are 14.135, 21.022,
and 25.011.</p>
<p>First try up to $N=20$.</p>
<pre><code class="language-py">&gt;&gt;&gt; argument_count(D_func, 20)
mpc(real='0.99999931531867581', imag='-3.2332902529067346e-24')
</code></pre>
<p>Mathematically, the above value <em>must</em> be an integer, so we know it is 1.</p>
<p>Now check the number of sign changes of $Z(s)$ from $\frac{1}{2}$ to
$\frac{1}{2} + 20i$.</p>
<pre><code class="language-py">&gt;&gt;&gt; L = compute_points(Z_func, 20)
&gt;&gt;&gt; sign_changes(L)
1
</code></pre>
<p>So it checks out. There is one zero between $0$ and $20i$ on the critical
strip, and it is in fact on the critical line, as expected!</p>
<p>Now let's verify the other two zeros from Wikipedia.</p>
<pre><code class="language-py">&gt;&gt;&gt; argument_count(D_func, 25)
mpc(real='1.9961479945577916', imag='-3.2332902529067346e-24')
&gt;&gt;&gt; L = compute_points(Z_func, 25)
&gt;&gt;&gt; sign_changes(L)
2
&gt;&gt;&gt; argument_count(D_func, 30)
mpc(real='2.9997317058520916', imag='-3.2332902529067346e-24')
&gt;&gt;&gt; L = compute_points(Z_func, 30)
&gt;&gt;&gt; sign_changes(L)
3
</code></pre>
<p>Both check out as well.</p>
<p>Since we are computing the points, we can go ahead and make a plot as well.
However, there is a technical difficulty. If you naively try to plot $Z(1/2 +
it)$, you will find that it decays rapidly, so fast that you cannot really
tell where it crosses 0:</p>
<pre><code class="language-py">&gt;&gt;&gt; def plot_points_bad(L, N):
...     npoints = len(L)
...     points = np.linspace(0, N, npoints)
...     plt.figure()
...     plt.plot(points, L)
...     plt.plot(points, [0]*npoints, linestyle=':')
&gt;&gt;&gt; plot_points_bad(L, 30)
</code></pre>
<img src="riemann-bad.svg" width="608"><p>So instead of plotting $Z(1/2 + it)$, we plot $\log(|Z(1/2 + it)|)$. The
logarithm will make the zeros go to $-\infty$, but these will be easy to see.</p>
<pre><code class="language-py">&gt;&gt;&gt; def plot_points(L, N):
...     npoints = len(L)
...     points = np.linspace(0, N, npoints)
...     p = [mpmath.log(abs(i)) for i in L]
...     plt.figure()
...     plt.plot(points, p)
...     plt.plot(points, [0]*npoints, linestyle=':')
&gt;&gt;&gt; plot_points(L, 30)
</code></pre>
<img src="riemann-30.svg" width="608"><p>The spikes downward are the zeros.</p>
<p>Finally, let's check up to N=100. <a href="https://oeis.org/A072080">OEIS A072080</a>
gives the number of zeros of $\zeta(s)$ in upper half-plane up to $10^ni$.
According to it, we should get 29 zeros between $0$ and $100i$.</p>
<pre><code class="language-py">&gt;&gt;&gt; argument_count(D_func, 100)
mpc(real='28.248036536895913', imag='-3.2332902529067346e-24')
</code></pre>
<p>This is not near an integer. This means we need to increase the precision of
the quadrature (the <code>maxdegree</code> argument).</p>
<pre><code class="language-py">&gt;&gt;&gt; argument_count(D_func, 100, maxdegree=9)
mpc(real='29.000000005970151', imag='-3.2332902529067346e-24')
</code></pre>
<p>And the sign changes...</p>
<pre><code class="language-py">&gt;&gt;&gt; L = compute_points(Z_func, 100)
__main__:11: UserWarning: You may need to increase the precisio
</code></pre>
<p>Our guard against the precision being too low was triggered. Try raising it
(the default dps is 15).</p>
<pre><code class="language-py">&gt;&gt;&gt; L = compute_points(Z_func, 100, dps=50)
&gt;&gt;&gt; sign_changes(L)
29
</code></pre>
<p>They both give 29. So we have verified the Riemann Hypothesis up to $100i$!</p>
<p>Here is a plot of these 29 zeros.</p>
<pre><code class="language-py">&gt;&gt;&gt; plot_points(L, 100)
</code></pre>
<img src="riemann-100.svg" width="608"><p>(remember that the spikes downward are the zeros)</p>
<h2>Conclusion</h2>
<p>$N=100$ takes a few minutes to compute, and I imagine larger and larger values
would require increasing the precision more, slowing it down even further, so
I didn't go higher than this. But it is clear that this method works.</p>
<p>This was just me playing around with SymPy and mpmath, but if I wanted to
actually verify the Riemann Hypothesis, I would try to find a more efficient
method of computing the above quantities. For the sake of simplicity, I used
$Z(s)$ for both the argument principle and sign changes computations, but it
would have been more efficient to use $\zeta(s)$ for the argument principle
integral, since it has a simpler formula. It would also be useful if there
were a formula with similar properties to $Z(s)$ (real on the critical line
with the same zeros as $\zeta(s)$), but that did not decay as rapidly.</p>
<p>Furthermore, for the argument principle integral, I would like to see precise
error estimates for the integral. We saw above with $N=100$ with the default
quadrature that we got a value of 28.248, which is not close to an integer.
This tipped us off that we should increase the quadrature, which ended up
giving us the right answer, but if the original number happened to be close to
an integer, we might have been fooled. Ideally, one would like know the exact
quadrature degree needed. If you can get error estimates guaranteeing the
error for the integral will be less than 0.5, you can always round the answer
to the nearest integer. For the sign changes, you don't need to be as
rigorous, because simply seeing as many sign changes as you have zeros is
sufficient. However, one could certainly be more efficient in computing the
values along the interval, rather than just naively computing 10000 points and
raising the precision until it works, as I have done.</p>
<p>One would also probably want to use a faster integrator than mpmath (like one
written in C), and perhaps also find a faster to evaluate expression than the
one I used for $Z(s)$. It is also possible that one could special-case the
quadrature algorithm knowing that it will be computed on $\zeta'(s)/\zeta(s)$.</p>
<p>In this post I described the Riemann zeta function and the Riemann Hypothesis,
and showed how to computationally verify it. But I didn't really go over the
details of why the Riemann Hypothesis matters. I encourage you to watch the
videos in my <a href="https://www.youtube.com/playlist?list=PLrFrByaoJbcqKjzgJvLs2-spSmzP7jolT">YouTube
playlist</a>
if you want to know this. Among other things, the truth of the Riemann
Hypothesis would give a very precise bound on the distribution of prime
numbers. Also, the non-trivial zeros of $\zeta(s)$ are, in some sense, the
"spectrum" of the prime numbers, meaning they exactly encode the position of
every prime on the number line.</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/quansight-labs-work-update-for-september-2019/" class="u-url">Quansight Labs Work Update for September, 2019</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Aaron Meurer
            </span></p>
            <p class="dateline">
            <a href="posts/quansight-labs-work-update-for-september-2019/" rel="bookmark">
            <time class="published dt-published" datetime="2019-10-07T00:00:00-05:00" itemprop="datePublished" title="2019-10-07 00:00">2019-10-07 00:00</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/quansight-labs-work-update-for-september-2019/#disqus_thread" data-disqus-identifier="cache/posts/quansight-labs-work-update-for-september-2019.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p><em>This post has been cross-posted on the <a href="https://labs.quansight.org/blog/2019/10/quansight-labs-work-update-for-september-2019/">Quansight Labs
Blog</a>.</em></p>
<p>As of November, 2018, I have been working at
<a href="https://www.quansight.com/">Quansight</a>. Quansight is a new startup founded by
the same people who started Anaconda, which aims to connect companies and open
source communities, and offers consulting, training, support and mentoring
services. I work under the heading of <a href="https://www.quansight.com/labs">Quansight
Labs</a>. Quansight Labs is a public-benefit
division of Quansight. It provides a home for a "PyData Core Team" which
consists of developers, community managers, designers, and documentation
writers who build open-source technology and grow open-source communities
around all aspects of the AI and Data Science workflow.</p>
<p>My work at Quansight is split between doing open source consulting for various
companies, and working on SymPy.
<a href="https://www.sympy.org/en/index.html">SymPy</a>, for those who do not know, is a
symbolic mathematics library written in pure Python. I am the lead maintainer
of SymPy.</p>
<p>In this post, I will detail some of the open source work that I have done
recently, both as part of my open source consulting, and as part of my work on
SymPy for Quansight Labs.</p>
<h3>Bounds Checking in Numba</h3>
<p>As part of work on a client project, I have been working on contributing code
to the <a href="https://numba.pydata.org">numba</a> project. Numba is a just-in-time
compiler for Python. It lets you write native Python code and with the use of
a simple <code>@jit</code> decorator, the code will be automatically sped up using LLVM.
This can result in code that is up to 1000x faster in some cases:</p>
<pre><code>
In [1]: import numba

In [2]: import numpy

In [3]: def test(x):
   ...:     A = 0
   ...:     for i in range(len(x)):
   ...:         A += i*x[i]
   ...:     return A
   ...:

In [4]: @numba.njit
   ...: def test_jit(x):
   ...:     A = 0
   ...:     for i in range(len(x)):
   ...:         A += i*x[i]
   ...:     return A
   ...:

In [5]: x = numpy.arange(1000)

In [6]: %timeit test(x)
249 ¬µs ¬± 5.77 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)

In [7]: %timeit test_jit(x)
336 ns ¬± 0.638 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each)

In [8]: 249/.336
Out[8]: 741.0714285714286
</code></pre>
<p>Numba only works for a subset of Python code, and primarily targets code that
uses NumPy arrays.</p>
<p>Numba, with the help of LLVM, achieves this level of performance through many
optimizations. One thing that it does to improve performance is to remove all
bounds checking from array indexing. This means that if an array index is out
of bounds, instead of receiving an <code>IndexError</code>, you will get garbage, or
possibly a segmentation fault.</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from numba import njit
&gt;&gt;&gt; def outtabounds(x):
...     A = 0
...     for i in range(1000):
...         A += x[i]
...     return A
&gt;&gt;&gt; x = np.arange(100)
&gt;&gt;&gt; outtabounds(x) # pure Python/NumPy behavior
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "&lt;stdin&gt;", line 4, in outtabounds
IndexError: index 100 is out of bounds for axis 0 with size 100
&gt;&gt;&gt; njit(outtabounds)(x) # the default numba behavior
-8557904790533229732
</code></pre>
<p>In numba pull request <a href="https://github.com/numba/numba/pull/4432">#4432</a>, I am
working on adding a flag to <code>@njit</code> that will enable bounds checks for array
indexing. This will remain disabled by default for performance purposes. But
you will be able to enable it by passing <code>boundscheck=True</code> to <code>@njit</code>, or by
setting the <code>NUMBA_BOUNDSCHECK=1</code> environment variable. This will make it
easier to detect out of bounds issues like the one above. It will work like</p>
<pre><code class="language-pycon">&gt;&gt;&gt; @njit(boundscheck=True)
... def outtabounds(x):
...     A = 0
...     for i in range(1000):
...         A += x[i]
...     return A
&gt;&gt;&gt; x = np.arange(100)
&gt;&gt;&gt; outtabounds(x) # numba behavior in my pull request #4432
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
IndexError: index is out of bounds
</code></pre>
<p>The pull request is still in progress, and many things such as the quality of
the error message reporting will need to be improved. This should make
debugging issues easier for people who write numba code once it is merged.</p>
<h3>removestar</h3>
<p><a href="https://www.asmeurer.com/removestar/">removestar</a> is a new tool I wrote to
automatically replace <code>import *</code> in Python modules with explicit imports.</p>
<p>For those who don't know, Python's <code>import</code> statement supports so-called
"wildcard" or "star" imports, like</p>
<pre><code class="language-py">from sympy import *
</code></pre>
<p>This will import every public name from the <code>sympy</code> module into the current
namespace. This is often useful because it saves on typing every name that is
used in the import line. This is especially useful when working interactively,
where you just want to import every name and minimize typing.</p>
<p>However, doing <code>from module import *</code> is generally frowned upon in Python. It is
considered acceptable when working interactively at a <code>python</code> prompt, or in
<code>__init__.py</code> files (removestar skips <code>__init__.py</code> files by default).</p>
<p>Some reasons why <code>import *</code> is bad:</p>
<ul>
<li>It hides which names are actually imported.</li>
<li>It is difficult both for human readers and static analyzers such as
pyflakes to tell where a given name comes from when <code>import *</code> is used. For
example, pyflakes cannot detect unused names (for instance, from typos) in
the presence of <code>import *</code>.</li>
<li>If there are multiple <code>import *</code> statements, it may not be clear which names
come from which module. In some cases, both modules may have a given name,
but only the second import will end up being used. This can break people's
intuition that the order of imports in a Python file generally does not
matter.</li>
<li>
<code>import *</code> often imports more names than you would expect. Unless the module
you import defines <code>__all__</code> or carefully <code>del</code>s unused names at the module
level, <code>import *</code> will import every public (doesn't start with an
underscore) name defined in the module file. This can often include things
like standard library imports or loop variables defined at the top-level of
the file. For imports from modules (from <code>__init__.py</code>), <code>from module import *</code> will include every submodule defined in that module. Using <code>__all__</code> in
modules and <code>__init__.py</code> files is also good practice, as these things are
also often confusing even for interactive use where <code>import *</code> is
acceptable.</li>
<li>In Python 3, <code>import *</code> is syntactically not allowed inside of a function
definition.</li>
</ul>
<p>Here are some official Python references stating not to use <code>import *</code> in
files:</p>
<ul>
<li>
<p><a href="https://docs.python.org/3/faq/programming.html?highlight=faq#what-are-the-best-practices-for-using-import-in-a-module">The official Python
FAQ</a>:</p>
<blockquote>
<p>In general, don‚Äôt use <code>from modulename import *</code>. Doing so clutters the
importer‚Äôs namespace, and makes it much harder for linters to detect
undefined names.</p>
</blockquote>
</li>
<li>
<p><a href="https://www.python.org/dev/peps/pep-0008/#imports">PEP 8</a> (the official
Python style guide):</p>
<blockquote>
<p>Wildcard imports (<code>from &lt;module&gt; import *</code>) should be avoided, as they
make it unclear which names are present in the namespace, confusing both
readers and many automated tools.</p>
</blockquote>
</li>
</ul>
<p>Unfortunately, if you come across a file in the wild that uses <code>import *</code>, it
can be hard to fix it, because you need to find every name in the file that is
imported from the <code>*</code> and manually add an import for it. Removestar makes this
easy by finding which names come from <code>*</code> imports and replacing the import
lines in the file automatically.</p>
<p>As an example, suppose you have a module <code>mymod</code> like</p>
<pre><code>mymod/
  | __init__.py
  | a.py
  | b.py
</code></pre>
<p>with</p>
<pre><code class="language-py"># mymod/a.py
from .b import *

def func(x):
    return x + y
</code></pre>
<p>and</p>
<pre><code class="language-py"># mymod/b.py
x = 1
y = 2
</code></pre>
<p>Then <code>removestar</code> works like:</p>
<pre><code>$ removestar -i mymod/
$ cat mymod/a.py
# mymod/a.py
from .b import y

def func(x):
    return x + y
</code></pre>
<p>The <code>-i</code> flag causes it to edit <code>a.py</code> in-place. Without it, it would just
print a diff to the terminal.</p>
<p>For implicit star imports and explicit star imports from the same module,
<code>removestar</code> works statically, making use of
<a href="https://github.com/PyCQA/pyflakes">pyflakes</a>. This means none of the code is
actually executed. For external imports, it is not possible to work statically
as external imports may include C extension modules, so in that case, it
imports the names dynamically.</p>
<p><code>removestar</code> can be installed with pip or conda:</p>
<pre><code>pip install removestar
</code></pre>
<p>or if you use conda</p>
<pre><code>conda install -c conda-forge removestar
</code></pre>
<h3>sphinx-math-dollar</h3>
<p>In SymPy, we make heavy use of LaTeX math in our documentation. For example,
in our <a href="https://docs.sympy.org/dev/modules/functions/special.html#sympy.functions.special.hyper.hyper">special functions
documentation</a>,
most special functions are defined using a LaTeX formula, like <img src="besselj_docs.png" alt="The docs for besselj"></p>
<p>(from <a href="https://docs.sympy.org/dev/modules/functions/special.html#sympy.functions.special.bessel.besselj">https://docs.sympy.org/dev/modules/functions/special.html#sympy.functions.special.bessel.besselj</a>)</p>
<p>However, the source for this math in the docstring of the function uses RST
syntax:</p>
<pre><code class="language-py">class besselj(BesselBase):
    """
    Bessel function of the first kind.

    The Bessel `J` function of order `\nu` is defined to be the function
    satisfying Bessel's differential equation

    .. math ::
        z^2 \frac{\mathrm{d}^2 w}{\mathrm{d}z^2}
        + z \frac{\mathrm{d}w}{\mathrm{d}z} + (z^2 - \nu^2) w = 0,

    with Laurent expansion

    .. math ::
        J_\nu(z) = z^\nu \left(\frac{1}{\Gamma(\nu + 1) 2^\nu} + O(z^2) \right),

    if :math:`\nu` is not a negative integer. If :math:`\nu=-n \in \mathbb{Z}_{&lt;0}`
    *is* a negative integer, then the definition is

    .. math ::
        J_{-n}(z) = (-1)^n J_n(z).
</code></pre>
<p>Furthermore, in SymPy's documentation we have configured it so that text
between `single backticks` is rendered as math. This was originally done for
convenience, as the alternative way is to write <code>:math:`\nu`</code> every
time you want to use inline math. But this has lead to many people being
confused, as they are used to Markdown where `single backticks` produce
<code>code</code>.</p>
<p>A better way to write this would be if we could delimit math with dollar
signs, like <code>$\nu$</code>. This is how things are done in LaTeX documents, as well
as in things like the Jupyter notebook.</p>
<p>With the new <a href="https://www.sympy.org/sphinx-math-dollar/">sphinx-math-dollar</a>
Sphinx extension, this is now possible. Writing <code>$\nu$</code> produces $\nu$, and
the above docstring can now be written as</p>
<pre><code class="language-py">class besselj(BesselBase):
    """
    Bessel function of the first kind.

    The Bessel $J$ function of order $\nu$ is defined to be the function
    satisfying Bessel's differential equation

    .. math ::
        z^2 \frac{\mathrm{d}^2 w}{\mathrm{d}z^2}
        + z \frac{\mathrm{d}w}{\mathrm{d}z} + (z^2 - \nu^2) w = 0,

    with Laurent expansion

    .. math ::
        J_\nu(z) = z^\nu \left(\frac{1}{\Gamma(\nu + 1) 2^\nu} + O(z^2) \right),

    if $\nu$ is not a negative integer. If $\nu=-n \in \mathbb{Z}_{&lt;0}$
    *is* a negative integer, then the definition is

    .. math ::
        J_{-n}(z) = (-1)^n J_n(z).
</code></pre>
<p>We also plan to add support for <code>$$double dollars$$</code> for display math so that <code>.. math ::</code> is no longer needed either .</p>
<p>For end users, the documentation on <a href="https://docs.sympy.org">docs.sympy.org</a>
will continue to render exactly the same, but for developers, it is much
easier to read and write.</p>
<p>This extension can be easily used in any Sphinx project. Simply install it
with pip or conda:</p>
<pre><code>pip install sphinx-math-dollar
</code></pre>
<p>or</p>
<pre><code>conda install -c conda-forge sphinx-math-dollar
</code></pre>
<p>Then enable it in your <code>conf.py</code>:</p>
<pre><code class="language-py">extensions = ['sphinx_math_dollar', 'sphinx.ext.mathjax']
</code></pre>
<h3>Google Season of Docs</h3>
<p>The above work on sphinx-math-dollar is part of work I have been doing to
improve the tooling around SymPy's documentation. This has been to assist our
technical writer Lauren Glattly, who is working with SymPy for the next three
months as part of the new <a href="https://developers.google.com/season-of-docs/">Google Season of
Docs</a> program. Lauren's project
is to improve the consistency of our docstrings in SymPy. She has already
identified many key ways our docstring documentation can be improved, and is
currently working on a style guide for writing docstrings. Some of the issues
that Lauren has identified require improved tooling around the way the HTML
documentation is built to fix. So some other SymPy developers and I have been
working on improving this, so that she can focus on the technical writing
aspects of our documentation.</p>
<p>Lauren has created a draft style guide for documentation at
<a href="https://github.com/sympy/sympy/wiki/SymPy-Documentation-Style-Guide">https://github.com/sympy/sympy/wiki/SymPy-Documentation-Style-Guide</a>. Please
take a moment to look at it and if you have any feedback on it, comment below
or write to the SymPy mailing list.</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/whats-new-in-sympy-14/" class="u-url">What's New in SymPy 1.4</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Aaron Meurer
            </span></p>
            <p class="dateline">
            <a href="posts/whats-new-in-sympy-14/" rel="bookmark">
            <time class="published dt-published" datetime="2019-05-02T00:00:00-05:00" itemprop="datePublished" title="2019-05-02 00:00">2019-05-02 00:00</time></a>
            </p>
                <p class="commentline">
        
    <a href="posts/whats-new-in-sympy-14/#disqus_thread" data-disqus-identifier="cache/posts/whats-new-in-sympy-14.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <div>
<p><em>This post has been cross-posted on the <a href="https://labs.quansight.org/blog/2019/04/whats-new-in-sympy-14/">Quansight Blog</a>.</em></p>
<p>As of November, 2018, I have been working at
<a href="https://www.quansight.com/">Quansight</a>. Quansight is a new startup founded by
the same people who started Anaconda, which aims to connect companies and open
source communities, and offers consulting, training, support and mentoring
services. I work under the heading of <a href="https://www.quansight.com/labs">Quansight
Labs</a>. Quansight Labs is a public-benefit
division of Quansight. It provides a home for a "PyData Core Team" which
consists of developers, community managers, designers, and documentation
writers who build open-source technology and grow open-source communities
around all aspects of the AI and Data Science workflow. As a part of this, I
am able to spend a fraction of my time working on SymPy.
<a href="https://www.sympy.org/en/index.html">SymPy</a>, for those who do not know, is a
symbolic mathematics library written in pure Python. I am the lead maintainer
of SymPy.</p>
<p>SymPy 1.4 was released on April 9, 2019. In this post, I'd like to go over
some of the highlights for this release. The full release notes for the
release can be found on the <a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-1.4">SymPy
wiki</a>.</p>
<p>To update to SymPy 1.4, use</p>
<pre><code class="language-bash">conda install sympy
</code></pre>
<p>or if you prefer to use pip</p>
<pre><code class="language-bash">pip install -U sympy
</code></pre>
<p>The SymPy 1.4 release contains over <a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-1.4#authors">500 changes from 38 different
submodules</a>,
so I will not be going over every change, but only a few of the main
highlights. A <a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-1.4#authors">total of 104
people</a>
contributed to this release, of whom 66 contributed for the first time for
this release.</p>
<p>While I did not personally work on any of the changes listed below (my work
for this release tended to be more invisible, behind the scenes fixes), I did
do the release itself.</p>
<h2>Automatic LaTeX rendering in the Jupyter notebook</h2>
<p>Prior to SymPy 1.4, SymPy expressions in the notebook rendered by default with their
string representation. To get <code>LaTeX</code> output, you had to call <code>init_printing()</code>:</p>
<img src="sympy-1.3-notebook.png" alt="SymPy 1.3 rendering in the Jupyter lab notebook"><p>In SymPy 1.4, SymPy expressions now automatically render as LaTeX in the notebook:</p>
<img src="sympy-1.4-notebook.png" alt="SymPy 1.4 rendering in the Jupyter lab notebook"><p>However, this only applies automatically if the type of an object is a SymPy
expression. For built-in types such as lists or ints, <code>init_printing()</code> is
still required to get LaTeX printing. For example, <code>solve()</code> returns a list,
so does not render as LaTeX unless <code>init_printing()</code> is called:</p>
<img src="sympy-1.4-notebook-2.png" alt="SymPy 1.4 rendering in the Jupyter lab notebook with init_printing()"><p><code>init_printing()</code> is also still needed if you want to change any of the
printing settings, for instance, passing flags to the <code>latex()</code> printer or
selecting a different printer.</p>
<p>If you want the string form of an expression for copy-pasting, you can use
<code>print</code>.</p>
<h2>Improved simplification of relational expressions</h2>
<p>Simplification of relational and piecewise expressions has been improved:</p>
<pre><code class="language-py">&gt;&gt;&gt; x, y, z, w = symbols('x y z w')
&gt;&gt;&gt; init_printing()
&gt;&gt;&gt; expr = And(Eq(x,y), x &gt;= y, w &lt; y, y &gt;= z, z &lt; y)
&gt;&gt;&gt; expr
x = y ‚àß x ‚â• y ‚àß y ‚â• z ‚àß w &lt; y ‚àß z &lt; y
&gt;&gt;&gt; simplify(expr)
x = y ‚àß y &gt; Max(w, z)
</code></pre>
<pre><code class="language-py">&gt;&gt;&gt; expr = Piecewise((x*y, And(x &gt;= y, Eq(y, 0))), (x - 1, Eq(x, 1)), (0, True))
&gt;&gt;&gt; expr
‚éß x‚ãÖy   for y = 0 ‚àß x ‚â• y
‚é™
‚é®x - 1      for x = 1
‚é™
‚é©  0        otherwise
&gt;&gt;&gt; simplify(expr)
0
</code></pre>
<h2>Improved MathML printing</h2>
<p>The MathML presentation printer has been greatly improved, putting it on par
with the existing Unicode and LaTeX pretty printers.</p>
<pre><code class="language-py">&gt;&gt;&gt; mathml(Integral(exp(-x**2), (x, -oo, oo)), 'presentation')
&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;&amp;#x222B;&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x221E;&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x221E;&lt;/mi&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mi&gt;&amp;ExponentialE;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;dd;&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;
</code></pre>
<p>If your <a href="https://caniuse.com/#feat=mathml">browser supports MathML</a> (at the
time of writing, only Firefox and Safari), you should see the above
presentation form for <code>Integral(exp(-x**2), (x, -oo, oo))</code> below:</p>
<p><math style="display: block;"><mrow><msubsup><mo>‚à´</mo><mrow><mo>-</mo><mi>‚àû</mi></mrow><mi>‚àû</mi></msubsup><msup><mi>‚Öá</mi><mrow><mo>-</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msup><mo>‚ÖÜ</mo><mi>x</mi></mrow></math></p>
<h2>Improvements to solvers</h2>
<p>Several improvements have been made to the solvers.</p>
<pre><code class="language-py">&gt;&gt;&gt; eq = Eq((x**2 - 7*x + 11)**(x**2 - 13*x + 42), 1)
&gt;&gt;&gt; eq
                2
               x  - 13‚ãÖx + 42
‚éõ 2           ‚éû
‚éùx  - 7‚ãÖx + 11‚é†               = 1
&gt;&gt;&gt; solve(eq, x) # In SymPy 1.3, this only gave the partial solution [2, 5, 6, 7]
[2, 3, 4, 5, 6, 7]
</code></pre>
<p>The ODE solver, <code>dsolve</code>, has also seen some improvements. Two new hints have
been added.</p>
<p><code>'nth_algebraic'</code> solves ODEs using <code>solve</code> by inverting the derivatives
algebraically:</p>
<pre><code class="language-py">&gt;&gt;&gt; f = Function('f')
&gt;&gt;&gt; eq = Eq(f(x) * (f(x).diff(x)**2 - 1), 0)
&gt;&gt;&gt; eq
‚éõ          2    ‚éû
‚éú‚éõd       ‚éû     ‚éü
‚éú‚éú‚îÄ‚îÄ(f(x))‚éü  - 1‚éü‚ãÖf(x) = 0
‚éù‚éùdx      ‚é†     ‚é†
&gt;&gt;&gt; dsolve(eq, f(x)) # In SymPy 1.3, this only gave the solution f(x) = C1 - x
[f(x) = 0, f(x) = C‚ÇÅ - x, f(x) = C‚ÇÅ + x]
</code></pre>
<p><code>'nth_order_reducible'</code> solves ODEs that only involve derivatives of <code>f(x)</code>,
via the substitution $g(x)=f^{(n)}(x)$.</p>
<pre><code class="language-py">&gt;&gt;&gt; eq = Eq(Derivative(f(x), (x, 2)) + x*Derivative(f(x), x), x)
&gt;&gt;&gt; eq
               2
  d           d
x‚ãÖ‚îÄ‚îÄ(f(x)) + ‚îÄ‚îÄ‚îÄ(f(x)) = x
  dx           2
             dx
&gt;&gt;&gt; dsolve(eq, f(x))
                  ‚éõ‚àö2‚ãÖx‚éû
f(x) = C‚ÇÅ + C‚ÇÇ‚ãÖerf‚éú‚îÄ‚îÄ‚îÄ‚îÄ‚éü + x
                  ‚éù 2  ‚é†
</code></pre>
<h2>Dropping Python 3.4 support</h2>
<p>This is the last release of SymPy to support Python 3.4. SymPy 1.4 supports
Python 2.7, 3.4, 3.5, 3.6, 3.7, and PyPy. What's perhaps more exciting is that
the next release of SymPy, 1.5, which will be released later this year, will
be the last version to support Python 2.7.</p>
<p>Our
<a href="https://github.com/sympy/sympy/wiki/Python-version-support-policy">policy</a> is
to drop support for major Python versions when they reach their <a href="https://devguide.python.org/#status-of-python-branches">End of
Life</a>. In other words,
they receive no further support from the core Python team. Python 3.4 reached
its end of life on May 19 of this year, and Python 2.7 will reach its end of
life on January 1, 2020.</p>
<p>I have <a href="https://www.asmeurer.com/blog/posts/moving-away-from-python-2/">blogged in the
past</a> on why I
believe it is important for library authors to be proactive in dropping Python
2 support, and since then <a href="https://python3statement.org">a large number of Python
libraries</a> have either dropped support or
announced their plans to by 2020.</p>
<p>Having Python 2 support removed will not only allow us to remove a <a href="https://github.com/sympy/sympy/blob/sympy-1.4/sympy/core/compatibility.py">large
amount of compatibility
cruft</a>
from our codebase, it will also allow us to use some Python 3-only features
that will clean up our API, such as <a href="https://python-3-for-scientists.readthedocs.io/en/latest/python3_advanced.html#keyword-only-arguments">keyword-only
arguments</a>,
<a href="https://python-3-for-scientists.readthedocs.io/en/latest/python3_features.html#function-annotations">type
hints</a>,
and <a href="https://python-3-for-scientists.readthedocs.io/en/latest/python3_features.html#unicode-variable-names">Unicode variable
names</a>.
It will also enable <a href="https://github.com/sympy/sympy/issues?q=is%3Aissue+is%3Aopen+label%3A%22Dropping+Python+2%22">several internal
changes</a>
that will not be visible to end-users, but which will result in a much cleaner
and more maintainable codebase.</p>
<p>If you are still using Python 2, I strongly recommend switching to Python 3,
as otherwise the entire ecosystem of Python libraries is soon going to stop
improving for you. Python 3 is already highly recommended for SymPy usage due
to several key improvements. In particular, in Python 3, division of two
Python <code>int</code>s like <code>1/2</code> produces the float <code>0.5</code>. In Python 2, it does
integer division (producing <code>1/2 == 0</code>). The Python 2 integer division
behavior can lead to very surprising results when using SymPy (imagine writing
<code>x**2 + 1/2*x + 2</code> and having the <code>x</code> term "disappear"). When using SymPy, we
<a href="https://docs.sympy.org/latest/tutorial/gotchas.html#two-final-notes-and">recommend</a>
using rational numbers (like <code>Rational(1, 2)</code>) and avoiding <code>int/int</code>, but the
Python 3 behavior will at least maintain a mathematically correct result if
you do not do this. SymPy is also <a href="https://speed.python.org/comparison/?exe=12%2BL%2Bmaster%2C12%2BL%2B3.5%2C12%2BL%2B3.6%2C12%2BL%2B2.7&amp;ben=666%2C667%2C669%2C668&amp;env=1%2C2&amp;hor=false&amp;bas=none&amp;chart=normal+bars">already faster in Python
3</a>
due to things like <code>math.gcd</code> and <code>functools.lru_cache</code> being written in C,
and general performance improvements in the interpreter itself.</p>
<h2>And much more</h2>
<p>These are only a few of the highlights of the hundreds of changes in this
release. The full release notes can be found on <a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-1.4">our
wiki</a>. The wiki
also has the in progress changes for our next release, <a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-1.5">SymPy
1.5</a>, which will be
released later this year. Our <a href="https://github.com/sympy/sympy-bot">bot</a>
automatically collects release notes from every pull request, meaning SymPy
releases have very comprehensive and readable release notes pages. If you see
any mistakes on either page, feel free to edit the wiki and fix them.</p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-4.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="asmeurer";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script>
MathJax = {
  tex: {
    inlineMath: [['$', '$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script></main><footer id="footer"><p>Contents ¬© 2020         <a href="mailto:asmeurer@gmail.com">Aaron Meurer</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         
</p>
<p xmlns:dct="https://purl.org/dc/terms/" xmlns:vcard="https://www.w3.org/2001/vcard-rdf/3.0#">
  <a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">
    <img src="https://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0"></a>
</p>
            
        </footer>
</div>
    
            <script src="assets/js/all-nocdn.js"></script><div style="text-align:center">
<span class="st_twitterfollow_large" displaytext="Twitter Follow" st_username="asmeurer"></span>
<span class="st_twitter_large" displaytext="Tweet"></span>
<span class="st_googleplus_large" displaytext="Google +"></span>
<span class="st_plusone_large" displaytext="Google +1"></span>

<span class="st_email_large" displaytext="Email"></span>

</div>


    
    <script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script>
MathJax = {
  tex: {
    inlineMath: [['$', '$']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
