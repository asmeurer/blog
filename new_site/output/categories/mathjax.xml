<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Aaron Meurer's SymPy Blog (mathjax)</title><link>http://asmeurersympy.wordpress.com/</link><description></description><language>en</language><lastBuildDate>Sun, 30 Mar 2014 12:56:28 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>When does x^log(y) = y^log(x)?</title><link>http://asmeurersympy.wordpress.com/posts/2013/03/03/when-does-xlogy-ylogx.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;em&gt;In this blog post, when I write $latex \log(x)$, I mean the natural logarithm, or log base $latex e$, i.e., $latex \ln(x)$.&lt;/em&gt;
&lt;p&gt;A discussion on a &lt;a href="https://github.com/sympy/sympy/pull/1845"&gt;pull request&lt;/a&gt; got me thinking about this question: what are the solutions to the complex equation $latex x^{\log{(y)}} = y^{\log(x)}$?  At the outset, they look like different expressions.  But clearly there some solutions. For example, if $latex x = y$, then obviously the two expressions will be the same.  We probably should exclude $latex x = y = 0$, though note that even if $latex 0^{\log(0)}$ is well-defined (probably if it is it is either 0 or complex $latex \infty$), it will be the same well-defined value. But for the remainder of this blog post, I'll assume that $latex x$ and $latex y$ are nonzero.&lt;/p&gt;
&lt;p&gt;Now, observe that if we apply $latex \log$ to both sides of the equation, we get $latex \log{\left(x^{\log(y)}\right )} = \log {\left (y^{\log(x)}\right )}$.  Now, supposing that we can apply the famous logarithm exponent rule, we would get $latex \log(x)\log(y) = \log(y)\log(x)$, which means that if additionally $latex \log$ is one-to-one, we would have that the original expressions must be equal.&lt;/p&gt;
&lt;p&gt;The second question, that of &lt;a href="http://en.wikipedia.org/wiki/Injective_function"&gt;injectivity&lt;/a&gt;, is easier to answer than the first, so I'll address it first.  Note that the complex exponential is not one-to-one, because for example $latex e^0 = e^{2\pi i} = 1$.  But we still define the complex logarithm as the "inverse" of the complex exponential.  What this really means is that the complex logarithm is strictly speaking not a function, because it is not well-defined. Recall that the definition of one-to-one means that $latex f(x) = f(y)$ implies $latex x = y$, and that the definition of well-defined is that $latex x = y$ implies $latex f(x) = f(y)$.  It is clear to see here that $latex f$ being one-to-one is the same as $latex f^{-1}$ being well-defined and visa-versa ($latex f^{-1}$ here is the same loose definition of an inverse as saying that the complex logarithm is the inverse of the complex exponential).&lt;/p&gt;
&lt;p&gt;So note that the complex logarithm is not well-defined exactly because the complex exponential is not one-to-one.  We of course fix this problem by making it well-defined, i.e., it normally is multivalued, but we pick a single value consistently (i.e., we pick a &lt;a href="http://en.wikipedia.org/wiki/Branch_point#Complex_logarithm"&gt;branch&lt;/a&gt;), so that it is well-defined.  For the remainder of this blog post, I will assume the standard choice of branch cut for the complex logarithm, i.e., the branch cut is along the negative axis, and we choose the branch where, for $latex x &amp;gt; 0$, $latex \log(x)$ is real and $latex \log(-x) = \log(x) + i\pi$.&lt;/p&gt;
&lt;p&gt;My point here is that we automatically know that the complex logarithm is one-to-one because we know that the complex exponential is well-defined.&lt;/p&gt;
&lt;p&gt;So our question boils down to, when does the identity $latex \log{\left (z^a\right)} = a \log(z)$ hold?  In SymPy, this identity is only applied by &lt;code&gt;expand_log()&lt;/code&gt; or &lt;code&gt;logcombine()&lt;/code&gt; when $latex a$ is real and $latex z$ is positive, so let us assume that we know that it holds under those conditions. Note that it also holds for some other values too.  For example, by our definition $latex \log{\left (e^{i\pi}\right)} = \log(-1) = \log(1) + i\pi = i\pi = i\pi\log(e)$.  For our example, this means that $latex x = e$, $latex y = -1$ is a non-trivial solution (non-trivial meaning $latex x \neq y$).   Actually, the way that the complex logarithm being the "inverse" of the complex exponential works is that $latex e^{\log(x)} = x$ for all $latex x$ (on the other hand $latex \log{\left(e^x\right)} \neq x$ in general), so that if $latex x = e$, then $latex x^{\log(y)} = e^{\log(y)} = y$ and $latex y^{\log(x)} = y^{\log(e)} = y^1 = y$.  In other words, $latex x = e$ is always a solution, for any $latex y\, (\neq 0)$ (and similarly $latex y = e$ for all $latex x$).  In terms of our question of when $latex \log{\left(z^a\right)} = a\log(z)$, this just says that this always true for $latex a = \log(e) = 1$, regardless of $latex z$, which is obvious.  We can also notice that this identity always holds for $latex a = 0$, regardless of $latex z$. In terms of our original equation, this means that $latex x = e^0 = 1$ is a solution for all $latex y$ (and as before, $latex y = 1$ for all $latex x$).&lt;/p&gt;
&lt;p&gt;Note that $latex z &amp;gt; 0$ and $latex a$ real corresponds to $latex x, y &amp;gt; 0$ and $latex \log(x), \log(y)$ real, respectively, (which are the same condition).  So we have so far that the following are solutions to $latex x^{\log(y)} = y^{\log(x)}$:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;span style="line-height:13px;"&gt;$latex x, y &amp;gt; 0$&lt;/span&gt;&lt;/li&gt;
    &lt;li&gt;$latex x = y$&lt;/li&gt;
    &lt;li&gt;$latex x = e$, $latex y$ arbitrary&lt;/li&gt;
    &lt;li&gt;$latex y = e$, $latex x$ arbitrary&lt;/li&gt;
    &lt;li&gt;$latex x = 1$, $latex y$ arbitrary&lt;/li&gt;
    &lt;li&gt;$latex y = 1$, $latex x$ arbitrary&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let's look at some cases where $latex \log{\left (z^a\right)} \neq a\log(z)$.  If $latex z &amp;lt; 0$ and $latex a$ is a nonzero even integer, then $latex z^a &amp;gt; 0$ so $latex \log{\left (z^a \right)}) = \log{\left (\left (-z\right )^a \right )} = a\log(-z)$, whereas $latex a\log(z) = a(\log(-z) + i\pi)$, which are different by our assumption that $latex a \neq 0$.  If $latex a$ is an odd integer not equal to 1, then $latex z^a &amp;lt; 0$, so $latex \log{\left (z^a \right)} = \log{\left (-z^a \right )} + i\pi$ = $latex \log{\left (\left(- z\right)^{a} \right )} + i\pi$ &lt;em&gt;Wordpress is refusing to render this. It should be&lt;/em&gt; log((-z)^a) + iπ = $latex a\log(-z) + i\pi$, whereas $latex a\log(z) = a(\log(-z) + i\pi)$ again, which is not the same because $latex a \neq 1$. This means that if we let $latex x &amp;lt; 0$ and $latex y = e^a$, where $latex a \neq 0, 1$, we get a non-solution (and the same if we swap $latex x$ and $latex y$).  &lt;/p&gt;
&lt;p&gt;This is as far as I got tonight. Wordpress is arbitrarily not rendering that LaTeX for no good reason.  That and the very ugly LaTeX images is pissing me off (why wordpress.com hasn't switched to MathJaX yet is beyond me).  The next time I get some free time, I am going to seriously consider switching my blog to something hosted on GitHub, probably using the IPython notebook.  I welcome any hints people can give me on that, especially concerning migrating pages from this blog.&lt;/p&gt;
&lt;p&gt;Here is some work on finding the rest of the solutions: the general definition of $latex \log(x)$ is $latex \log(|x|) + i\arg(x)$, where $latex \arg(x)$ is chosen in $latex (-\pi, \pi]$.  Therefore, if $latex \log{\left(z^a\right )} = a\log(z)$, we must have $latex \arg(z^a) = a\arg(z)$.  I believe a description of all such complex $latex z$ and $latex a$ will give all solutions $latex x = z$, $latex y = e^a$ (and $latex y = z$, $latex x = e^a$) to $latex x^{\log(y)} = y^{\log(x)}$.  I need to verify that, though, and I also need to think about how to describe such $latex z$ and $latex a$. I will (hopefully) continue this post later, either by editing this one or writing a new one (depending on how much more I come up with).  &lt;/p&gt;
&lt;p&gt;Any comments to this post are welcome.  I know you can't preview comments, but if you want to use math, just write it as &lt;code&gt;$latex math$&lt;/code&gt; (like &lt;code&gt;$latex \log(x)$&lt;/code&gt; for $latex \log(x)$). If you mess something up, I'll edit your comment and fix it.&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2013/03/03/when-does-xlogy-ylogx.html</guid><pubDate>Sun, 03 Mar 2013 06:49:35 GMT</pubDate></item><item><title>sqrt(x) now prints as "sqrt(x)"</title><link>http://asmeurersympy.wordpress.com/posts/2011/08/18/sqrtx-now-prints-as-sqrtx.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Just a few moments ago, &lt;a href="https://github.com/sympy/sympy/pull/548" target="_blank"&gt;a branch&lt;/a&gt; was pushed in that fixed one of my biggest grievances in SymPy, if not the biggest.  Previously we had this behavior:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: sqrt(x)&lt;/p&gt;
&lt;p&gt;Out[1]: x**(1/2)&lt;/p&gt;
&lt;p&gt;In [2]: solve(x**2 - 2, x)&lt;/p&gt;
&lt;p&gt;Out[2]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Now suppose you took the output of those expressions and pasted them into isympy:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [3]: x**(1/2)&lt;/p&gt;
&lt;p&gt;Out[3]: x**0.5&lt;/p&gt;
&lt;p&gt;In [4]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;Out[4]: [-1.41421356237, 1.41421356237]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;That's with &lt;code&gt;&lt;strong&gt;future&lt;/strong&gt;.division&lt;/code&gt;.  Here's what would happen with old division:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [2]: x**(1/2)&lt;/p&gt;
&lt;p&gt;Out[2]: 1&lt;/p&gt;
&lt;p&gt;In [3]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;Out[3]: [-1, 1]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;This is because with old division, &lt;code&gt;1/2&lt;/code&gt; evaluates to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The problem is that Python evaluates &lt;code&gt;1/2&lt;/code&gt; to &lt;code&gt;0.5&lt;/code&gt; (or &lt;code&gt;0&lt;/code&gt;) before SymPy has a change to convert it to a Rational.  There were several ways that people got around this.  If you copy an expression with number division in it and want to paste it into a SymPy session, the easiest way to do this was to pass it as a string to &lt;code&gt;sympify()&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: sympify("x**(1/2)")&lt;/p&gt;
&lt;p&gt;Out[1]: x**(1/2)&lt;/p&gt;
&lt;p&gt;In [2]: sympify("[-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]")&lt;/p&gt;
&lt;p&gt;Out[2]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;If that was too much typing for you, you could use the &lt;code&gt;S()&lt;/code&gt; shortcut to &lt;code&gt;sympify()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [3]: S("x**(1/2)")&lt;/p&gt;
&lt;p&gt;Out[3]: x**(1/2)&lt;/p&gt;
&lt;p&gt;In [4]: S("[-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]")&lt;/p&gt;
&lt;p&gt;Out[4]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;This solution is fine if you want to paste an expression into a SymPy session, but it's not a very clean one if you want to paste code into a script. For that, you need to modify the code so that it no longer contains Python int/Python int.  The easiest way to do this is to sympify one of the ints.  So you would do something like&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [5]: x**(S(1)/2)&lt;/p&gt;
&lt;p&gt;Out[5]: x**(1/2)&lt;/p&gt;
&lt;p&gt;In [6]: [-2&lt;strong&gt;(S(1)/2), 2&lt;/strong&gt;(S(1)/2)]&lt;/p&gt;
&lt;p&gt;Out[6]: [-2&lt;strong&gt;(1/2), 2&lt;/strong&gt;(1/2)]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;This wasn't terribly readable, though.  The &lt;em&gt;best&lt;/em&gt; way to fix the problem when you had a power of one half was to use &lt;code&gt;sqrt()&lt;/code&gt;, which is a shortcut to &lt;code&gt;Pow(…, Rational(1, 2))&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;Well, this last item should make you think.  If &lt;code&gt;sqrt(x)&lt;/code&gt; is more readable than &lt;code&gt;x&lt;strong&gt;(S(1)/2)&lt;/strong&gt;&lt;/code&gt; or even &lt;code&gt;x(1/2)&lt;/code&gt;, why not print it like that in the first place.  Well, I thought so, so I changed the string printer, and now this is the way that SymPy works.  So 90% of the time, you can just paste the result of &lt;code&gt;str()&lt;/code&gt; or &lt;code&gt;print&lt;/code&gt;, and it will just work, because there won't be any &lt;code&gt;**(1/2)&lt;/code&gt;, which was by far the most common problem of "Python evaluating the expression to something before we can."  In the git master, SymPy now behaves like&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: sqrt(x)&lt;/p&gt;
&lt;p&gt;Out[1]: sqrt(x)&lt;/p&gt;
&lt;p&gt;In [2]: solve(x**2 - 2, x)&lt;/p&gt;
&lt;p&gt;Out[2]: [-sqrt(2), sqrt(2)]&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;You can obviously just copy and paste these results, and you get the exact same thing back.  Not only does this make expressions more copy-and-pastable, but the output is &lt;em&gt;much&lt;/em&gt; nicer in terms of readability.  Here are some before and afters that come from actual SymPy doctests that I had to change after fixing the printer:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; e = ((2+2&lt;em&gt;sqrt(2))&lt;/em&gt;x+(2+sqrt(8))*y)/(2+sqrt(2))&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; radsimp(e)&lt;/p&gt;
&lt;p&gt;2&lt;strong&gt;(1/2)*x + 2&lt;/strong&gt;(1/2)*y&lt;/p&gt;
&lt;p&gt;After:&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; radsimp(e)&lt;/p&gt;
&lt;p&gt;sqrt(2)&lt;em&gt;x + sqrt(2)&lt;/em&gt;y&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; b = besselj(n, z)&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; b.rewrite(jn)&lt;/p&gt;
&lt;p&gt;2&lt;strong&gt;(1/2)*z&lt;/strong&gt;(1/2)&lt;em&gt;jn(n - 1/2, z)/pi&lt;/em&gt;*(1/2)&lt;/p&gt;
&lt;p&gt;After:&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; b.rewrite(jn)&lt;/p&gt;
&lt;p&gt;sqrt(2)&lt;em&gt;sqrt(z)&lt;/em&gt;jn(n - 1/2, z)/sqrt(pi)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; x = sympify('-1/(-3/2+(1/2)&lt;em&gt;sqrt(5))&lt;/em&gt;sqrt(3/2-1/2*sqrt(5))')&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; x&lt;/p&gt;
&lt;p&gt;(3/2 - 5&lt;strong&gt;(1/2)/2)&lt;/strong&gt;(-1/2)&lt;/p&gt;
&lt;p&gt;After&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; x&lt;/p&gt;
&lt;p&gt;1/sqrt(3/2 - sqrt(5)/2)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;And not only is &lt;code&gt;sqrt(x)&lt;/code&gt; easier to read than &lt;code&gt;x**(1/2)&lt;/code&gt; but it's fewer characters.&lt;/p&gt;
&lt;p&gt;In the course of changing this, I went ahead and did some greps of the repository to get rid of all &lt;code&gt;&lt;strong&gt;(S(1)/2)&lt;/strong&gt;&lt;/code&gt;, &lt;code&gt;Rational(1, 2)&lt;/code&gt; and similar throughout the code base (not just in the output of doctests where the change had to be made), replacing them with just &lt;code&gt;sqrt&lt;/code&gt;.  Big thanks to Chris Smith for helping me catch all instances of this.  Now the code should be a little easier to read and maintain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a big change, and I believe it will fix the copy-paste problem for 90% of expressions. But it does not solve it completely.  It is still possible to get int/int in the string form of an expression.  Only powers of 1/2 and -1/2 are converted to sqrt, so any other rational power will still print as a/b, like&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: x**Rational(3, 2)&lt;/p&gt;
&lt;p&gt;Out[1]: x**(3/2)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Also, as you may have noticed in the last example above, a rational number that sits by itself will still be printed as int/int, like&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [2]: (1 + x)/2&lt;/p&gt;
&lt;p&gt;Out[2]: x/2 + 1/2&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Therefore, I'm leaving the &lt;a href="http://code.google.com/p/sympy/issues/detail?id=2359" target="_blank"&gt;issue for this&lt;/a&gt; open to discuss potential future fixes to the string printer.  One idea is to create a &lt;code&gt;root&lt;/code&gt; function that is a shortcut to &lt;code&gt;root(x, a) == x**(1/a)&lt;/code&gt;. This would work for rational powers where the numerator is 1.  For other rational powers, we could then denest these with an integer power.  It's important to do this in the right order, though, as they are not equivalent.  You can see that SymPy auto-simplifies it when it is mathematically correct in all cases, and not when it is not:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [3]: sqrt(x**3)&lt;/p&gt;
&lt;p&gt;Out[3]: sqrt(x**3)&lt;/p&gt;
&lt;p&gt;In [4]: sqrt(x)**3&lt;/p&gt;
&lt;p&gt;Out[4]: x**(3/2)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Thus $latex \left(\sqrt{x}\right)^3 = x^{\frac{3}{2}}$ but $latex \sqrt{x^3} \neq x^{\frac{3}{2}}$ (to see this, replace $latex x$ with -1).&lt;/p&gt;
&lt;p&gt;So the idea would be to print &lt;code&gt;Pow(expr, Rational(a, b))&lt;/code&gt; as &lt;code&gt;root(expr, b)**a&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;The merits of this are debatable, but anyway I think we should have this &lt;code&gt;root()&lt;/code&gt; function in any case (see &lt;a href="http://code.google.com/p/sympy/issues/detail?id=2643" target="_blank"&gt;issue 2643&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Another idea, which is probably not a good one, is to always print &lt;code&gt;int/int&lt;/code&gt; as &lt;code&gt;S(int)/int&lt;/code&gt;.  So we would get&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; Rational(1, 2)&lt;/p&gt;
&lt;p&gt;S(1)/2&lt;/p&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; x**Rational(4, 5)&lt;/p&gt;
&lt;p&gt;x**(S(4)/5)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;This is probably a bad idea because even though expressions would always be copy-pastable, they would be slightly less readable.  &lt;/p&gt;
&lt;p&gt;By the way, in case you didn't catch it, all of these changes only affect the string printer.  The pretty printer remained unaffected, and would under any additional changes, as it isn't copy-pastable anyway, and already does a superb job of printing roots.&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2011/08/18/sqrtx-now-prints-as-sqrtx.html</guid><pubDate>Thu, 18 Aug 2011 03:11:32 GMT</pubDate></item><item><title>Merging integration3 with sympy-0.7.0 nightmare</title><link>http://asmeurersympy.wordpress.com/posts/2011/07/25/merging-integration3-with-sympy-0-7-0-nightmare.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;For a long time, there have been several problems in my &lt;code&gt;integration3&lt;/code&gt; branch that were fixed in &lt;code&gt;master&lt;/code&gt;.  I decided that as an incentive to finish the release, I would hold off on merging &lt;code&gt;master&lt;/code&gt; into my branch until the 0.7.0 release was finished.  Well, here's a little timeline:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;June 28, 2011:&lt;/strong&gt; SymPy 0.7.0 final is released.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;June 29, 2011:&lt;/strong&gt; I type &lt;code&gt;git merge sympy-0.7.0&lt;/code&gt; in my &lt;code&gt;integration3&lt;/code&gt; branch.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;July 24, 2011 (today; technically July 25 because it's 2 AM):&lt;/strong&gt; I finish merging &lt;code&gt;sympy-0.7.0&lt;/code&gt; into &lt;code&gt;integration3&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's right, it took me over three weeks---almost a month---to merge &lt;code&gt;sympy-0.7.0&lt;/code&gt; into &lt;code&gt;integration3&lt;/code&gt; (granted, I worked on other things at the same time, such as the &lt;a href="https://github.com/asmeurer/sympy/commit/52657848516ce7f4f7119b921d6b8d64131b58d3" target="_blank"&gt;SciPy 2011 conference&lt;/a&gt;, but to me, any merge that takes longer than a day to complete is a problem).  This is because git decided that I needed to fix as a merge conflict just about every single change in the release branch since the base of &lt;code&gt;integration3&lt;/code&gt;.  The total was over 100 files.  You can see the final merge commit &lt;a href="https://github.com/asmeurer/sympy/commit/52657848516ce7f4f7119b921d6b8d64131b58d3" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So I started &lt;code&gt;git mergetool&lt;/code&gt;, without which this whole ordeal would have been 10 times worse.  The mergetool, which on my computer is opendiff, i.e., File Merge, gave the correct change by default in most cases, so I actually did not have to manually fix the majority of the conflicts.  But I did have to go through and do a lot of them.  I had to manually check each difference in the polys, as I had made several changes there in the course of working on &lt;code&gt;integration3&lt;/code&gt;.  In several occisaions, I had to research a change using &lt;code&gt;git log -S&lt;/code&gt; and fancy methods.  And I noticed at least two regressions in the polys, which I fixed.&lt;/p&gt;
&lt;p&gt;mergetool was useless against &lt;code&gt;risch.py&lt;/code&gt; and &lt;code&gt;test_risch.py&lt;/code&gt;, because in my branch I had renamed these to &lt;code&gt;heurisch.py&lt;/code&gt; and &lt;code&gt;test_heurisch.py&lt;/code&gt;.  Fortunately, these were not really modified much by me, so I could basically just replace them with the &lt;code&gt;sympy-0.7.0&lt;/code&gt; versions.&lt;/p&gt;
&lt;p&gt;Once I finished merging I had to deal with test failures.  This was partly expected, as my branch has always had test failures due to my hack disabling algebraic substitution in &lt;code&gt;exp&lt;/code&gt;, which is required for &lt;code&gt;risch_integrate()&lt;/code&gt; to work, but there were also several unrelated ones.  &lt;/p&gt;
&lt;p&gt;Some of these were caused by wrong merge conflict resolutions by me.  So I went through &lt;code&gt;git diff sympy-0.7.0&lt;/code&gt; change by change and made sure that nothing was different that I didn't want to be.  I would recommend doing this for any big merge.&lt;/p&gt;
&lt;p&gt;Then, I had to fix a few bugs that caused test failures.  Several semantics were changed in the release.  I think the ones that I had to change were the renaming of &lt;code&gt;has_any_symbols&lt;/code&gt; to just &lt;code&gt;has&lt;/code&gt;, the renaming of &lt;code&gt;Poly.as_basic()&lt;/code&gt; to &lt;code&gt;Poly.as_expr()&lt;/code&gt;, and the swapping of the meanings of &lt;code&gt;quo&lt;/code&gt; and &lt;code&gt;exquo&lt;/code&gt; in the polys.  There were also some doctest failures due to the change to lexicographic ordering in the printer.&lt;/p&gt;
&lt;p&gt;After all that, there were two regressions that caused test failures.  The first was the following:&lt;/p&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;p&gt;[code lang="py"]&lt;/p&gt;
&lt;p&gt;In [1]: Integral((exp(x&lt;em&gt;log(x))&lt;/em&gt;log(x)), x).subs(exp(x&lt;em&gt;log(x)), x&lt;/em&gt;*x)&lt;/p&gt;
&lt;p&gt;Out[1]: &lt;/p&gt;
&lt;p&gt;⌠             &lt;/p&gt;
&lt;p&gt;⎮  x          &lt;/p&gt;
&lt;p&gt;⎮ x ⋅log(x) dx&lt;/p&gt;
&lt;p&gt;⌡             &lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;After:&lt;/p&gt;
&lt;p&gt;[code lang="py"]&lt;/p&gt;
&lt;p&gt;In [1]: Integral((exp(x&lt;em&gt;log(x))&lt;/em&gt;log(x)), x).subs(exp(x&lt;em&gt;log(x)), x&lt;/em&gt;*x)&lt;/p&gt;
&lt;p&gt;Out[1]: &lt;/p&gt;
&lt;p&gt;⌠                    &lt;/p&gt;
&lt;p&gt;⎮  x⋅log(x)          &lt;/p&gt;
&lt;p&gt;⎮ ℯ        ⋅log(x) dx&lt;/p&gt;
&lt;p&gt;⌡                    &lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;This substitution is necessary because the Risch algorithm requires expressions like $latex x^x$ to be rewritten as $latex e^{x\log(x)}$ before it can integrate them, but I try to convert them back after integrating so that the user gets the same thing in the result that he entered.  I created &lt;a href="http://code.google.com/p/sympy/issues/detail?id=2571" target="_blank"&gt;issue 2571&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;The second was that I had several places in my docstrings with things like&lt;/p&gt;
&lt;blockquote&gt;

Given a derivation D on k[t] and f, g in k(t) with f weakly normalized with respect to t, either raise NonElementaryIntegralException, in which case the equation Dy + f*y == g has no solution in k(t), or the quadruplet (a, b, c, h) such that a, h in k[t], b, c in k, and for any solution y in k(t) of Dy + f*y == g, q = y*h in k satisfies a*Dq + b*q == c.

&lt;/blockquote&gt;

&lt;p&gt;The problem here is the "raise NonElementaryIntegralException," part.  The code quality checker things that this is an old style exception (like &lt;code&gt;raise Exception, message&lt;/code&gt;), due to a poorly formed regular expression.  I fixed this in a &lt;a href="https://github.com/sympy/sympy/pull/511" target="_blank"&gt;pull request&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The good news is that now a lot of stuff works that didn't before because of fixes that were required that only existed in &lt;code&gt;master&lt;/code&gt;.  For example, the following did not work before, but now does due to improvements to &lt;code&gt;RootSum&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;[code lang="py"]&lt;/p&gt;
&lt;p&gt;In [1]: risch_integrate(1/(exp(5*x) + exp(x) + 1), x)&lt;/p&gt;
&lt;p&gt;Out[1]: 
           ⎛    2                                                         &lt;br&gt;
x + RootSum⎝21⋅z  + 6⋅z + 1, Lambda(_i, _i&lt;em&gt;log(-3381&lt;/em&gt;_i&lt;strong&gt;4/4 - 3381*_i&lt;/strong&gt;3/4 &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;                                   &lt;span class="err"&gt;⎞&lt;/span&gt;          &lt;span class="err"&gt;⎛&lt;/span&gt;     &lt;span class="mi"&gt;3&lt;/span&gt;        &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;625&lt;em&gt;_i&lt;/em&gt;&lt;em&gt;2/2 - 125&lt;/em&gt;_i/2 + exp(x) - 5))⎠ + RootSum⎝161⋅z  + 115⋅z  + 19⋅z + &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1, Lambda(_i, _i&lt;em&gt;log(-3381&lt;/em&gt;_i&lt;strong&gt;4/4 - 3381*_i&lt;/strong&gt;3/4 - 625&lt;em&gt;_i&lt;/em&gt;&lt;em&gt;2/2 - 125&lt;/em&gt;_i/2 +&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;         &lt;span class="err"&gt;⎞&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;exp(x) - 5))⎠&lt;/p&gt;
&lt;p&gt;In [2]: cancel(risch_integrate(1/(exp(5*x) + exp(x) + 1), x).diff(x))&lt;/p&gt;
&lt;p&gt;Out[2]: 
      1    &lt;br&gt;
─────────────
 5⋅x    x  &lt;br&gt;
ℯ    + ℯ  + 1&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;The general definition of the logarithmic part of an integral is a sum over the roots of a polynomial, which must be expressed as a &lt;code&gt;RootSum&lt;/code&gt; in the general case.  Previously, &lt;code&gt;RootSum.diff&lt;/code&gt; did not work, but thanks to Mateusz, an algorithm for computing exactly the RootSum where the Lambda expression is a rational function was implemented (see &lt;a href="http://mattpap.github.com/scipy-2011-tutorial/html/mathematics.html#summing-roots-of-polynomials" target="_blank"&gt;this bit&lt;/a&gt; from our SciPy tutorial for an idea on how this works), so now the Risch Algorithm can work with RootSum objects just as well with as an ordinary sum of logarithms.&lt;/p&gt;
&lt;p&gt;Also, there was a bug in the square free algorithm in my branch that was fixed in &lt;code&gt;master&lt;/code&gt; that was causing wrong results (I don't remember the expression that produced them right now), and also there was a fix by me in &lt;code&gt;master&lt;/code&gt; to make &lt;code&gt;is_rational_function()&lt;/code&gt; faster, as it was significantly slowing down the calculation of some integrals (for example, &lt;code&gt;risch_integrate(Add(&lt;em&gt;(exp(i&lt;/em&gt;x) for i in range(1000))))&lt;/code&gt;, which is still slow to calculate, but now it's because of other things).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About big branches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So this merge, along with the poly12 fiasco (which by the way, I think part of the reason git made me do all these merge conflict resolutions was because &lt;code&gt;polys12&lt;/code&gt; was rebased from the &lt;code&gt;polys11&lt;/code&gt; I had merged into integration3), has shown me very clearly that it is very bad to go off with your own branch and do a lot of work and wait a long time before merging it back into the main repo.&lt;/p&gt;
&lt;p&gt;This is what was done with &lt;code&gt;polys12&lt;/code&gt;.  Mateusz had a lot of new polynomials code that he developed in one big branch, and when it finally came to merging it back in, it was a mess.  This was for several reasons, which I do not want to discuss too much here, but it became clear to everyone I think that doing this was bad, and that it would have been better to have submitted many changes as pull requests as they were made than keeping them all together in one big branch for a long time.&lt;/p&gt;
&lt;p&gt;This model also affected my work, as I had to work off of latest the polys branch, not &lt;code&gt;master&lt;/code&gt;, as my work relied heavily on the latest and greatest in the polys.  &lt;/p&gt;
&lt;p&gt;Well, with this merge of the main repo into my branch, I see that my branch is starting to become the same way.  I originally thought that I should finish the Risch algorithm before submitting it to be merged into &lt;code&gt;master&lt;/code&gt;.  I know know that this is the wrong approach.  Development in &lt;code&gt;master&lt;/code&gt; is too fast to keep code away from it for too long.  The divergence makes it more and more difficult to merge back with every time.  Furthermore, there are regressions that were never noticed to be regressions because the code that would have shown them existed only in my branch.  Now I have to fix these, whereas if the code were in &lt;code&gt;master&lt;/code&gt;, the regression would have never happened in the first place, because the author would have seen it immediately from the test failures.&lt;/p&gt;
&lt;p&gt;I also thought that I should wait to merge because there were so many bugs in my code.  But I see now that this is also wrong.  Merging with &lt;code&gt;master&lt;/code&gt; will help me find these bugs, as people will actually use my code.  Sure, I've asked people to try out &lt;code&gt;risch_integrate()&lt;/code&gt;, and some people have (and I thank you), but having it in the default &lt;code&gt;integrate()&lt;/code&gt; in &lt;code&gt;master&lt;/code&gt; will result in finding more bugs in the code than I ever would alone, which is basically the way it is right now with the code living only in my own branch.&lt;/p&gt;
&lt;p&gt;I would prepare my code for merging with &lt;code&gt;master&lt;/code&gt; today, if it weren't for this &lt;code&gt;exp.subs&lt;/code&gt; hack, which causes test failures and is technically a regression, but is required for the preparsing code to the Risch algorithm to work.  This is why I &lt;a href="http://groups.google.com/group/sympy/browse_thread/thread/4a19d0f39f51fda6#" target="_blank"&gt;wrote to the list&lt;/a&gt; two weeks ago asking for advice on how to structure the substitution code so that we can nicely have various kinds of substitutions (e.g., exact like I need and algebraic like currently exists in &lt;code&gt;exp&lt;/code&gt;) living together without cluttering up the code.&lt;/p&gt;
&lt;p&gt;Therefore, I am going to focus my energies on fixing this subs problem so I can get my code merged with &lt;code&gt;master&lt;/code&gt;.  Then, when this is done, I will continue my work on implementing the remaining cases of the Risch algorithm.  &lt;/p&gt;
&lt;p&gt;So let this tale be a warning to people working on a lot of code in a big branch.  This especially applies to our GSoC students, as it's extremely easy to let your code accumulate when you're a GSoC student (technically this branch of mine is a GSoC branch).  I see that some of our students are doing a better job of this than others.  To those who have your code all in one big branch that hasn't been merged, I recommend you ready your branch for merge now.  And in the future, try to break your code up into small but still meaningful chunks and submit those as pull requests.  With git, it's easy to base the code you are currently working on on code that hasn't been merged yet, while still keeping things in small chunks for the pull requests.  &lt;/p&gt;
&lt;p&gt;On the other hand, git will only take you so far if you keep everything in a big branch, because there are going to be changes in &lt;code&gt;master&lt;/code&gt; that will affect your work, no matter how isolated you think it is, and these are the sorts of things that it is impossible for git to fix for you.  But if your code is in &lt;code&gt;master&lt;/code&gt;, it will be supported by everyone, and any major change that affects it will have to fix it. For example, if someone changes a printer and the doctests change, then he will have to change your doctest too if it's in &lt;code&gt;master&lt;/code&gt;, but if it's in your branch, then you will have to fix it when you next merge/rebase with/against &lt;code&gt;master&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2011/07/25/merging-integration3-with-sympy-0-7-0-nightmare.html</guid><pubDate>Mon, 25 Jul 2011 09:25:13 GMT</pubDate></item><item><title>Nondeterminism</title><link>http://asmeurersympy.wordpress.com/posts/2011/06/05/nondeterminism.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;So from Saturday to Wednesday of this week, I was on vacation to the Grand Canyon without my computer.  Therefore, I did not do a whole lot with respect to SymPy this week.  The vacation was very fun, though.  My family and I hiked to the bottom of the Grand Canyon and stayed a day at the bottom in a lodge at Phantom Ranch, then hiked back up.  I would highly recommend it to anyone who does not mind doing a little hiking.  &lt;/p&gt;
&lt;p&gt;Regarding what I did do, other than catching up on the email from when I was gone, I did some more work finishing patches for the release.  We are now &lt;em&gt;very&lt;/em&gt; close to having a release.  All the remaining &lt;a href="http://code.google.com/p/sympy/issues/list?q=label:Milestone-Release0.7.0"&gt;blocking issues&lt;/a&gt; either have patches that need to be reviewed, or decisions that need to be made.&lt;/p&gt;
&lt;p&gt;I also did some work on the Risch Algorithm, though it wasn't very much.  One of my favorite ways to "do work" on the code is to stress test &lt;code&gt;risch_integrate()&lt;/code&gt; and if I find a bug or find that it runs too slow, see what needs to be done to fix it.  This week, I discovered that &lt;code&gt;risch_integrate()&lt;/code&gt; has a bit of nondeterminism built into it.  Actually, I already knew this, but I recently found an example that demonstrates it very nicely.  The problem is that when it builds the extension to integrate the function, &lt;code&gt;risch_integrate()&lt;/code&gt; uses &lt;code&gt;.atoms()&lt;/code&gt; to get the parts of the expression (for example, &lt;code&gt;expr.atoms(log)&lt;/code&gt; gets all the logarithms in &lt;code&gt;expr&lt;/code&gt;).  But &lt;code&gt;.atoms()&lt;/code&gt; returns a set (I believe this is for performance reasons, though I'm not certain).  So we get things like&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hover over the code and click on the left-most, "view source" icon (a paper icon with &lt;tt&gt;&amp;lt; &amp;gt;&lt;/tt&gt; over it) to view without breaks.  Opens in a new window.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: a = Add(&lt;em&gt;(log(x&lt;/em&gt;*i) for i in range(10)))&lt;/p&gt;
&lt;p&gt;In [2]: a&lt;/p&gt;
&lt;p&gt;Out[2]: 
            ⎛ 2⎞      ⎛ 3⎞      ⎛ 4⎞      ⎛ 5⎞      ⎛ 6⎞      ⎛ 7⎞      ⎛ 8⎞      ⎛ 9⎞
log(x) + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠ + log⎝x ⎠&lt;/p&gt;
&lt;p&gt;In [3]: b = risch_integrate(a, x)&lt;/p&gt;
&lt;p&gt;In [4]: b&lt;/p&gt;
&lt;p&gt;Out[4]: 
                ⎛ 4⎞
        45⋅x⋅log⎝x ⎠
-45⋅x + ────────────
             4    &lt;br&gt;
[/code]&lt;/p&gt;
&lt;p&gt;This is correct, since we have&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [5]: expand(b.diff(x) - a)&lt;/p&gt;
&lt;p&gt;Out[5]: 0&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;(remember that $latex \log{(x^n)}=n\log{(x)}$).  The integral can be expressed in terms of any of the logarithms in the expression.  It happens to be expressed in terms of $latex \log{(x^4)}$ because that happened to be the first one that came out of &lt;code&gt;a.atoms(log)&lt;/code&gt; during iteration.  This is problematic.  First, it's not exactly what is expected.  The ideal solution would be if the answer was written in terms of $latex \log{(x)}$.  &lt;/p&gt;
&lt;p&gt;But it's actually worse than that.  Like I mentioned, this is nondeterministic.  It depends on the order of iteration through a set, which is not guaranteed to be in any particular order.  Indeed, if I run the following in 32-bit Python 2.7 and again in 64-bit Python 2.7), the output is exactly the same except for &lt;code&gt;i&lt;/code&gt; = 64 to &lt;code&gt;i&lt;/code&gt; = 77.&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;for i in range(100):
    print risch_integrate(Add(&lt;em&gt;(log(x&lt;/em&gt;*j) for j in range(i))), x)
[/code]&lt;/p&gt;
&lt;p&gt;The actual output seems to follow a pattern, though it's had to discern exactly what it is.  The output for 32-bit is &lt;a href="https://gist.github.com/1008685" title="32-bit Python risch_integrate(logarithms)" target="_blank"&gt;https://gist.github.com/1008685&lt;/a&gt; and the output for 64-bit is &lt;a href="https://gist.github.com/1008684" title="64-bit Python risch_integrate(logarithms)" target="_blank"&gt;https://gist.github.com/1008684&lt;/a&gt; (sorry, I forgot to print &lt;code&gt;i&lt;/code&gt;; just subtract 4 from the line number).  &lt;/p&gt;
&lt;p&gt;So this has gotten me thinking about how to reduce nondeterminism.  Clearly, I need to sort the result of &lt;code&gt;.atoms()&lt;/code&gt;, or else &lt;code&gt;risch_integrate()&lt;/code&gt; might return a different (though equivalent) result on different platforms. Actually, I've seen &lt;code&gt;list(set)&lt;/code&gt; return a different result in the &lt;em&gt;same&lt;/em&gt; Python session.  That means that you could potentially get something like &lt;code&gt;risch_integrate(expr, x) == risch_integrate(expr, x) =&amp;gt; False&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;The problem is how to sort the atoms.  We recently added a &lt;code&gt;sort_key()&lt;/code&gt; function that can be passed as a key to &lt;code&gt;sorted()&lt;/code&gt;, which is completely deterministic and platform independent.  That would solve the determinism problem, but actually, I think this requires more thought.  The order that the differential extension is built in can affect not only the form of the resulting antiderivative (though it will always be equivalent, up to a constant), but also the speed with which it is computed.  To take an example from &lt;a href="http://code.google.com/p/sympy/issues/detail?id=2010#c1"&gt;issue 2010&lt;/a&gt;, the issue about &lt;code&gt;risch_integrate()&lt;/code&gt; (you may also &lt;a href="http://asmeurersympy.wordpress.com/2010/08/05/prototype-risch_integrate-function-ready-for-testing/"&gt;recognize&lt;/a&gt; this example if you are a regular reader of this blog), the &lt;code&gt;handle_first&lt;/code&gt; keyword argument to &lt;code&gt;risch_integrate()&lt;/code&gt; affects if it builds the extension tower looking for logarithms first or exponentials first.  Whichever comes last is what is integrated first (the tower is integrated from the top to the bottom).  If the last extension was an exponential, then it uses the exponential algorithm.  If it was a logarithm, then it uses the logarithm algorithm.  These are completely different algorithms, and indeed the results can appear in different forms (and sometimes, one will raise NotImplementedError while the other will work because I have implemented the exponential algorithm more completely than the logarithmic one).  It also affects the speed because the integrand might be of a different "type" in the different extensions.  In the example below, the answers are different because it tries to make the argument of the logarithmic part monic with respect to the exponential or the logarithm, respectively. Also notice the speed difference.  This can be exasperated more for integrands of different forms than this one.&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: f = (x&lt;em&gt;(x + 1)&lt;/em&gt;((x&lt;strong&gt;2&lt;em&gt;exp(2&lt;/em&gt;x&lt;/strong&gt;2) - log(x + 1)&lt;strong&gt;2)&lt;/strong&gt;2 +
   ...: 2&lt;em&gt;x&lt;/em&gt;exp(3&lt;em&gt;x&lt;strong&gt;2)&lt;em&gt;(x - (2&lt;/em&gt;x&lt;/strong&gt;3 + 2&lt;/em&gt;x&lt;strong&gt;2 + x + 1)&lt;em&gt;log(x + 1))))/((x +
   ...: 1)&lt;/em&gt;log(x + 1)&lt;/strong&gt;2 - (x&lt;strong&gt;3 + x&lt;/strong&gt;2)&lt;em&gt;exp(2&lt;/em&gt;x&lt;strong&gt;2))&lt;/strong&gt;2&lt;/p&gt;
&lt;p&gt;In [2]: f&lt;/p&gt;
&lt;p&gt;Out[2]: 
          ⎛                          2                                                   ⎞
          ⎜⎛                       2⎞                                                   2⎟
          ⎜⎜     2           2  2⋅x ⎟        ⎛    ⎛           2      3⎞           ⎞  3⋅x ⎟
x⋅(1 + x)⋅⎝⎝- log (1 + x) + x ⋅ℯ    ⎠  + 2⋅x⋅⎝x - ⎝1 + x + 2⋅x  + 2⋅x ⎠⋅log(1 + x)⎠⋅ℯ    ⎠&lt;/p&gt;
&lt;p&gt;──────────────────────────────────────────────────────────────────────────────────────────
                                                                2                       &lt;br&gt;
                         ⎛                                    2⎞                        &lt;br&gt;
                         ⎜   2                  ⎛ 2    3⎞  2⋅x ⎟                        &lt;br&gt;
                         ⎝log (1 + x)⋅(1 + x) - ⎝x  + x ⎠⋅ℯ    ⎠                          &lt;/p&gt;
&lt;p&gt;In [3]: risch_integrate(f, x, handle_first='log')&lt;/p&gt;
&lt;p&gt;Out[3]: 
       ⎛              ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞                           &lt;br&gt;
       ⎜log(1 + x)    ⎝x ⎠⎟                   ⎜  log(1 + x)    ⎝x ⎠⎟          ⎛ 2⎞             &lt;br&gt;
    log⎜────────── + ℯ    ⎟                log⎜- ────────── + ℯ    ⎟       2  ⎝x ⎠             &lt;br&gt;
       ⎝    x             ⎠                   ⎝      x             ⎠      x ⋅ℯ    ⋅log(1 + x)  &lt;br&gt;
x + ─────────────────────── - log(1 + x) - ───────────────────────── + ──────────────────────────
               2                                       2                                        2
                                                                              2           3  2⋅x 
                                                                       - x⋅log (1 + x) + x ⋅ℯ    &lt;/p&gt;
&lt;p&gt;In [4]: risch_integrate(f, x, handle_first='exp')&lt;/p&gt;
&lt;p&gt;Out[4]: 
       ⎛                ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞        ⎛ 2⎞           &lt;br&gt;
       ⎜                ⎝x ⎠⎟                   ⎜                ⎝x ⎠⎟        ⎝x ⎠           &lt;br&gt;
    log⎝log(1 + x) + x⋅ℯ    ⎠                log⎝log(1 + x) - x⋅ℯ    ⎠     x⋅ℯ    ⋅log(1 + x)&lt;br&gt;
x + ───────────────────────── - log(1 + x) - ───────────────────────── - ──────────────────────
                2                                        2                                    2
                                                                            2           2  2⋅x 
                                                                         log (1 + x) - x ⋅ℯ    &lt;/p&gt;
&lt;p&gt;In [5]: %timeit risch_integrate(f, x, handle_first='log')&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 1.49 s per loop&lt;/p&gt;
&lt;p&gt;In [6]: %timeit risch_integrate(f, x, handle_first='exp')&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 1.21 s per loop&lt;/p&gt;
&lt;p&gt;In [7]: cancel(risch_integrate(f, x, handle_first='log').diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[7]: 0&lt;/p&gt;
&lt;p&gt;In [8]: cancel(risch_integrate(f, x, handle_first='exp').diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[8]: 0&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;So what I think I really need to do is to do some research on what order of building the tower makes it the most efficient.  Also, &lt;code&gt;handle_first&lt;/code&gt; needs to be modified to be more dynamic than just looking at exponentials or logarithms first, but also considering which exponentials or logarithms to look at first, and the others might be rewritten in terms of those (this needed to be done anyway to make it work for three types of extensions: exponentials, logarithms, and tangents).  &lt;/p&gt;
&lt;p&gt;There can also be more heuristics for this.  Currently, there are heuristics for exponentials to prefer rewriting $latex e^{2x}$ as $latex \left({e^{x}}\right)^2$ instead of rewriting $latex e^{x}$ as $latex \sqrt{e^{2x}}$ (this is necessary not only for keeping things in terms of the nicer looking gcds but also because &lt;code&gt;risch_integrate()&lt;/code&gt; doesn't know how to handle algebraic extensions like square roots). I didn't realize it at the time, but the corollary heuristic for logarithms should try to rewrite $latex \log{(x^2)}$ in terms of $latex \log{(x)}$ and not the other way around.  We can use the exact same gcd algorithm (called &lt;a href="https://github.com/asmeurer/sympy/blob/integration3/sympy/integrals/risch.py#L44"&gt;&lt;code&gt;integer_powers()&lt;/code&gt;&lt;/a&gt; in &lt;code&gt;risch.py&lt;/code&gt;, and I now realize that it should actually be called &lt;code&gt;integer_multiples()&lt;/code&gt;) as we do for the exponential, only use the powers of the arguments instead of coefficients.  This might require some factorization to do completely correctly, so it certainly requires some thought.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I discovered that there's an easier way to show the nondeterminism of the above than running it on different architectures.  You just have to change the variable of integration:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: a = Add(&lt;em&gt;(log(x&lt;/em&gt;*i) for i in range(10)))&lt;/p&gt;
&lt;p&gt;In [2]: risch_integrate(a, x)&lt;/p&gt;
&lt;p&gt;Out[2]: 
                ⎛ 4⎞
        45⋅x⋅log⎝x ⎠
-45⋅x + ────────────
             4      &lt;/p&gt;
&lt;p&gt;In [3]: b = Add(&lt;em&gt;(log(y&lt;/em&gt;*i) for i in range(10)))&lt;/p&gt;
&lt;p&gt;In [4]: risch_integrate(b, y)&lt;/p&gt;
&lt;p&gt;Out[4]: -45⋅y + 45⋅y⋅log(y)&lt;/p&gt;
&lt;p&gt;In [5]: c = Add(&lt;em&gt;(log(z&lt;/em&gt;*i) for i in range(10)))&lt;/p&gt;
&lt;p&gt;In [6]: risch_integrate(c, z)&lt;/p&gt;
&lt;p&gt;Out[6]: 
                ⎛ 2⎞
        45⋅z⋅log⎝z ⎠
-45⋅z + ────────────
             2    &lt;br&gt;
[/code]&lt;/p&gt;
&lt;p&gt;Clearly the code for this needs to be doing some canonicalization. &lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2011/06/05/nondeterminism.html</guid><pubDate>Sun, 05 Jun 2011 06:08:21 GMT</pubDate></item><item><title>Update for the Beginning of the Summer</title><link>http://asmeurersympy.wordpress.com/posts/2011/05/26/update-for-the-beginning-of-the-summer.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;So the Google Summer of Code coding period officially started on Monday, and in solidarity with the students, I will be blogging once a week about various things.  Some of the posts will just be about what I have done that week.  Others will be continuations of my Risch Algorithm series of blog posts (see parts &lt;a href="http://asmeurersympy.wordpress.com/2010/06/11/integration-of-rational-functions/" target="_blank"&gt;0&lt;/a&gt;, &lt;a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/" target="_blank"&gt;1&lt;/a&gt;, &lt;a href="http://asmeurersympy.wordpress.com/2010/07/24/the-risch-algorithm-part-2-elementary-functions/" target="_blank"&gt;2&lt;/a&gt;, and &lt;a href="http://asmeurersympy.wordpress.com/2010/08/14/the-risch-algorithm-part-3-liouvilles-theorem/" target="_blank"&gt;3&lt;/a&gt;).  &lt;/p&gt;
&lt;p&gt;This week, I will do the former.  I have spend the past several weeks preparing for the release.  The main thing right now is to clear out &lt;a href="http://code.google.com/p/sympy/issues/list?can=2&amp;amp;q=Milestone%3DRelease0.7.0+&amp;amp;colspec=ID+Type+Status+Priority+Milestone+Owner+Summary+Stars&amp;amp;cells=tiles" target="_blank"&gt;the issues that are blocking the release&lt;/a&gt;.  I merged in a branch that included all of my polys related fixes from my integration3 branch. Along with similar branch from earlier that had some non-polys related fixes (like some fixes to the integrals), all of my fixes from integration3 not directly related to my implementation of the Risch Algorithm should no be in master. &lt;/p&gt;
&lt;p&gt;Once those issues are fixed, I should be ready to make a release candidate for the release.  The last release was over a year ago (March 2010), and we've racked up &lt;a href="https://github.com/sympy/sympy/wiki/Release-Notes-for-0.7.0" target="_blank"&gt;quite a few changes&lt;/a&gt; since then.  A few big ones are:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt; &lt;strong&gt;The new polys&lt;/strong&gt;.  This is (in my opinion) the biggest change.  Because of the new polys, everything is faster, and simplification is far more powerful than it was before.  This is for a few reasons.  The biggest reason is that the new polys allow polynomials in any kind of expression, not just Symbols.  This means that you can do things like factor the expression $latex \cos^2{x} + 2\cos{x} + 1$.  As you can imagine, many simplifications of complex expressions are nothing more than polynomial simplifications, where the polynomial is in some function.  
&lt;p&gt;In addition to this, the new polys have a much faster implementation, and if you have gmpy installed, it will use that and be even faster.  There are also several faster algorithms, like a faster algorithm for multivariate factorization, that have been implemented. These all lead to blazing fast simplification and polynomial minipulation in SymPy.&lt;/p&gt;&lt;/li&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;li&amp;gt;&amp;lt;strong&amp;gt;&lt;/span&gt;The Quantum Module&lt;span class="nt"&gt;&amp;lt;/strong&amp;gt;&lt;/span&gt;.  Unfortunatly, I can't say much about this, since I don't know anything about quantum physics.  Furthermore, at the time of the writing of this blog post, that part of the release notes hasn't been written yet.  Suffice it say that thanks to two GSoC projects from last summer (see &lt;span class="nt"&gt;&amp;lt;a&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://code.google.com/p/sympy/wiki/SymbolicQMReport"&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;"_blank"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;this&lt;span class="nt"&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; and &lt;span class="nt"&gt;&amp;lt;a&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://code.google.com/p/sympy/wiki/Quantum_Computation_Report"&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;"_blank"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;this&lt;span class="nt"&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; page), we now have a quantum physics module.  A lot of the stuff in that module, from my understanding, is unique to SymPy, which is very exciting.  (By the way, if you're interested in this, Brian Granger can tell you more about it).&lt;span class="nt"&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;li&amp;gt;&amp;lt;strong&amp;gt;&lt;/span&gt;Various backwards incompatible changes&lt;span class="nt"&gt;&amp;lt;/strong&amp;gt;&lt;/span&gt;.  We've taken advantage of the fact that this will be a point release (0.7.0) to clean up some old cruft.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;
    &lt;li&gt;We've renamed the functions &lt;code&gt;abs()&lt;/code&gt; and &lt;code&gt;sum()&lt;/code&gt; to &lt;code&gt;Abs()&lt;/code&gt; and &lt;code&gt;summation()&lt;/code&gt;, respectively, because they conflicted with built-in names (although thanks to &lt;code&gt;&lt;strong&gt;abs&lt;/strong&gt;&lt;/code&gt; magic, &lt;code&gt;abs(expr)&lt;/code&gt; will still work with the built-in &lt;code&gt;abs()&lt;/code&gt; function).  &lt;/li&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;li&amp;gt;&lt;/span&gt;This will be the last release to support Python 2.4.  This will be a big benefit to not have to support Python 2.4 anymore after this release.  There were &lt;span class="nt"&gt;&amp;lt;a&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"http://docs.python.org/whatsnew/2.5.html"&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;"_blank"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;a ton of features&lt;span class="nt"&gt;&amp;lt;/a&amp;gt;&lt;/span&gt; added in Python 2.5 that we have had to either manually re-implement (like any() and all()), or have had to do without (like the with statement).    Also, this will make porting to Python 3 much easier (this is one of our GSoC projects).  &lt;span class="nt"&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;



&lt;span class="nt"&gt;&amp;lt;li&amp;gt;&lt;/span&gt;We split the class Basic, which is the base class of all SymPy types, into Basic and a subclass Expr.  Mathematical objects like &lt;span class="nt"&gt;&amp;lt;code&amp;gt;&lt;/span&gt;cos(x)&lt;span class="nt"&gt;&amp;lt;/code&amp;gt;&lt;/span&gt; or &lt;span class="nt"&gt;&amp;lt;code&amp;gt;&lt;/span&gt;x*y*z**2&lt;span class="nt"&gt;&amp;lt;/code&amp;gt;&lt;/span&gt; are instances of Expr.  Objects that do not make sense in mathematical expressions, but still want to have some of the standard SymPy methods like .args and .subs() are Basic.  For example, a Set object is Basic, but not Expr.&lt;span class="nt"&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/li&amp;gt;&lt;/span&gt;



&lt;span class="nt"&gt;&amp;lt;li&amp;gt;&amp;lt;strong&amp;gt;&lt;/span&gt;Lots of little bug fixes and new features&lt;span class="nt"&gt;&amp;lt;/strong&amp;gt;&lt;/span&gt;.  See the &lt;span class="nt"&gt;&amp;lt;a&lt;/span&gt; &lt;span class="na"&gt;href=&lt;/span&gt;&lt;span class="s"&gt;"https://github.com/sympy/sympy/wiki/Release-Notes-for-0.7.0"&lt;/span&gt; &lt;span class="na"&gt;target=&lt;/span&gt;&lt;span class="s"&gt;"_blank"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;release notes&lt;span class="nt"&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;.&lt;span class="nt"&gt;&amp;lt;/li&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/ul&gt;

&lt;p&gt;Once we have the release out, I plan to go back to work on the Risch Algorithm.  I am very close to finishing the exponential case, which means that once I do, any transcendental elementary function built up of only exponential extensions could be integrated or proven not to have an elementary integral by my algorithm.  I also want to start getting the code ready to merge with the main code base, so that it can go in the next release (0.7.1).  &lt;/p&gt;
&lt;p&gt;Finally, I want to announce that I have been selected for a &lt;a href="http://conference.scipy.org/scipy2011/student.php" target="_blank"&gt;student sponsorship&lt;/a&gt; to the SciPy 2011 conference in Austin, TX in the week of July 11. Mateusz and I will be presenting a tutorial on SymPy.  This will be the first time I have ever attended a conference, and I am very excited.  &lt;/p&gt;&lt;/ul&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2011/05/26/update-for-the-beginning-of-the-summer.html</guid><pubDate>Thu, 26 May 2011 05:41:50 GMT</pubDate></item><item><title>Major API Change for the Risch Algorithm Functions</title><link>http://asmeurersympy.wordpress.com/posts/2010/12/27/major-api-change-for-the-risch-algorithm-functions.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;I have been able to get to work again on the Risch Algorithm now that I have a month winter break from classes.  So the first thing I did was commit a bunch of bug fixes that had been sitting there since the end of the summer.  Then, I set out to make a major internal API change to the entire Risch Algorithm.&lt;/p&gt;
&lt;p&gt;Let me give some background.  When I first started programming the Risch Algorithm at the beginning of the summer, I didn't have a very good idea of how differential extensions worked yet (remember that I programmed the algorithm as I learned it from Bronstein's book).  Let me use the function &lt;code&gt;derivation()&lt;/code&gt; to demonstrate how the API has changed.  &lt;code&gt;derivation()&lt;/code&gt; takes the Poly &lt;code&gt;p&lt;/code&gt; in &lt;code&gt;t&lt;/code&gt; and computes the derivative (&lt;code&gt;t&lt;/code&gt; is some transcendental extension, like $latex e^x$).  Also, the integration variable is &lt;code&gt;x&lt;/code&gt;.  The first internal API that I used was&lt;/p&gt;
&lt;p&gt;&lt;code&gt;derivation(p, D, x, t)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;D&lt;/code&gt; is a Poly of the derivative of &lt;code&gt;t&lt;/code&gt;, and &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt; are Symbols (see &lt;a href="https://github.com/asmeurer/sympy/commit/0f6a3d90f724118fadc5fdaf290a0cb3e3963efd"&gt;this commit&lt;/a&gt;).   The problem here is that &lt;code&gt;p&lt;/code&gt; might not be in just one symbol, &lt;code&gt;t&lt;/code&gt;, but in many. This would happen whenever the function had more than one transcendental function, or extension, in it.  So, for example, $latex e^x\log{x}$ would have this problem. Surprisingly, according to the git log, it took me until July 4 to figure this out (that above linked commit, which is the first occurrence of this function and when I started the full algorithm, dates from June 7, so it took me almost a month!), after which I had already written a good portion of the Risch Algorithm.  I changed the API to&lt;/p&gt;
&lt;p&gt;&lt;code&gt;derivation(p, D, x, T)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;T&lt;/code&gt; is a list of the extension variables and &lt;code&gt;D&lt;/code&gt; is a list of the derivations of the respective elements of &lt;code&gt;T&lt;/code&gt; with respect to the lower elements and x (see &lt;a href="https://github.com/asmeurer/sympy/commit/20b7a5f8ca8dec579065f85583f11cc0955b96f0"&gt;this commit&lt;/a&gt;).  Now, the derivation of &lt;code&gt;x&lt;/code&gt; is always &lt;code&gt;Poly(1, x)&lt;/code&gt;, so I didn't think it was necessary to include it.  But it turns out that it is easier to just always include this in &lt;code&gt;D&lt;/code&gt; rather than try to special case it in the code.  Also, the lowest extension variable, &lt;code&gt;x&lt;/code&gt;, isn't used very often in the code, so it also doesn't make much sense to keep it separate from the rest of the variables in &lt;code&gt;T&lt;/code&gt;.  Now this didn't take me as long to figure out (July 11).  Therefore, I changed the API to just&lt;/p&gt;
&lt;p&gt;&lt;code&gt;derivation(p, D, T)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where the first element of &lt;code&gt;T&lt;/code&gt; is always &lt;code&gt;x&lt;/code&gt; and the first element of &lt;code&gt;D&lt;/code&gt; is always &lt;code&gt;Poly(1, x)&lt;/code&gt; (see &lt;a href="https://github.com/asmeurer/sympy/commit/bca2b19844ae71aa1ef8e27a9f77eabb70b4aa5f"&gt;this commit&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Now this API worked quite well for the remainder of the summer.  However, at the very end, I discovered that a function required to handle some special cases in certain parts of the algorithm needed four more lists (the elements of the extension that are logarithms, the elements of the extension that are exponentials, the arguments of those logarithms, and the arguments of those exponentials).  I had previously thought that these lists would only be needed when creating the extension at the beginning of integration, but it turned out that this was not the case and that they could be needed in several rather deep places in the algorithm.  The only way to get them there would be to pass them through to every single function in the algorithm.    &lt;/p&gt;
&lt;p&gt;So I was faced with a dilemma.  I didn't want to pass six arguments through each function just because a few might need them all.  I knew that the answer was to create an object to store all the data for a differential extension and to just pass this object around.  Unfortunately, this happened at the very end of the summer, so I hadn't been able to do that until now.  &lt;/p&gt;
&lt;p&gt;This brings us to now.  Over the past couple of weeks, I created an object called &lt;code&gt;DifferentialExtension&lt;/code&gt;, and replaced the API in the Risch Algorithm to use it.  See &lt;a href="https://github.com/asmeurer/sympy/commit/d9d9548625513188aaa663621bfe4e097aebf741"&gt;this commit&lt;/a&gt; and &lt;a href="https://github.com/asmeurer/sympy/commit/1935b6d6e1fdf8eae4deb5a4f56ea53c5d6989fa"&gt;this commit&lt;/a&gt; and some of the ones in between to see what I did. More or less, the object is like a C struct---it does little more than hold a lot of information as attributes.  However, at the suggestion of Ronan Lamy on the &lt;a href="http://groups.google.com/group/sympy/browse_thread/thread/a051b5ba1fb5cb4d"&gt;mailing list&lt;/a&gt;, I have moved all the relevant code for building the extension from the &lt;code&gt;build_extension()&lt;/code&gt; function into &lt;code&gt;DifferentialExtension.&lt;strong&gt;init&lt;/strong&gt;()&lt;/code&gt;.  I have also created some "magic" to handle the recursive nature of the algorithm.  A DifferentialExtension object has an attribute &lt;code&gt;level&lt;/code&gt;, which represents the level of the extension that the algorithm is working in.  So you can store all the derivations of the extension in &lt;code&gt;DifferentialExtension.D&lt;/code&gt;, but only have &lt;code&gt;DifferentialExtension.d&lt;/code&gt; point to the "current" outermost derivation.  This replaces things like&lt;/p&gt;
&lt;p&gt;&lt;code&gt;D = D[:-1]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;T = T[:-1]&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;from the old API to just&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DE.decrement_level()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;(and then later on, &lt;code&gt;DE.increment_level()&lt;/code&gt;).  The entire API is now just&lt;/p&gt;
&lt;p&gt;&lt;code&gt;derivation(p, DE)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;DE&lt;/code&gt; is a &lt;code&gt;DifferentialExtension&lt;/code&gt; object.  Changing the API of the entire code base at this point was a bit of work, but I have finally finished it, and I must say, this is much cleaner.  True, you now have to use &lt;code&gt;DE.t&lt;/code&gt; everywhere instead of &lt;code&gt;t&lt;/code&gt; (with &lt;code&gt;t = T[-1]&lt;/code&gt; at the top of the function), which is three characters more space for every use, but I think in the end it is cleaner.  For example, the function that used to be&lt;/p&gt;
&lt;p&gt;&lt;code&gt;is_log_deriv_k_t_radical(fa, fd, L_K, E_K, L_args, E_args, D, T)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;is now just&lt;/p&gt;
&lt;p&gt;&lt;code&gt;is_log_deriv_k_t_radical(fa, fd, DE)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Also, because it is an object, I can do cool things like override &lt;code&gt;DifferentialExtension.&lt;strong&gt;str&lt;/strong&gt;()&lt;/code&gt; to print out a tuple of the most important attributes of the object, making debugging much easier (now there is just one print statement instead of five).  &lt;/p&gt;
&lt;p&gt;Another thing I had to do was to allow the creation of these objects manually, because what is now &lt;code&gt;DifferentialExtension.&lt;strong&gt;init&lt;/strong&gt;()&lt;/code&gt; cannot yet handle, for example, tangent extensions, but some of the tests involve those.  So I created an &lt;code&gt;extension&lt;/code&gt; flag to &lt;code&gt;&lt;strong&gt;init&lt;/strong&gt;()&lt;/code&gt; to which you could pass a dictionary, and it would create a skeleton extension from that (see &lt;a href="https://github.com/asmeurer/sympy/commit/7121b06eab3f1e0f8464c287438fb7175f07762b"&gt;this commit&lt;/a&gt;).  I made it smart enough to create some attributes automatically, so I only have to pass the list &lt;code&gt;D&lt;/code&gt; in most tests---it creates attributes like &lt;code&gt;T&lt;/code&gt; from that automatically.  Thus, this in some ways made the tests a little simpler, because I didn't have to worry about &lt;code&gt;T&lt;/code&gt; any more.  &lt;/p&gt;
&lt;p&gt;We'll see how things go, but this fourth API change should hopefully be the last.  This should also make it much easier whenever I add trigonometric function support, where I will have to add even more attributes to the object.  I won't have to change the code in any existing function (unless it specifically needs to be able to know about trig extensions), because, to them, the information in &lt;code&gt;DE&lt;/code&gt; will not change.&lt;/p&gt;
&lt;p&gt;So the good news behind all of this, as I mentioned at the beginning of this post, is that I can now write some algorithm that requires those &lt;code&gt;L_K&lt;/code&gt;, &lt;code&gt;E_K&lt;/code&gt;, &lt;code&gt;L_args&lt;/code&gt;, &lt;code&gt;E_args&lt;/code&gt; variables from arbitrary places within the algorithm.  This should allow me to completely finish the exponential case.  So look forward soon to a &lt;code&gt;risch_integrate()&lt;/code&gt; that can handle completely any transcendental function of exponentials (either produce an integral or prove that no elementary integral exists).  &lt;/p&gt;
&lt;p&gt;And just to be clear, this doesn't change anything with &lt;code&gt;risch_integrate()&lt;/code&gt;---this is only an internal change. And at the moment, it doesn't add any features, though that should soon change. So keep on testing it for me!  If you see any errors along the lines of "Variable t not defined," it probably means that I missed that one when I was switching the API due to poor test coverage in that area of the code.  I would love to know about any errors you find, or, indeed, any testing you do with &lt;code&gt;risch_integrate&lt;/code&gt;.  Remember that you can obtain my branch at &lt;a href="https://github.com/asmeurer/sympy/tree/integration3"&gt;my GitHub account (branch integration3)&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2010/12/27/major-api-change-for-the-risch-algorithm-functions.html</guid><pubDate>Mon, 27 Dec 2010 07:34:30 GMT</pubDate></item><item><title>The Risch Algorithm: Part 3, Liouville's Theorem</title><link>http://asmeurersympy.wordpress.com/posts/2010/08/14/the-risch-algorithm-part-3-liouvilles-theorem.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;So this is the last official week of the Summer of Code program, and my work is mostly consisting of removing &lt;code&gt;NotImplementedError&lt;/code&gt;s (i.e., implementing stuff), and fixing bugs. None of this is particularly interesting, so instead of talking about that, I figured I would produce another one of my Risch Algorithm blog posts.  It is recommended that you read parts &lt;a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/"&gt;1&lt;/a&gt; and &lt;a href="http://asmeurersympy.wordpress.com/2010/07/24/the-risch-algorithm-part-2-elementary-functions/"&gt;2&lt;/a&gt; first, as well as my post on &lt;a href="http://asmeurersympy.wordpress.com/2010/06/11/integration-of-rational-functions/"&gt;rational function integration&lt;/a&gt;, which could be considered part 0.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liouville's Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anyone who's taken calculus intuitively knows that integration is hard, while differentiation is easy.  For differentiation, we can produce the derivative of any elementary function, and we can do so easily, using a simple algorithm consisting of the sum and product rules, the chain rule, and the rules for the derivative of all the various elementary functions.  But for integration, we have to try to work backwards.  &lt;/p&gt;
&lt;p&gt;There are two things that make integration difficult.  First is the existence of functions that simply do not have any elementary antiderivative.  $latex e^{-x^2}$ is perhaps the most famous example of such a function, since it arises from the normal distribution in statistics.  But there are many others.  $latex \sin{(x^2)}$, $latex \frac{1}{\log{(x)}}$, and $latex x^x$ are some other examples of famous non-integrable functions.  &lt;/p&gt;
&lt;p&gt;The second problem is that no one single simple rule for working backwards will always be applicable.  We know that u-substitution and integration by parts are the reverse of the chain rule and the product rule, respectively.  But those methods will only work if those rules were the ones that were applied originally, and then only if you chose the right $latex u$ and $latex dv$.  &lt;/p&gt;
&lt;p&gt;But there is a much simpler example that gets right down to the point with Liouville's theorem.  The power rule, which is that $latex \frac{d}{dx}x^n=nx^{n-1}$ is easily reversed for integration.  Given the power rule for differentiation, it's easy to see that the reverse rule should be $latex \int{x^ndx}=\frac{x^{n+1}}{n+1}$.  This works fine, except that were are dividing something, $latex n+1$.  In mathematics, whenever we do that, we have to ensure that whatever we divide by is not 0. In this case, it means that we must assert $latex n\neq -1$.  This excludes $latex \int{\frac{1}{x}dx}$.  We know from calculus that this integral requires us to introduce a special function, the natural logarithm.  &lt;/p&gt;
&lt;p&gt;But we see that $latex n=-1$ is the only exception to the power rule, so that the integral of any (&lt;a href="http://en.wikipedia.org/wiki/Laurent_polynomial"&gt;Laurent&lt;/a&gt;) polynomial is again a (Laurent) polynomial, plus a logarithm.  Recall from part 0 (&lt;a href="http://asmeurersympy.wordpress.com/2010/06/11/integration-of-rational-functions/"&gt;Rational Function Integration&lt;/a&gt;) that the same thing is true for any rational function: the integral is again a rational function, plus a logarithm (we can combine multiple logarithms into one using the logarithmic identities, so assume for simplicity that there is just one).  The argument is very similar, too.  Assume that we have split the denominator rational function into linear factors in the &lt;a href="http://en.wikipedia.org/wiki/Algebraic_splitting_field"&gt;algebraic splitting field&lt;/a&gt; (such as the complex numbers).  Then perform a partial fractions decomposition on the rational function.  Each term in the decomposition will be either a polynomial, or of the form $latex \frac{a}{(x - b)^n}$. The integration of these terms is the same as with the power rule, making the substitution $latex u = x - b$. When $latex n\geq 2$, the integral will be $latex \frac{-1}{n - 1}\frac{a}{(x - b)^{n - 1}}$; when $latex n = 1$, the integral will be $latex a\log{(x - b)}$.  Now computationally, we don't want to work with the algebraic splitting field, but it turns out that we don't need to actually compute it to find the integral.  But theory is what we are dealing with here, so don't worry about that.  &lt;/p&gt;
&lt;p&gt;Now the key observation about differentiation, as I have pointed out in the earlier parts of this blog post series,  is that the derivative of an elementary function can be expressed in terms of itself, in particular, as a polynomial in itself.  To put it another way, functions like $latex e^x$, $latex \tan{(x)}$, and $latex \log{(x)}$ all satisfy linear differential equations with rational coefficients (e.g., for these, $latex y'=y$, $latex y'=1 + y^2$, and $latex y'=\frac{1}{x}$).  &lt;/p&gt;
&lt;p&gt;Now, the theory gets more complicated, but it turns out that, using a careful analysis of this fact, we can prove a similar result to the one about rational functions to any elementary function. In a nutshell, Liouville's Theorem says this:  if an elementary function has an elementary integral, then that integral is a composed only of functions from the original integrand, plus a finite number of logarithms of functions from the integrand, which can be considered one logarithm, as mentioned above ("functions from" more specifically means a rational function in the terms from our elementary extension).  Here is the formal statement of the theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem (Liouville's Theorem - Strong version)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Let $latex K$ be a differential field, $latex C=\mathrm{Const}(K)$, and $latex f\in K$. If there exist an elementary extension $latex E$ of $latex K$ and $latex g \in E$ such that $latex Dg =f$, then there are $latex v \in K$, $latex c_1, \dots, c_n\in \bar{C}$, and $latex u_1, \dots,u_n\in K(c_1,\dots,c_n)^*$ such that &lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;

$latex f = Dv + \sum_{i=1}^n c_i\frac{Du_i}{u_i}$.

&lt;/h2&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Looking closely at the formal statement of the theorem, we can see that it says the same thing as my "in a nutshell" statement.  $latex K$ is the differential extension, say of $latex \mathbb{Q}(x)$, that contains all of our elementary functions (see &lt;a href="http://asmeurersympy.wordpress.com/2010/07/24/the-risch-algorithm-part-2-elementary-functions/"&gt;part 2&lt;/a&gt;).  $latex E$ is an extension of $latex K$.  The whole statement of the theorem is that $latex E$ need not be extended from $latex K$ by anything more than some logarithms.   $latex f$ is our original function and $latex g=\int f$.  Recall from &lt;a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/"&gt;part 1&lt;/a&gt; that $latex Dg = \frac{Du}{u}$ is just another way of saying that $latex g = \log{(u)}$.  The rest of the formal statement is some specifics dealing with the constant field, which assure us that we do not need to introduce any new constants in the integration. This fact is actually important to the decidability of the Risch Algorithm, because many problems about constants are either unknown or undecidable (such as the transcendence degree of $latex \mathbb{Q}(e, \pi)$).  But this ensures us that as long as we start with a constant field that is computable, our constant field for our antiderivative will also be computable, and will in fact be the same field, except for some possible algebraic extensions (the $latex c_i$).  &lt;/p&gt;
&lt;p&gt;At this point, I want to point out that even though my work this summer has been only on the purely transcendental case of the Risch Algorithm, Liouville's Theorem is true for all elementary functions, which includes algebraic functions.  However, if you review the proof of the theorem, the proof of the algebraic part is completely different from the proof of the transcendental part, which is the first clue that the algebraic part of the algorithm is completely different from the transcendental part (and also a clue that it is harder).&lt;/p&gt;
&lt;p&gt;Liouville's Theorem is what allows us to prove that a given function does not have an elementary antiderivative, by giving us the form that any antiderivative must have.  We first perform the same Hermite Reduction from the &lt;a href="http://asmeurersympy.wordpress.com/2010/06/11/integration-of-rational-functions/"&gt;rational integration case&lt;/a&gt;. Then, a generalization of the same Lazard-Rioboo-Trager Algorithm due to Rothstein allows us to find the logarithmic part of any integral (the $latex \sum_{i=1}^n c_i\frac{Du_i}{u_i}$ from Liouville's Theorem).  &lt;/p&gt;
&lt;p&gt;Now a difference here is that sometimes, the part of the integrand that corresponds to the $latex \frac{a}{x - b}$ for general functions doesn't always have an elementary integral (these are called &lt;em&gt;simple&lt;/em&gt; functions.  I think I will talk about them in more detail in a future post in this series).   An example of this is $latex \frac{1}{\log{(x)}}$.  Suffice it to say that any elementary integral of $latex \frac{1}{\log{(x)}}$ must be part of some log-extension of $latex \mathbb{Q}(x, \log{(x)})$, and that we can prove that no such logarithmic extension exists in the course of trying to compute it with the Lazard-Rioboo-Rothstein-Trager Algorithm.&lt;/p&gt;
&lt;p&gt;In the rational function case, after we found the rational part and the logarithmic part, we were practically done, because the only remaining part was a polynomial.  Well, for the general transcendental function case, we are left with an analogue, which are called &lt;em&gt;reduced&lt;/em&gt; functions, and we are far from done.  This is the hardest part of the integration algorithm.  This will also be the topic of a future post in this series.  Suffice it to say that this is where most of the proofs of non-integrability come from, including the other integrals than $latex \frac{1}{\log{(x)}}$ that I gave above.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That's it for now.  Originally, I was also going to include a bit on the structure theorems too, but I think I am going to save that for part 4 instead.  I may or may not have another post ready before the official end of coding date for Google Summer of Code, which is Monday (three days from now).  I want to make a post with some nice graphs comparing the timings of the new &lt;code&gt;risch_integrate()&lt;/code&gt; and the old &lt;code&gt;heurisch()&lt;/code&gt; (what is currently behind SymPy's &lt;code&gt;integrate()&lt;/code&gt;).  But as I have said before, I plan on continuing coding the integration algorithm beyond the program until I finish it, and even beyond that (there are lots of cool ways that the algorithm can be extended to work with special functions, there's definite integration with Meijer-G functions, and there's of course the algebraic part of the algorithm, which is a much larger challenge).  And along with it, I plan to continue keeping you updated with blog posts, including at least all the Risch Algorithm series posts that I have promised (I have counted at least three topics that I have explicitly promised but haven't done yet).  And of course, there will be the mandatory GSoC wrap-up blog post, detailing my work for the summer.  &lt;/p&gt;
&lt;p&gt;Please continue to test my prototype &lt;a href="http://asmeurersympy.wordpress.com/2010/08/05/prototype-risch_integrate-function-ready-for-testing/"&gt;&lt;code&gt;risch_integrate()&lt;/code&gt;&lt;/a&gt; function in my &lt;a href="http://github.com/asmeurer/sympy/tree/integration3"&gt;integration3&lt;/a&gt; branch, and tell me what you think (or if you find a bug).&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2010/08/14/the-risch-algorithm-part-3-liouvilles-theorem.html</guid><pubDate>Sat, 14 Aug 2010 02:55:23 GMT</pubDate></item><item><title>Prototype risch_integrate() function ready for testing!</title><link>http://asmeurersympy.wordpress.com/posts/2010/08/05/prototype-risch_integrate-function-ready-for-testing.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;So today I finally finished up the prototype function I talked about &lt;a href="http://asmeurersympy.wordpress.com/2010/07/31/integration-of-primitive-functions/"&gt;last week&lt;/a&gt;.  The function is called &lt;code&gt;risch_integrate()&lt;/code&gt; and is available at my &lt;a href="http://github.com/asmeurer/sympy/tree/integration3"&gt;integration3&lt;/a&gt; branch.  Unlike the inner level functions I have showcased in &lt;a href="http://asmeurersympy.wordpress.com/2010/07/31/integration-of-primitive-functions/"&gt;previous&lt;/a&gt; &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;blog posts&lt;/a&gt;, this function does not require you to do substitution for dummy variables and manually create a list of derivatives, etc.  All you have to do is pass it a function and the integration variable, and it will return the result, just like normal &lt;code&gt;integrate()&lt;/code&gt;. I have spent the past few days working on a monster of a function called &lt;code&gt;build_extension()&lt;/code&gt; that does this preparsing work for you.  The reason that the function was so hard to write is that the transcendental Risch Algorithm is very picky.  &lt;em&gt;Every&lt;/em&gt; differential extension has to be transcendental over the previous extensions.  This means that if you have a function like $latex e^x + e^{\frac{x}{2}}$, you cannot write this as $latex t_0 + t_1$ with $latex t_0=e^x$ and $latex t_1=e^{\frac{x}{2}}$ because $latex t_0$ and $latex t_1$ will each be algebraic over the other ($latex t_0=t_1^2$).  You also cannot let $latex t_0=e^{x}$ and rewrite the whole integral in terms of $latex t_0$ because you will get $latex t_0 + \sqrt{t_0}$, which is an algebraic function.  The only way that you can do it is to let $latex t_0=e^{\frac{x}{2}}$, and then your function will be $latex t_0^2 + t_0$.  &lt;/p&gt;
&lt;p&gt;Now, fortunately, there is an algorithm that provides necessary and sufficient conditions for determining if an extension is algebraic over the previous ones.  It's called the Risch Structure Theorems.  My first order of business this week was to finish implementing these.  This is actually the reason that I we had to wait until now to get this prototype function.  The Structure Theorems are at the very end of Bronstein's book, and the integration algorithm is not correct without them (namely, it is not correct if you add an algebraic extension).  I just recently got to them in my reading.  Actually, I skipped some work on tangent integration so I could get to them first.  I hope to talk a little about them in a future "Risch Integration" blog post, though be aware that they require some extremely intense algebraic machinery to prove, so I won't be giving any proofs.&lt;/p&gt;
&lt;p&gt;Even though these algorithms can tell me, for example, that I shouldn't have added $latex t_0=e^x$ above because it makes $latex e^{\frac{x}{2}}=\sqrt{t_0}$, that means that I have to go back and restart my search for an extension so that I can try to get $latex t_0=e^{\frac{x}{2}}$ instead.  So I wrote a simple function that takes the arguments of the exponentials and determines the lowest common factor.  This heuristic saves a lot of time.  &lt;/p&gt;
&lt;p&gt;I also noticed (actually, Chris Smith inadvertently pointed it out to me; super thanks to him), that the Structure Theorem algorithms only tell you if the terms are the same as monomials.  It would tell you that $latex e^x = e^{x + 1}$ because both satisfy $latex Dt=t$.  Therefore, I had to also modify the structure theorem algorithms to pull out any constant term.  &lt;/p&gt;
&lt;p&gt;It can still be necessary to restart building the extension even with the above heuristic.  For example, if you have $latex e^x + e^{x^2} + e^{\frac{x}{2} + x^2}$, and start with $latex t_0=e^x$ and $latex t_1=e^{x^2}$, then the structure theorems will tell you that $latex e^{x/2 + x^2} = \sqrt{t_0}t_1$, which we cannot use because of the radical.  The solution it uses is to split it up as $latex e^x + e^{x^2} + e^{\frac{x}{2}}e^{x^2}$ (the structure theorems tell you exactly how to do this so you are splitting in terms of the other exponentials) and then restart the extension building entirely.  This can be an expensive operation, because you have to rebuild $latex t_0$ and $latex t_1$, but this time, the heuristic function I wrote from above handles the $latex e^{\frac{x}{2}}$ correctly, making $latex t_0=e^{\frac{x}{2}}$, with the final answer $latex t_0^2 + t_1 + t_0t_1$.  I could have probably made it smarter by only going back to before the conflicting extensions, but this was quite a bit more work, and adds more difficulties such as non-trivial relationships, so I just took the lazy way and restarted completely.  It doesn't take &lt;em&gt;that&lt;/em&gt; much time.  &lt;/p&gt;
&lt;p&gt;Of course, sometimes, you cannot add a new exponential, no matter how you add the extensions.  The classic example is $latex e^{\frac{\log{(x)}}{2}}$, which you can see is actually equal to $latex \sqrt{x}$, an algebraic function.  Therefore, I had to implement some tricky logic to keep the &lt;code&gt;build_extension()&lt;/code&gt; function from trying again infinitely.  I hope I did it right, so that it never infinite loops, and never fails when it really can be done.  Only time and testing will tell.&lt;/p&gt;
&lt;p&gt;It is exactly the same for logarithms, except in that case, when a new logarithm is algebraic in terms of old ones, it can be written as a linear combination of them.  This means that there are never any radicals to worry about, though you do also have to worry about constants.  For example, $latex \log{(x)}$ looks the same as $latex \log{(2x)}$ because they both satisfy $latex Dt=\frac{1}{x}$.  An example of a logarithm that is algebraic over old ones is $latex \log{(x^2 - 1)}$ over $latex \log{(x + 1)}$ and $latex \log{(x - 1)}$, because $latex \log{(x^2 - 1)}=\log{((x + 1)(x - 1))}=\log{(x + 1)} + \log{(x - 1)}$.  &lt;/p&gt;
&lt;p&gt;The parallels between exponentials and logarithms are amazing.  For the structure theorems, the exponential case is exactly the same as the logarithmic case except replacing addition with multiplication and multiplication with exponentiation.  For the exponential case, you need the arguments of the already added logarithms to find the algebraic dependence, and the arguments of the already added exponentials to find the constant term.  For the logarithmic case, you need the arguments of the already added exponentials to find the algebraic dependence, and the arguments of the already added logarithms to find the content term. Everything else is exactly the same, except for the shift in operators.  Of course, I realize why these things are, mathematically, but the symmetry still amazing to me.  I will hopefully explain in more detail in my future Structure Theorems post.  &lt;/p&gt;
&lt;p&gt;So onto the &lt;code&gt;risch_integrate()&lt;/code&gt; function.  Here is the text that I have basically put in my &lt;a href="http://github.com/asmeurer/sympy/commit/e3cd5f18f86fd6377836f33f726182c8bd4dc1a0"&gt;commit message&lt;/a&gt;, the &lt;a href="http://code.google.com/p/sympy/issues/detail?q=2010"&gt;aptly numbered issue&lt;/a&gt; that I have created for it, and the &lt;a href="http://groups.google.com/group/sympy/browse_thread/thread/2464fa764f6f47aa"&gt;post to the mailing list&lt;/a&gt; (it's not so much that I am lazy as that I was really excited to get this out there).&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;I have ready in my integration3 branch a prototype risch_integrate() function that is a user-level function for the full Risch Algorithm I have been implementing this summer.  Pull from h&lt;a href="//github.com/asmeurer/sympy/tree/integration3"&gt;ttp://github.com/asmeurer/sympy/tree/integration3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is NOT ready to go in.  It is a prototype function that I am making available so people can try out the new algorithm and hopefully help me to find the bugs in it.  Please pass it your favorite non-elementary integrals and see if it can determine that they are not elementary.  If you try to pass it a very crazy function at random, the chances are pretty high that it will not be elementary.  So a better way to test it is to come up with a crazy function, then differentiate it. Then pass the derivative and see if it can give you your original function back.  Note that it will probably not look exactly the same as your original function, and may differ by a constant.  You should verify by differentiating the result you get and calling cancel() (or simplify(), but usually cancel() is enough) on the difference.&lt;/p&gt;
&lt;p&gt;So you can review the code too, if you like, but just know that things are not stable yet, and this isn't strictly a branch for review.  &lt;/p&gt;
&lt;p&gt;So far, this function only supports exponentials and logarithms.&lt;/p&gt;
&lt;p&gt;Support for trigonometric functions is planned.  Algebraic functions are&lt;/p&gt;
&lt;p&gt;not supported. If the function returns an unevaluated Integral, it means&lt;/p&gt;
&lt;p&gt;that it has proven the integral to be non-elementary.  Note that several&lt;/p&gt;
&lt;p&gt;cases are still not implemented, so you may get NotImplementedError&lt;/p&gt;
&lt;p&gt;instead. Eventually, these will all be eliminated, and the only&lt;/p&gt;
&lt;p&gt;NotImplementedError you should see from this function is&lt;/p&gt;
&lt;p&gt;NotImplementedError("Algebraic extensions are not supported.")&lt;/p&gt;
&lt;p&gt;This function has not been integrated in any way with the already&lt;/p&gt;
&lt;p&gt;existing integrate() yet, and you can use it to compare.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: risch_integrate(exp(x**2), x)&lt;/p&gt;
&lt;p&gt;Out[1]:&lt;/p&gt;
&lt;p&gt;⌠&lt;/p&gt;
&lt;p&gt;⎮  ⎛ 2⎞&lt;/p&gt;
&lt;p&gt;⎮  ⎝x ⎠&lt;/p&gt;
&lt;p&gt;⎮ ℯ     dx&lt;/p&gt;
&lt;p&gt;⌡&lt;/p&gt;
&lt;p&gt;In [2]: risch_integrate(x*&lt;em&gt;100&lt;/em&gt;exp(x), x).diff(x)&lt;/p&gt;
&lt;p&gt;Out[2]:
 100  x
x   ⋅ℯ&lt;/p&gt;
&lt;p&gt;In [3]: %timeit risch_integrate(x*&lt;em&gt;100&lt;/em&gt;exp(x), x).diff(x)&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 270 ms per loop&lt;/p&gt;
&lt;p&gt;In [4]: integrate(x*&lt;em&gt;100&lt;/em&gt;exp(x), x)&lt;/p&gt;
&lt;p&gt;... hangs ...&lt;/p&gt;
&lt;p&gt;In [5]: risch_integrate(x/log(x), x)&lt;/p&gt;
&lt;p&gt;Out[5]:&lt;/p&gt;
&lt;p&gt;⌠&lt;/p&gt;
&lt;p&gt;⎮   x&lt;/p&gt;
&lt;p&gt;⎮ ────── dx&lt;/p&gt;
&lt;p&gt;⎮ log(x)&lt;/p&gt;
&lt;p&gt;⌡&lt;/p&gt;
&lt;p&gt;In [6]: risch_integrate(log(x)**10, x).diff(x)&lt;/p&gt;
&lt;p&gt;Out[6]:
   10
log  (x)&lt;/p&gt;
&lt;p&gt;In [7]: integrate(log(x)**10, x).diff(x)&lt;/p&gt;
&lt;p&gt;Out[7]:
   10
log  (x)&lt;/p&gt;
&lt;p&gt;In [8]: %timeit risch_integrate(log(x)**10, x).diff(x)&lt;/p&gt;
&lt;p&gt;10 loops, best of 3: 159 ms per loop&lt;/p&gt;
&lt;p&gt;In [9]: %timeit integrate(log(x)**10, x).diff(x)&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 2.35 s per loop&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Be warned that things are still very buggy and you should always verify&lt;/p&gt;
&lt;p&gt;results by differentiating.  Usually, cancel(diff(result, x) - result)&lt;/p&gt;
&lt;p&gt;should be enough.  This should go to 0.&lt;/p&gt;
&lt;p&gt;So please, please, PLEASE, try out this function and report any bugs that you find.  It is not necessary to report NotImplementedError bugs, because I already know about those (I put them in there), and as I mentioned above, they are all planned to disappear.  Also, I am continually updating my branch with fixes, so you should do a "git pull" and try again before you report anything.&lt;/p&gt;
&lt;p&gt;Also, I am aware that there are test failures.  This is because I had to hack exp._eval_subs() to only do exact substitution (no algebraic substitution).  It's just a quick hack workaround, and I should eventually get a real fix.  &lt;/p&gt;
&lt;p&gt;Finally, I'm thinking there needs to be a way to differentiate between an unevaluated Integral because the integrator failed and an unevaluated Integral because it has proven the integral to be non-elementary.  Any ideas?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also, looking at the integral from the previous blog post, you can get the different results by using the &lt;code&gt;handle_log&lt;/code&gt; argument to &lt;code&gt;risch_integrate()&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;handle_first == 'log'&lt;/code&gt; (the default right now), then it will gather all logarithms first, and then exponentials (insomuch as it can do it in that order).  If &lt;code&gt;handle_first='exp'&lt;/code&gt;, it gathers exponentials first.  The difference is that the Risch Algorithm integrates recursively, one extension at a time, starting with the outer-most one. So if you have an expression with both logarithms and exponentials, such that they do not depend on each other, &lt;code&gt;handle_first == 'log'&lt;/code&gt; will integrate the exponentials first, because they will be gathered last (be at the top of the tower of extensions), and &lt;code&gt;handle_first == 'exp'&lt;/code&gt; will integrate the logarithms first.  Right now, I have defaulted to 'log' because the exponential integration algorithm is slightly more complete.  If you get &lt;code&gt;NotImplementedError&lt;/code&gt; with one, it is possible (though I don't know for sure yet) that you might get an answer with the other.  &lt;/p&gt;
&lt;p&gt;Also, they can give different looking results, and at different speeds.  For example:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hover over the code and click on the left-most, "view source" icon (a paper icon with &lt;tt&gt;&amp;lt; &amp;gt;&lt;/tt&gt; over it) to view without breaks.  Opens in a new window.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: f = (x&lt;em&gt;(x + 1)&lt;/em&gt;((x&lt;strong&gt;2&lt;em&gt;exp(2&lt;/em&gt;x&lt;/strong&gt;2) - log(x + 1)&lt;strong&gt;2)&lt;/strong&gt;2 +
   ...: 2&lt;em&gt;x&lt;/em&gt;exp(3&lt;em&gt;x&lt;strong&gt;2)&lt;em&gt;(x - (2&lt;/em&gt;x&lt;/strong&gt;3 + 2&lt;/em&gt;x&lt;strong&gt;2 + x + 1)&lt;em&gt;log(x + 1))))/((x +
   ...: 1)&lt;/em&gt;log(x + 1)&lt;/strong&gt;2 - (x&lt;strong&gt;3 + x&lt;/strong&gt;2)&lt;em&gt;exp(2&lt;/em&gt;x&lt;strong&gt;2))&lt;/strong&gt;2&lt;/p&gt;
&lt;p&gt;In [2]: f&lt;/p&gt;
&lt;p&gt;Out[2]: 
          ⎛                          2                                                   ⎞
          ⎜⎛                       2⎞                                                   2⎟
          ⎜⎜     2           2  2⋅x ⎟        ⎛    ⎛           2      3⎞           ⎞  3⋅x ⎟
x⋅(1 + x)⋅⎝⎝- log (1 + x) + x ⋅ℯ    ⎠  + 2⋅x⋅⎝x - ⎝1 + x + 2⋅x  + 2⋅x ⎠⋅log(1 + x)⎠⋅ℯ    ⎠&lt;/p&gt;
&lt;p&gt;──────────────────────────────────────────────────────────────────────────────────────────
                                                                2                       &lt;br&gt;
                         ⎛                                    2⎞                        &lt;br&gt;
                         ⎜   2                  ⎛ 2    3⎞  2⋅x ⎟                        &lt;br&gt;
                         ⎝log (1 + x)⋅(1 + x) - ⎝x  + x ⎠⋅ℯ    ⎠                          &lt;/p&gt;
&lt;p&gt;In [3]: risch_integrate(f, x, handle_first='log')&lt;/p&gt;
&lt;p&gt;Out[3]: 
       ⎛              ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞                           &lt;br&gt;
       ⎜log(1 + x)    ⎝x ⎠⎟                   ⎜  log(1 + x)    ⎝x ⎠⎟          ⎛ 2⎞             &lt;br&gt;
    log⎜────────── + ℯ    ⎟                log⎜- ────────── + ℯ    ⎟       2  ⎝x ⎠             &lt;br&gt;
       ⎝    x             ⎠                   ⎝      x             ⎠      x ⋅ℯ    ⋅log(1 + x)  &lt;br&gt;
x + ─────────────────────── - log(1 + x) - ───────────────────────── + ──────────────────────────
               2                                       2                                        2
                                                                              2           3  2⋅x 
                                                                       - x⋅log (1 + x) + x ⋅ℯ    &lt;/p&gt;
&lt;p&gt;In [4]: risch_integrate(f, x, handle_first='exp')&lt;/p&gt;
&lt;p&gt;Out[4]: 
       ⎛                ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞        ⎛ 2⎞           &lt;br&gt;
       ⎜                ⎝x ⎠⎟                   ⎜                ⎝x ⎠⎟        ⎝x ⎠           &lt;br&gt;
    log⎝log(1 + x) + x⋅ℯ    ⎠                log⎝log(1 + x) - x⋅ℯ    ⎠     x⋅ℯ    ⋅log(1 + x)&lt;br&gt;
x + ───────────────────────── - log(1 + x) - ───────────────────────── - ──────────────────────
                2                                        2                                    2
                                                                            2           2  2⋅x 
                                                                         log (1 + x) - x ⋅ℯ    &lt;/p&gt;
&lt;p&gt;In [5]: %timeit risch_integrate(f, x, handle_first='log')&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 1.49 s per loop&lt;/p&gt;
&lt;p&gt;In [6]: %timeit risch_integrate(f, x, handle_first='exp')&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 1.21 s per loop&lt;/p&gt;
&lt;p&gt;In [7]: cancel(risch_integrate(f, x, handle_first='log').diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[7]: 0&lt;/p&gt;
&lt;p&gt;In [8]: cancel(risch_integrate(f, x, handle_first='exp').diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[8]: 0&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;So go now, and pull my &lt;a href="//github.com/asmeurer/sympy/tree/integration3"&gt;branch&lt;/a&gt;, and try this function out.  And report any problems that you have back to me, either through the mailing list, IRC, issue 2010, or as a comment to this blog post (I don't really care how).&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2010/08/05/prototype-risch_integrate-function-ready-for-testing.html</guid><pubDate>Thu, 05 Aug 2010 22:30:00 GMT</pubDate></item><item><title>Integration of primitive functions</title><link>http://asmeurersympy.wordpress.com/posts/2010/07/31/integration-of-primitive-functions.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;strong&gt;Integration of Primitive Functions&lt;/strong&gt;
&lt;p&gt;So this past week, I had another break through in my project.  The &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;first break through&lt;/a&gt;, as you may recall, was the completion of the &lt;code&gt;integrate_hyperexponential()&lt;/code&gt; function, which allowed for the integration in hyperexponential extensions, including proving the nonexistence of elementary integrals.  Now I have worked my way up to this level on the other major half of the integration algorithm (actually, major third; more on that later): integration of primitive elements.  &lt;/p&gt;
&lt;p&gt;This time, I can refer you to my &lt;a href="http://asmeurersympy.wordpress.com/2010/07/24/the-risch-algorithm-part-2-elementary-functions/"&gt;previous blog post&lt;/a&gt; for definitions.  The chief thing here is that there is now a function in my &lt;tt&gt;integration3&lt;/tt&gt; branch called &lt;code&gt;integrate_primitive()&lt;/code&gt;, and it is used primarily for integrating functions with logarithms.&lt;/p&gt;
&lt;p&gt;So, how about some examples?  The first one comes from &lt;a href="http://asmeurersympy.wordpress.com/"&gt;Algorithms for computer algebra By Keith O. Geddes, Stephen R. Czapor, George Labahn&lt;/a&gt; (example 12.8).  I like it because it contains both exponentials and logarithms, in a way that they do not depend on each other, so it can be integrated with either &lt;code&gt;integrate_primitive()&lt;/code&gt; or &lt;code&gt;integrate_hyperexponential()&lt;/code&gt;.  In either case, the polynomial part is $latex \frac{x}{x + 1}$, so recursively calling the other function is not required.  (for those of you who have been following my &lt;tt&gt;integration3&lt;/tt&gt; branch, you may notice that this is blatantly taken from the commit history).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hover over the code and click on the left-most, "view source" icon (a paper icon with &lt;tt&gt;&amp;lt; &amp;gt;&lt;/tt&gt; over it) to view without breaks.  Opens in a new window.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [1]: from sympy.integrals.risch import integrate_primitive,&lt;/p&gt;
&lt;p&gt;integrate_hyperexponential&lt;/p&gt;
&lt;p&gt;In [2]: f = (x&lt;em&gt;(x + 1)&lt;/em&gt;((x&lt;strong&gt;2&lt;em&gt;exp(2&lt;/em&gt;x&lt;/strong&gt;2) - log(x + 1)&lt;strong&gt;2)&lt;/strong&gt;2 +&lt;/p&gt;
&lt;p&gt;2&lt;em&gt;x&lt;/em&gt;exp(3&lt;em&gt;x&lt;strong&gt;2)&lt;em&gt;(x - (2&lt;/em&gt;x&lt;/strong&gt;3 + 2&lt;/em&gt;x*&lt;em&gt;2 + x + 1)&lt;/em&gt;log(x + 1))))/((x +&lt;/p&gt;
&lt;p&gt;1)&lt;em&gt;log(x + 1)&lt;strong&gt;2 - (x&lt;/strong&gt;3 + x&lt;strong&gt;2)&lt;em&gt;exp(2&lt;/em&gt;x&lt;/strong&gt;2))&lt;/em&gt;*2&lt;/p&gt;
&lt;p&gt;In [3]: f&lt;/p&gt;
&lt;p&gt;Out[3]:
          ⎛                          2                                                   ⎞
          ⎜⎛                       2⎞                                                   2⎟
          ⎜⎜     2           2  2⋅x ⎟        ⎛    ⎛           2      3⎞           ⎞  3⋅x ⎟
x⋅(1 + x)⋅⎝⎝- log (1 + x) + x ⋅ℯ    ⎠  + 2⋅x⋅⎝x - ⎝1 + x + 2⋅x  + 2⋅x ⎠⋅log(1 + x)⎠⋅ℯ    ⎠&lt;/p&gt;
&lt;p&gt;──────────────────────────────────────────────────────────────────────────────────────────
                                                                2
                         ⎛                                    2⎞
                         ⎜   2                  ⎛ 2    3⎞  2⋅x ⎟
                         ⎝log (1 + x)⋅(1 + x) - ⎝x  + x ⎠⋅ℯ    ⎠&lt;/p&gt;
&lt;p&gt;In [4]: var('t0, t1')&lt;/p&gt;
&lt;p&gt;Out[4]: (t₀, t₁)&lt;/p&gt;
&lt;p&gt;In [5]: a, d = map(lambda i: Poly(i, t1), f.subs(exp(x**2),&lt;/p&gt;
&lt;p&gt;t0).subs(log(x + 1), t1).as_numer_denom())&lt;/p&gt;
&lt;p&gt;In [6]: a&lt;/p&gt;
&lt;p&gt;Out[6]:&lt;/p&gt;
&lt;p&gt;Poly((x + x&lt;strong&gt;2)*t1&lt;/strong&gt;4 + (-2&lt;em&gt;t0&lt;strong&gt;2*x&lt;/strong&gt;3 - 2&lt;/em&gt;t0&lt;strong&gt;2*x&lt;/strong&gt;4)&lt;em&gt;t1&lt;/em&gt;*2 +&lt;/p&gt;
&lt;p&gt;(-2&lt;em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;2 - 4&lt;/em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;3 - 6&lt;em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;4 - 8&lt;/em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;5 -&lt;/p&gt;
&lt;p&gt;4&lt;em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;6)&lt;/em&gt;t1 + 2&lt;em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;3 + 2&lt;/em&gt;t0&lt;strong&gt;3*x&lt;/strong&gt;4 + t0&lt;em&gt; &lt;/em&gt;4&lt;em&gt;x&lt;/em&gt;*5 +&lt;/p&gt;
&lt;p&gt;t0&lt;strong&gt;4*x&lt;/strong&gt;6, t1, domain='ZZ[x,t0]')&lt;/p&gt;
&lt;p&gt;In [7]: d&lt;/p&gt;
&lt;p&gt;Out[7]: Poly((1 + 2&lt;em&gt;x + x&lt;strong&gt;2)*t1&lt;/strong&gt;4 + (-2&lt;/em&gt;t0&lt;strong&gt;2*x&lt;/strong&gt;2 - 4*t0&lt;strong&gt;2*x&lt;/strong&gt;3 -&lt;/p&gt;
&lt;p&gt;2&lt;em&gt;t0&lt;strong&gt;2*x&lt;/strong&gt;4)&lt;/em&gt;t1&lt;strong&gt;2 + t0&lt;/strong&gt;4&lt;em&gt;x&lt;strong&gt;4 + 2*t0&lt;/strong&gt;4&lt;/em&gt;x&lt;strong&gt;5 + t0&lt;/strong&gt;4&lt;em&gt;x&lt;/em&gt;*6, t1,&lt;/p&gt;
&lt;p&gt;domain='ZZ[x,t0]')&lt;/p&gt;
&lt;p&gt;In [8]: D = [Poly(1, x), Poly(2&lt;em&gt;x&lt;/em&gt;t0, t0), Poly(1/(x + 1), t1)]&lt;/p&gt;
&lt;p&gt;In [9]: r = integrate_primitive(a, d, D, [x, t0, t1], [lambda x: log(x +&lt;/p&gt;
&lt;p&gt;1), lambda x: exp(x**2)])&lt;/p&gt;
&lt;p&gt;In [10]: r&lt;/p&gt;
&lt;p&gt;Out[10]:&lt;/p&gt;
&lt;p&gt;⎛   ⎛                ⎛ 2⎞⎞      ⎛                ⎛ 2⎞⎞        ⎛ 2⎞                                ⎞&lt;/p&gt;
&lt;p&gt;⎜   ⎜                ⎝x ⎠⎟      ⎜                ⎝x ⎠⎟        ⎝x ⎠                ⌠               ⎟&lt;/p&gt;
&lt;p&gt;⎜log⎝log(1 + x) + x⋅ℯ    ⎠   log⎝log(1 + x) - x⋅ℯ    ⎠     x⋅ℯ    ⋅log(1 + x)     ⎮   x           ⎟&lt;/p&gt;
&lt;p&gt;⎜───────────────────────── - ───────────────────────── - ────────────────────── + ⎮ ───── dx, True⎟&lt;/p&gt;
&lt;p&gt;⎜            2                           2                                    2   ⎮ 1 + x         ⎟&lt;/p&gt;
&lt;p&gt;⎜                                                           2           2  2⋅x    ⌡               ⎟&lt;/p&gt;
&lt;p&gt;⎝                                                        log (1 + x) - x ⋅ℯ                       ⎠&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;An explanation:  &lt;code&gt;f&lt;/code&gt; is the function we are integrating.  Preparsing is not implemented yet, so we have to do it manually in &lt;tt&gt;[5]&lt;/tt&gt;.  &lt;tt&gt;[8]&lt;/tt&gt; is the list of derivations of the monomials we are working with, &lt;code&gt;[x, t0, t1]&lt;/code&gt;, which represent $latex x$, $latex e^{x^2}$, and $latex \log{(x + 1)}$, respectively. Because the outermost monomial is a logarithm (primitive), we call &lt;code&gt;integrate_primitive()&lt;/code&gt; on it.  The last argument of the function is the back substitution list, in reverse order because that is the order we have to back substitute in.  We can see the result contains an unevaluated Integral.  This is because the recursive calls to integrate over the smaller extensions have not yet been implemented.  In the final version, &lt;code&gt;integrate()&lt;/code&gt; will automatically call &lt;code&gt;ratint()&lt;/code&gt; in this case on it to give the complete answer.  The second argument of the result, True, indicates that the integral was elementary and that this is the complete integral.&lt;/p&gt;
&lt;p&gt;Because the extensions did not depend on each other, we could have also integrated in $latex \mathbb{Q}(x, \log{(x + 1)}, e^{x^2})$ instead of $latex \mathbb{Q}(x, e^{x^2}, \log{(x + 1)})$:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [11]: a1, d1 = map(lambda i: Poly(i, t0), f.subs(exp(x**2), t0).subs(log(x + 1), t1).as_numer_denom())&lt;/p&gt;
&lt;p&gt;In [12]: D1 = [Poly(1, x), Poly(1/(x + 1), t1), Poly(2&lt;em&gt;x&lt;/em&gt;t0, t0)]&lt;/p&gt;
&lt;p&gt;In [13]: r1 = integrate_hyperexponential(a1, d1, D1, [x, t1, t0], [lambda x: exp(x**2), lambda x: log(x + 1)])&lt;/p&gt;
&lt;p&gt;In [14]: r1&lt;/p&gt;
&lt;p&gt;Out[14]:&lt;/p&gt;
&lt;p&gt;⎛   ⎛              ⎛ 2⎞⎞      ⎛                ⎛ 2⎞⎞                                                ⎞&lt;/p&gt;
&lt;p&gt;⎜   ⎜log(1 + x)    ⎝x ⎠⎟      ⎜  log(1 + x)    ⎝x ⎠⎟          ⎛ 2⎞                                  ⎟&lt;/p&gt;
&lt;p&gt;⎜log⎜────────── + ℯ    ⎟   log⎜- ────────── + ℯ    ⎟       2  ⎝x ⎠                  ⌠               ⎟&lt;/p&gt;
&lt;p&gt;⎜   ⎝    x             ⎠      ⎝      x             ⎠      x ⋅ℯ    ⋅log(1 + x)       ⎮   x           ⎟&lt;/p&gt;
&lt;p&gt;⎜─────────────────────── - ───────────────────────── + ────────────────────────── + ⎮ ───── dx, True⎟&lt;/p&gt;
&lt;p&gt;⎜           2                          2                                        2   ⎮ 1 + x         ⎟&lt;/p&gt;
&lt;p&gt;⎜                                                             2           3  2⋅x    ⌡               ⎟&lt;/p&gt;
&lt;p&gt;⎝                                                      - x⋅log (1 + x) + x ⋅ℯ                       ⎠&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;We can verify by taking the derivative that the results in each case are antiderivatives of the original function, &lt;code&gt;f&lt;/code&gt;, even though they appear different.&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [15]: cancel(r[0].diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[15]: 0&lt;/p&gt;
&lt;p&gt;In [16]: cancel(r1[0].diff(x) - f)&lt;/p&gt;
&lt;p&gt;Out[16]: 0&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;We can see in each case, the remaining unevaluated &lt;code&gt;Integral&lt;/code&gt; was in $latex \mathbb{Q}(x)$ only, meaning that the recursive call to &lt;code&gt;integrate_hyperexponential()&lt;/code&gt; or &lt;code&gt;integrate_primitive()&lt;/code&gt;, respectively, would not have been necessary. Finally, we can see that choosing the correct extension to integrate over can make a difference, time wise:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [17]: %timeit integrate_primitive(a, d, D, [x, t0, t1], [lambda x: log(x + 1), lambda x: exp(x**2)])&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 1.91 s per loop&lt;/p&gt;
&lt;p&gt;In [18]: %timeit integrate_hyperexponential(a1, d1, D1, [x, t1, t0], [lambda x: exp(x**2), lambda x: log(x + 1)])&lt;/p&gt;
&lt;p&gt;1 loops, best of 3: 2.63 s per loop&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Just as with the exponential case, the function can prove the integrals are non-elementary. This is the so-called &lt;a href="http://en.wikipedia.org/wiki/Logarithmic_integral"&gt;logarithmic integral&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [19]: f1 = 1/log(x)&lt;/p&gt;
&lt;p&gt;In [20]: a, d = map(lambda i: Poly(i, t1), f1.subs(log(x), t1).as_numer_denom())&lt;/p&gt;
&lt;p&gt;In [21]: a&lt;/p&gt;
&lt;p&gt;Out[21]: Poly(1, t1, domain='ZZ')&lt;/p&gt;
&lt;p&gt;In [22]: d&lt;/p&gt;
&lt;p&gt;Out[22]: Poly(t1, t1, domain='ZZ')&lt;/p&gt;
&lt;p&gt;In [23]: integrate_primitive(a, d, [Poly(1, x), Poly(1/x, t1)], [x, t1], [log])&lt;/p&gt;
&lt;p&gt;Out[23]: (0, False)&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;The second argument, &lt;code&gt;False&lt;/code&gt;, indicates that the integral was non-elementary.  Namely, the function has proven that the function $latex f - D(0) = \frac{1}{\log{(x)}}$ does not have an elementary anti-derivative over $latex \mathbb{Q}(x, \log{(x)})$ (see the &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;previous post&lt;/a&gt; for more information).&lt;/p&gt;
&lt;p&gt;Finally, be aware that, just as with &lt;code&gt;integrate_hyperexponential()&lt;/code&gt; many integrals will  raise &lt;code&gt;NotImplementedError&lt;/code&gt;, because the subroutines necessary to solve them have not yet been finished.&lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [25]: f = log(log(x))**2&lt;/p&gt;
&lt;p&gt;In [26]: f.diff(x)&lt;/p&gt;
&lt;p&gt;Out[26]:&lt;/p&gt;
&lt;p&gt;2⋅log(log(x))&lt;/p&gt;
&lt;p&gt;─────────────
   x⋅log(x)&lt;/p&gt;
&lt;p&gt;In [27]: a, d = map(lambda i: Poly(i, t1),&lt;/p&gt;
&lt;p&gt;cancel(f.diff(x)).subs(log(x), t0).subs(log(t0), t1).as_numer_denom())&lt;/p&gt;
&lt;p&gt;In [28]: a&lt;/p&gt;
&lt;p&gt;Out[28]: Poly(2*t1, t1, domain='ZZ')&lt;/p&gt;
&lt;p&gt;In [29]: d&lt;/p&gt;
&lt;p&gt;Out[29]: Poly(t0*x, t1, domain='ZZ[x,t0]')&lt;/p&gt;
&lt;p&gt;In [30]: D = [Poly(1, x), Poly(1/x, t0), Poly(1/(x*t0), t1)]&lt;/p&gt;
&lt;p&gt;In [31]: integrate_primitive(a, d, D, [x, t0, t1], [lambda x: log(log(x)), log])&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;NotImplementedError: Remaining cases for Poly RDE not yet implemented.&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Now one thing that I want to add from the above examples taken from the commit message is that logarithms are not the only function that are primitive.  The Li function (the logarithmic integral, as above), considered as an elementary extension of $latex \mathbb{Q}(x, \log{(x)})$ is also primitive.  But even among the commonly defined elementary functions, there is one other, acrtangents.  &lt;/p&gt;
&lt;p&gt;[code language="py"]&lt;/p&gt;
&lt;p&gt;In [32]: diff(atan(x)**2, x)&lt;/p&gt;
&lt;p&gt;Out[32]: &lt;/p&gt;
&lt;p&gt;2⋅atan(x)&lt;/p&gt;
&lt;p&gt;─────────
       2 
  1 + x  &lt;/p&gt;
&lt;p&gt;In [33]: integrate_primitive(Poly(2*t, t), Poly(1 + x&lt;strong&gt;2, t), [Poly(1, x), Poly(1/(1 + x&lt;/strong&gt;2), t)], [x, t], [atan])&lt;/p&gt;
&lt;p&gt;Out[33]: &lt;/p&gt;
&lt;p&gt;⎛    2         ⎞&lt;/p&gt;
&lt;p&gt;⎝atan (x), True⎠&lt;/p&gt;
&lt;p&gt;In [34]: integrate_primitive(Poly(t, t), Poly(x, t), [Poly(1, x), Poly(1/(1 + x**2), t)], [x, t], [atan])&lt;/p&gt;
&lt;p&gt;Out[34]: &lt;/p&gt;
&lt;p&gt;⎛⌠                  ⎞&lt;/p&gt;
&lt;p&gt;⎜⎮ atan(x)          ⎟&lt;/p&gt;
&lt;p&gt;⎜⎮ ─────── dx, False⎟&lt;/p&gt;
&lt;p&gt;⎜⎮    x             ⎟&lt;/p&gt;
&lt;p&gt;⎝⌡                  ⎠&lt;/p&gt;
&lt;p&gt;[/code]&lt;/p&gt;
&lt;p&gt;Due to a bug in the code right now, the final version returns the non-elementary integral in the final result.  Suffice it to say that it has proven that $latex \int {\frac{\arctan{(x)}}{x} dx}$ is non-elementary. As far as I know, this isn't any special function.  Actually, it's just a random function containing arctan that looked non-elementary to me that I plugged in and found out that I was correct.  It's very similar in form to the &lt;a href="http://en.wikipedia.org/wiki/Exponential_integral"&gt;exponential integral&lt;/a&gt; (Ei) or the &lt;a href="http://en.wikipedia.org/wiki/Sine_integral#Sine_integral"&gt;Sine/Cosine Integral&lt;/a&gt; (Si/Ci), which is how I guessed that it would be non-elementary.  Maybe it should be called ATi().&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status Update&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So it has come to my attention that the suggested "pencils down" date is one week from Monday, and the hard "pencils down" date is two weeks from Monday (see the &lt;a href="http://socghop.appspot.com/document/show/gsoc_program/google/gsoc2010/timeline"&gt;Google Summer of Code Timeline&lt;/a&gt;).  Now, no matter how fast I work, my work cannot be pushed in until Mateusz's latest polys branch gets pushed in, because my work is based on top of it.  I plan on continuing work on the integration algorithm beyond the summer until I finish the transcendental part of the algorithm, and even after that, I want to look into implementing other integration related things, like definite integration using &lt;a href="http://en.wikipedia.org/wiki/Meijer-G"&gt;Meijer G-functions,&lt;/a&gt; and the algebraic part of the algorithm.  But for now, these are the things that I need to do for the transcendental part, which is this summer's work:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;1. Implement the preparsing algorithms. &lt;/em&gt; This part is two-fold.  First, I need to implement algorithms based on the Risch Structure Theorems, which allow me to determine if an extension is algebraic or not (if it is algebraic, we cannot integrate it because only the transcendental part is implemented).  The other part will be the function that actually goes through an expression and tries to build up a differential extension from it so it can be integrated.  This can be a tricky part. For example, if we want to integrate $latex f = e^x + e^{\frac{x}{2}}$, we want to first choose $latex t_1=e^{\frac{x}{2}}$ so that $latex f = t_1^2 + t_1$, because if we choose $latex t_1=e^x$, then $latex t_2=e^{\frac{x}{2}}=\sqrt{t_1}$ will be algebraic over $latex \mathbb{Q}(x, t_1)$.  This is one case where we might try adding an algebraic extensions but where it can be avoided.  The solution will have to be to go through and find the common denominators of the exponentials.  I'm also considering that this might happen in more advanced ways, so it could be necessary for the function to backtrack in the extension tree to see if it can do it in an entirely transcendental way.  Fortunately, the Risch Structure Theorems give us a decision procedure for determining if an extension can be written in terms of the previous extensions (is algebraic over it), but this will still be a very hard function to get right.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;2. Finish the remaining cases for &lt;code&gt;integrate_hyperexponential()&lt;/code&gt; and &lt;code&gt;integrate_primitive()&lt;/code&gt;.&lt;/em&gt; As you could see in this post, as well as in the &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;previous one&lt;/a&gt;, there are many integrals that cannot yet be integrated because the special cases for them have not been implemented yet.  Most of these actually rely on implementing the structure theorem algorithms from &lt;strong&gt;1&lt;/strong&gt;, and implementing them once that is finished will not take long, because they will just be straight copying of the pseudocode from Bronstein's book.  But some of them, particularly ones from the primitive case, are not spelt out so well in Bronstein's book, and will require more thinking (and thus time) on my part.  I should note that the Structure Theorem algorithms are also this way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt; 3. Implement the hypertangent case. &lt;/em&gt; The ability to integrate in tangent extensions is the other &lt;em&gt;third&lt;/em&gt; I mentioned above.  Since tangents require more special casing, I plan on doing this only after I have finished &lt;strong&gt;1&lt;/strong&gt; and &lt;strong&gt;2&lt;/strong&gt;.  This is actually not much work, because most of the algorithms for solving the particular subproblem for tangents (called the &lt;em&gt;Coupled Risch Differential Equation&lt;/em&gt;) are exactly the same as those for solving the subproblem for hyperexponentials (the &lt;em&gt;Risch Differential Equation&lt;/em&gt;), which are already (mostly) implemented in the hyperexponential part.  There are only a few extra functions that need to be written for it.  Also, you will still be able to integrate functions that contain tangents, such as $latex e^{\tan{(x)}}$ (recall &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;last time&lt;/a&gt; that we showed that &lt;code&gt;integrate_hyperexponential()&lt;/code&gt; can prove that this does not have an elementary integral).  It just won't be able to integrate when the top-most extension is a tangent.&lt;/p&gt;
&lt;p&gt;So here is what I plan on doing.  Right now, I am going to focus my work on &lt;strong&gt;1&lt;/strong&gt;, since most of &lt;strong&gt;2&lt;/strong&gt; can't be done until it is anyway.  But more importantly, I want to have a prototype user-level function for the Risch Algorithm.  The reason I want this is so that people can try it out, without having to do the preparsing like I did above, but rather they can just call &lt;code&gt;risch_integrate(f, x)&lt;/code&gt;, and it will return the integral of &lt;code&gt;f&lt;/code&gt;, prove that it is non-elementary and reduce it into the elementary and non-elementary parts, or explain why it cannot do it (either because the function is not transcendental or because something is not implemented yet).  My chief desire for doing this is so that people can try out my code and find the bugs in it for me.  I have already found many critical errors in the code (returns a wrong result), and I want to iron these out before anything goes in.  The best way to do this will be to release a working user-level function and hope that people try it out for me.  &lt;/p&gt;
&lt;p&gt;Also, even if &lt;strong&gt;2&lt;/strong&gt; and &lt;strong&gt;3&lt;/strong&gt; are not finished, if I have &lt;strong&gt;1&lt;/strong&gt;, I can integrate it with &lt;code&gt;integrate()&lt;/code&gt; (no pun intended) and just have it bail if it raises &lt;code&gt;NotImplementedError&lt;/code&gt; I will need to come up with a way to differentiate between this and the case where it returns an unevaluated &lt;code&gt;Integral&lt;/code&gt; because it has proven that an elementary antiderivative does not exist.  Any suggestions?&lt;/p&gt;
&lt;p&gt;I plan on continuing work after the summer until I finish &lt;strong&gt;1&lt;/strong&gt; through &lt;strong&gt;3&lt;/strong&gt;, though I won't pretend that my work won't slow down considerably when I start classes in August.  I also promise to finish the &lt;a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/"&gt;Risch Algorithm posts&lt;/a&gt; that I promised.&lt;/p&gt;
&lt;p&gt;And for what it's worth, I plan on working my ass off this next two weeks.&lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2010/07/31/integration-of-primitive-functions.html</guid><pubDate>Sat, 31 Jul 2010 06:44:31 GMT</pubDate></item><item><title>The Risch Algorithm: Part 2, Elementary Functions</title><link>http://asmeurersympy.wordpress.com/posts/2010/07/24/the-risch-algorithm-part-2-elementary-functions.html</link><dc:creator>Aaron Meurer</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;In &lt;a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/"&gt;Part 1&lt;/a&gt; of this series of blog posts, I gave what I believed to be the prerequisites to understanding the mathematics behind the Risch Algorithm (aside from a basic understanding of derivatives and integrals from calculus).  In this post, I will elaborate on what is meant by "elementary function," a term that is thrown around a lot when talking about Risch integration.&lt;/p&gt;
&lt;p&gt;The usual definition of elementary function given in calculus is any function that is a constant, a polynomial, an exponential ($latex e^x$, $latex 2^x$), a logarithm ($latex \ln({x})$, $latex \log_{10}({x})$), one of the standard trig functions or their inverses (sin, cos, tan, arcsin, arccos, arctan, etc.), and any combination of these functions via addition, subtraction, multiplication, division, taking powers, and composition.  Thus, even a function as crazy as &lt;a href="http://asmeurersympy.wordpress.com/2010/07/crazy-function.png"&gt;&lt;img src="http://asmeurersympy.wordpress.com/2010/07/crazy-function.png" alt="" title="crazy function" width="193" height="41" class="alignnone size-full wp-image-632"&gt;&lt;/a&gt; is elementary, by this definition.  &lt;/p&gt;
&lt;p&gt;But for the rigorous definition of an elementary function, we must take into consideration what field we are working over.  Before I get into that, I need some definitions.  Suppose that $latex k$ is the field we are working over.  You can imagine that $latex k=\mathbb{Q}(x)$, the field of rational functions in x with rational number coefficients.  As with the previous post, imagine $latex t$ as a function, for example, $latex t = f(x)$.  Let $latex K$ be a differential extension of $latex k$.  We have not defined this, but it basically means that our derivation $latex D$ works the same in $latex K$ as it does in $latex k$.  You can imagine here that $latex K=k[t]$.  &lt;/p&gt;
&lt;p&gt;We say that $latex t \in K$ is a &lt;strong&gt;primitive&lt;/strong&gt; over $latex k$ if $latex Dt \in k$.  In other words, the derivative of $latex t$ is does not contain $latex t$, only elements of $latex k$.  Obviously, by the definition of a derivation (see the &lt;a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/"&gt;last post&lt;/a&gt; in the series), any element of $latex k$ is a primitive over $latex K$, because the derivative of any element of a field is again an element of that field (you can see this by the definition of a derivation, also given in the last post).  But also if $latex t=log(a)$ for some $latex a \in k$, then $latex t$ is a primitive over $latex k$, because $latex Dt=\frac{Da}{a}\in k$.  &lt;/p&gt;
&lt;p&gt;We say that $latex t \in K^*$ is a &lt;strong&gt;hyperexponential&lt;/strong&gt; over $latex k$ if $latex \frac{Dt}{t}\in k$.  Written another way, $latex Dt=at$ for some $latex a\in k$.  We know from calculus that the functions that satisfy differential equations of the type $latex \frac{dy}{dx}=ay$ are exactly the exponential functions, i.e., $latex y=e^{\int{a\ dx}}$.  &lt;/p&gt;
&lt;p&gt;The last class of functions that needs to be considered is &lt;strong&gt;&lt;a href="http://en.wikipedia.org/wiki/Algebraic_function"&gt;algebraic functions&lt;/a&gt;&lt;/strong&gt;.  I will not go into depth on algebraic functions, because my work this summer is only on integrating purely transcendental functions.  Therefore, the only concern we shall have with algebraic functions in relation to the integration algorithm is to make sure that whatever function we are integrating is &lt;em&gt;not&lt;/em&gt; algebraic, because the transcendental algorithms will not be valid if they are.  Hopefully in a future post I will be able to discuss the Risch Structure Theorems, which give necessary and sufficient conditions for determing if a Liouvillian function (see next paragraph) is algebraic.  &lt;/p&gt;
&lt;p&gt;Now, we say that a function $latex t \in K$ is &lt;strong&gt;Liouvillian&lt;/strong&gt; over $latex k$ if $latex t$ is algebraic, a primitive, or a hyperexponential over $latex k$.  For $latex t\in K$ to be a &lt;strong&gt;Liouvillian monomial&lt;/strong&gt; over $latex k$, we have the additional condition that $latex \mathrm{Const}(k) = \mathrm{Const}(k(t))$. This just means that we cannot consider something like $latex \log({2})$ over $latex \mathbb{Q}$ as a Liouvillian monomial.  Otherwise (I believe) we could run into undecidability problems.  &lt;/p&gt;
&lt;p&gt;We call $latex t \in K$ a &lt;strong&gt;logarithm&lt;/strong&gt; over $latex k$ if $latex Dt=\frac{Db}{b}$ for some $latex b \in k^&lt;em&gt;$, i.e., $latex t=\log({b})$.  We call $latex t \in K^&lt;/em&gt;$ an &lt;strong&gt;exponential&lt;/strong&gt; over $latex k$ if $latex \frac{Dt}{t}=Db$ (or $latex Dt=tDb$) for some $latex b \in k$, i.e., $latex t=e^b$.  Note the difference between an &lt;em&gt;exponential&lt;/em&gt; monomial and a &lt;em&gt;hyperexponential&lt;/em&gt; monomial.  &lt;/p&gt;
&lt;p&gt;We can finally give the rigorous definition of an elementary extension.  $latex K$ is an &lt;strong&gt;elementary extension&lt;/strong&gt; of $latex k$ if there are $latex t_1, \dots, t_n \in K$ such that $latex K=k(t_1,\dots,t_n)$ and $latex t_i$ is elementary over $latex k(t_1, \dots, t_{i-1})$ for all $latex i \in {1,\dots,n}$.  An &lt;strong&gt;elementary function &lt;/strong&gt; is any element of an elementary extension of $latex \mathbb{C}(x)$ with the derivation $latex D=\frac{d}{dx}$.  A function $latex f\in k$ has an &lt;strong&gt;elementary integral&lt;/strong&gt; over $latex k$ if there exists an elementary extension $latex K$ of $latex k$ and $latex g\in K$ such that $latex Dg=f$, i.e., $latex f=\int{g}$.  &lt;/p&gt;
&lt;p&gt;Usually, we start with $latex \mathbb{Q}(x)$, the field of rational functions in x with rational number coefficients. We then build up an elementary extension one function at a time, with each function either being a logarithm or exponential of what we have already built up, or algebraic over it.  As I noted above, we will ignore algebraic functions here.  We generally start with $latex \mathbb{Q}$ because it is computable (important problems such as the zero equivalence problem or the problem of determining certain field isomorphisms are decidable), but the above definition lets us start with any subfield of $latex \mathbb{C}$.  &lt;/p&gt;
&lt;p&gt;Now you may be wondering: we've covered algebraic functions, exponentials and logarithms, and obviously rational functions are elements of $latex \mathbb{Q}(x)$, but what about trigonometric functions?  Well, from a theoretical stand point, we can make our lives easier by noticing that all the common trigonometric functions can be represented as exponentials and logarithms over $latex \mathbb{Q}(i)$.  For example, $latex \cos{x} = \frac{e^{ix} + e^{-ix}}{2}$.  You can see &lt;a href="http://en.wikipedia.org/wiki/Trig_identities#Exponential_definitions"&gt;here&lt;/a&gt; that all the common trig functions can be represented as complex exponentials or logarithms like this.  However, from an algorithmic standpoint, we don't want do convert all trig expressions into complex exponentials and logarithms in order to integrate them.  For one thing, our final result will be in terms of complex exponentials and logarithms, not the original functions we started with, and converting them back may or may not be an easy thing to do.  Also, aside from the fact that we have different functions than we were expecting, we also will end up with an answer containing $latex \sqrt{-1}$, even if our original integrand did not.  &lt;/p&gt;
&lt;p&gt;Fortunately, the integrating tangents directly is a solved problem, just like integrating algebraic, exponential, or logarithmic functions is solved.  We can't integrate functions like $latex \sin{x}$ or $latex \cos{x}$ directly as monomials like we can with $latex \tan{x}$ or $latex e^x$, because the derivatives of sin and cos are not polynomials in their respective selves with coefficients in $latex \mathbb{C}(x)$.  However, we can use a trick or two to integrate them.  One way is to rewrite $latex \cos{x}=\frac{1 - \tan^2{\frac{x}{2}}}{1 + \tan^2{\frac{x}{2}}}$ and proceed to integrate it as a tangent.  Another alternative is to write $latex \cos{x}=\frac{1}{\sec{x}}=\sqrt{\frac{1}{\sec^2{x}}}=\sqrt{\frac{1}{\tan^2{x} + 1}}$.  This function is algebraic over $latex \mathbb{Q}(x, \tan{(x)})$, but if we do not already have $latex \tan{x}$ in our differential extension, it is transcendental, and we can rewrite it as $latex e^{-\frac{\log{(1 + \tan^2{x})}}{2}}$ (this is used in Bronstein's text, so I believe what I just said is correct, though I haven't verified it with the structure theorems just yet).   These both work using the relevant identities for sin too.  Of course, there is still the problem of rewriting the final integrand back in terms of sin or cos.  Otherwise, you will get something like $latex \frac{2e^x\tan({\frac{x}{2}}) - \tan^2({\frac{x}{2}})e^x + e^x}{2 + 2\tan^2({\frac{x}{2}})}$ instead of $latex \frac{e^x(\sin{(x)} + \cos{(x)})}{2}$ for $latex \int{\cos{(x)}e^xdx}$.  Bronstein doesn't elaborate on this too much in his book, so it is something that I will have to figure out on my own.&lt;/p&gt;
&lt;p&gt;The second option I gave above leads nicely into the main point I wanted to make here about elementary functions.  Notice that everywhere in the definitions above, things depend on the field we are working in.  Therefore, $latex e^{\tan{x}}$ cannot be an elementary extension over $latex \mathbb{Q}(x)$, but it can be over $latex \mathbb{Q}(x, \tan{x})$.  Also, the &lt;a href="http://en.wikipedia.org/wiki/Error_function"&gt;error function&lt;/a&gt;, defined as $latex \mathrm{erf}{(x)} = \frac{2}{\sqrt{\pi}}\int{e^{-x^2}dx}$ cannot be an elementary extension over $latex \mathbb{Q}(x)$, but it can over $latex \mathbb{Q}(x, e^{-x^2})$. In fact this is how we can integrate in terms of some special functions, including the error function: by manually adding $latex e^{-x^2}$ (or whatever) to our differential extension.   Therefore, the usual definition of an elementary anti-derivaitve and the above Risch Algorithm definition of an elementary integral coincide only when the extension consists only of elementary functions of the form of the usual definition (note that above, our final fields are $latex \mathbb{Q}(x, \tan{x}, e^{\tan{x}})$ and $latex \mathbb{Q}(x, e^{-x^2}, \mathrm{erf}{(x)})$, respectively).  &lt;/p&gt;
&lt;p&gt;Originally, I was also going to talk about Liouville's Theorem in this blog post, but I think it has already gotten long enough (read "I'm getting tired"), so I'll put that off until next time.  &lt;/p&gt;&lt;/div&gt;</description><category>mathjax</category><guid>http://asmeurersympy.wordpress.com/posts/2010/07/24/the-risch-algorithm-part-2-elementary-functions.html</guid><pubDate>Sat, 24 Jul 2010 03:32:57 GMT</pubDate></item></channel></rss>