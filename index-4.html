<!DOCTYPE html><html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="My blog">
    <meta name="author" content="Aaron Meurer">
    <title>Aaron Meurer's Blog (old posts, page 4) | Aaron Meurer's Blog</title>
    
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
   tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
   },
   displayAlign: 'left', // Change this to 'center' to center equations.
   "HTML-CSS": {
       styles: {'.MathJax_Display': {"margin": 0}}
   }
});
</script>

            <link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
      <link rel="canonical" href="http://asmeurer.github.io/index-4.html">
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js" type="text/javascript"></script>
    <![endif]-->
            <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">

    
    
    
<link rel="stylesheet" type="text/css" href="assets/css/tipuesearch.css">
</head><body><div id="tipue_search_content" style="margin-left: auto; margin-right: auto; padding: 20px;"></div>



<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container-fluid"><!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://asmeurer.github.io/">Aaron Meurer's Blog</a>
        </div><!-- /.navbar-header -->
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                
                <li><a href="archive.html">Archives</a>
                </li><li><a href="categories/index.html">Tags</a>
                </li><li><a href="rss.xml">RSS</a>

            </li></ul>
                
<span class="navbar-form pull-left">
<input type="text" id="tipue_search_input">
</span>

            <ul class="nav navbar-nav navbar-right">
                
                
                    
            </ul>
        </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
</nav>

<!-- End of Menubar -->

<div class="container">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/08/05/prototype-risch_integrate-function-ready-for-testing/" class="u-url">Prototype risch_integrate() function ready for testing!</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-08-05T22:30:00-05:00">2010-08-05 22:30</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>So to­day I fi­nal­ly fin­ished up the pro­to­type func­tion I talked about  <a href="http://asmeurersympy.wordpress.com/2010/07/31/integration-of-primitive-functions/">last week</a>.  The func­tion is called  <code>risch_in­te­grate()</code>  and is avail­able at my  <a href="http://github.com/asmeurer/sympy/tree/integration3">in­te­gra­tion3</a>  branch.  Un­like the in­ner lev­el func­tions I have show­cased in  <a href="http://asmeurersympy.wordpress.com/2010/07/31/integration-of-primitive-functions/">pre­vi­ous</a>   <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">blog posts</a>, this func­tion does not re­quire you to do sub­sti­tu­tion for dum­my vari­ables and man­u­al­ly cre­ate a list of deriva­tives, etc.  All you have to do is pass it a func­tion and the in­te­gra­tion vari­able, and it will re­turn the re­sult, just like nor­mal  <code>in­te­grate()</code>. I have spent the past few days work­ing on a mon­ster of a func­tion called  <code>build_ex­ten­sion()</code>  that does this prepars­ing work for you.  The rea­son that the func­tion was so hard to write is that the tran­scen­den­tal Risch Al­go­rithm is very pick­y.   <em>Ev­ery</em>  dif­fer­en­tial ex­ten­sion has to be tran­scen­den­tal over the pre­vi­ous ex­ten­sion­s.  This means that if you have a func­tion like $la­tex e^x + e^{\frac{x}{2}}$, you can­not write this as $la­tex t_0 + t_1$ with $la­tex t_0=e^x$ and $la­tex t_1=e^{\frac{x}{2}}$ be­cause $la­tex t_0$ and $la­tex t_1$ will each be al­ge­bra­ic over the oth­er ($la­tex t_0=t_1^2$).  You al­so can­not let $la­tex t_0=e^{x}$ and re­write the whole in­te­gral in terms of $la­tex t_0$ be­cause you will get $la­tex t_0 + \sqrt{t_0}$, which is an al­ge­bra­ic func­tion.  The on­ly way that you can do it is to let $la­tex t_0=e^{\frac{x}{2}}$, and then your func­tion will be $la­tex t_0^2 + t_0$.   </p> 
 <p>Now, for­tu­nate­ly, there is an al­go­rithm that pro­vides nec­es­sary and suf­fi­cient con­di­tions for de­ter­min­ing if an ex­ten­sion is al­ge­bra­ic over the pre­vi­ous ones.  It's called the Risch Struc­ture The­o­rem­s.  My first or­der of busi­ness this week was to fin­ish im­ple­ment­ing these.  This is ac­tu­al­ly the rea­son that I we had to wait un­til now to get this pro­to­type func­tion.  The Struc­ture The­o­rems are at the very end of Bron­stein's book, and the in­te­gra­tion al­go­rithm is not cor­rect with­out them (name­ly, it is not cor­rect if you add an al­ge­bra­ic ex­ten­sion).  I just re­cent­ly got to them in my read­ing.  Ac­tu­al­ly, I skipped some work on tan­gent in­te­gra­tion so I could get to them first.  I hope to talk a lit­tle about them in a fu­ture "Risch In­te­gra­tion" blog post, though be aware that they re­quire some ex­treme­ly in­tense al­ge­bra­ic ma­chin­ery to prove, so I won't be giv­ing any proof­s.</p> 
 <p>Even though these al­go­rithms can tell me, for ex­am­ple, that I should­n't have added $la­tex t_0=e^x$ above be­cause it makes $la­tex e^{\frac{x}{2}}=\sqrt{t_0}$, that means that I have to go back and restart my search for an ex­ten­sion so that I can try to get $la­tex t_0=e^{\frac{x}{2}}$ in­stead.  So I wrote a sim­ple func­tion that takes the ar­gu­ments of the ex­po­nen­tials and de­ter­mines the low­est com­mon fac­tor.  This heuris­tic saves a lot of time.   </p> 
 <p>I al­so no­ticed (ac­tu­al­ly, Chris Smith in­ad­ver­tent­ly point­ed it out to me; su­per thanks to him), that the Struc­ture The­o­rem al­go­rithms on­ly tell you if the terms are the same as mono­mi­al­s.  It would tell you that $la­tex e^x = e^{x + 1}$ be­cause both sat­is­fy $la­tex Dt=t$.  There­fore, I had to al­so mod­i­fy the struc­ture the­o­rem al­go­rithms to pull out any con­stant ter­m.   </p> 
 <p>It can still be nec­es­sary to restart build­ing the ex­ten­sion even with the above heuris­tic.  For ex­am­ple, if you have $la­tex e^x + e^{x^2} + e^{\frac{x}{2} + x^2}$, and start with $la­tex t_0=e^x$ and $la­tex t_1=e^{x^2}$, then the struc­ture the­o­rems will tell you that $la­tex e^{x/2 + x^2} = \sqrt{t_0}t_1$, which we can­not use be­cause of the rad­i­cal.  The so­lu­tion it us­es is to split it up as $la­tex e^x + e^{x^2} + e^{\frac{x}{2}}e^{x^2}$ (the struc­ture the­o­rems tell you ex­act­ly how to do this so you are split­ting in terms of the oth­er ex­po­nen­tial­s) and then restart the ex­ten­sion build­ing en­tire­ly.  This can be an ex­pen­sive op­er­a­tion, be­cause you have to re­build $la­tex t_0$ and $la­tex t_1$, but this time, the heuris­tic func­tion I wrote from above han­dles the $la­tex e^{\frac{x}{2}}$ cor­rect­ly, mak­ing $la­tex t_0=e^{\frac{x}{2}}$, with the fi­nal an­swer $la­tex t_0^2 + t_1 + t_0t_1$.  I could have prob­a­bly made it smarter by on­ly go­ing back to be­fore the con­flict­ing ex­ten­sion­s, but this was quite a bit more work, and adds more dif­fi­cul­ties such as non-triv­ial re­la­tion­ship­s, so I just took the lazy way and restart­ed com­plete­ly.  It does­n't take  <em>that</em>  much time.   </p> 
 <p>Of course, some­times, you can­not add a new ex­po­nen­tial, no mat­ter how you add the ex­ten­sion­s.  The clas­sic ex­am­ple is $la­tex e^{\frac{\log{(x)}}{2}}$, which you can see is ac­tu­al­ly equal to $la­tex \sqrt{x}$, an al­ge­bra­ic func­tion.  There­fore, I had to im­ple­ment some tricky log­ic to keep the  <code>build_ex­ten­sion()</code>  func­tion from try­ing again in­fin­ite­ly.  I hope I did it right, so that it nev­er in­fi­nite loop­s, and nev­er fails when it re­al­ly can be done.  On­ly time and test­ing will tel­l.</p> 
 <p>It is ex­act­ly the same for log­a­rithm­s, ex­cept in that case, when a new log­a­rithm is al­ge­bra­ic in terms of old ones, it can be writ­ten as a lin­ear com­bi­na­tion of them.  This means that there are nev­er any rad­i­cals to wor­ry about, though you do al­so have to wor­ry about con­stants.  For ex­am­ple, $la­tex \log{(x)}$ looks the same as $la­tex \log{(2x)}$ be­cause they both sat­is­fy $la­tex Dt=\frac{1}{x}$.  An ex­am­ple of a log­a­rithm that is al­ge­bra­ic over old ones is $la­tex \log{(x^2 - 1)}$ over $la­tex \log{(x + 1)}$ and $la­tex \log{(x - 1)}$, be­cause $la­tex \log{(x^2 - 1)}=\log{((x + 1)(x - 1))}=\log{(x + 1)} + \log{(x - 1)}$.   </p> 
 <p>The par­al­lels be­tween ex­po­nen­tials and log­a­rithms are amaz­ing.  For the struc­ture the­o­rem­s, the ex­po­nen­tial case is ex­act­ly the same as the log­a­rith­mic case ex­cept re­plac­ing ad­di­tion with mul­ti­pli­ca­tion and mul­ti­pli­ca­tion with ex­po­nen­ti­a­tion.  For the ex­po­nen­tial case, you need the ar­gu­ments of the al­ready added log­a­rithms to find the al­ge­bra­ic de­pen­dence, and the ar­gu­ments of the al­ready added ex­po­nen­tials to find the con­stant ter­m.  For the log­a­rith­mic case, you need the ar­gu­ments of the al­ready added ex­po­nen­tials to find the al­ge­bra­ic de­pen­dence, and the ar­gu­ments of the al­ready added log­a­rithms to find the con­tent ter­m. Ev­ery­thing else is ex­act­ly the same, ex­cept for the shift in op­er­a­tors.  Of course, I re­al­ize why these things are, math­e­mat­i­cal­ly, but the sym­me­try still amaz­ing to me.  I will hope­ful­ly ex­plain in more de­tail in my fu­ture Struc­ture The­o­rems post.   </p> 
 <p>So on­to the  <code>risch_in­te­grate()</code>  func­tion.  Here is the text that I have ba­si­cal­ly put in my  <a href="http://github.com/asmeurer/sympy/commit/e3cd5f18f86fd6377836f33f726182c8bd4dc1a0">com­mit mes­sage</a>, the  <a href="http://code.google.com/p/sympy/issues/detail?q=2010">apt­ly num­bered is­sue</a>  that I have cre­at­ed for it, and the  <a href="http://groups.google.com/group/sympy/browse_thread/thread/2464fa764f6f47aa">post to the mail­ing list</a>  (it's not so much that I am lazy as that I was re­al­ly ex­cit­ed to get this out there).</p> 
 <p></p><blockquote>
<p>I have ready in my in­te­gra­tion3 branch a pro­to­type risch_in­te­grate() func­tion that is a user-lev­el func­tion for the full Risch Al­go­rithm I have been im­ple­ment­ing this sum­mer.  Pull from h<a href="//github.com/asmeurer/sympy/tree/integration3">ttp://github.­com/as­meur­er/sympy/tree/in­te­gra­tion3</a>.</p> 
 <p>This is NOT ready to go in.  It is a pro­to­type func­tion that I am mak­ing avail­able so peo­ple can try out the new al­go­rithm and hope­ful­ly help me to find the bugs in it.  Please pass it your fa­vorite non-ele­men­tary in­te­grals and see if it can de­ter­mine that they are not el­e­men­tary.  If you try to pass it a very crazy func­tion at ran­dom, the chances are pret­ty high that it will not be el­e­men­tary.  So a bet­ter way to test it is to come up with a crazy func­tion, then dif­fer­en­ti­ate it. Then pass the de­riv­a­tive and see if it can give you your orig­i­nal func­tion back.  Note that it will prob­a­bly not look ex­act­ly the same as your orig­i­nal func­tion, and may dif­fer by a con­stan­t.  You should ver­i­fy by dif­fer­en­ti­at­ing the re­sult you get and call­ing can­cel() (or sim­pli­fy(), but usu­al­ly can­cel() is enough) on the dif­fer­ence.</p> 
 <p>So you can re­view the code too, if you like, but just know that things are not sta­ble yet, and this is­n't strict­ly a branch for re­view.   </p> 
 <p>So far, this func­tion on­ly sup­ports ex­po­nen­tials and log­a­rithm­s.</p> 
 <p>Sup­port for trigono­met­ric func­tions is planned.  Al­ge­bra­ic func­tions are</p> 
 <p>not sup­port­ed. If the func­tion re­turns an un­eval­u­at­ed In­te­gral, it means</p> 
 <p>that it has proven the in­te­gral to be non-ele­men­tary.  Note that sev­er­al</p> 
 <p>cas­es are still not im­ple­ment­ed, so you may get NotIm­ple­ment­ed­Er­ror</p> 
 <p>in­stead. Even­tu­al­ly, these will all be elim­i­nat­ed, and the on­ly</p> 
 <p>NotIm­ple­ment­ed­Er­ror you should see from this func­tion is</p> 
 <p>NotIm­ple­ment­ed­Er­ror("Al­ge­bra­ic ex­ten­sions are not sup­port­ed.")</p> 
 <p>This func­tion has not been in­te­grat­ed in any way with the al­ready</p> 
 <p>ex­ist­ing in­te­grate() yet, and you can use it to com­pare.</p> 
 <p>Ex­am­ples:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: risch_in­te­grate(­ex­p(x**2), x)</p> 
 <p>Out­[1]:</p> 
 <p>⌠</p> 
 <p>⎮  ⎛ 2⎞</p> 
 <p>⎮  ⎝x ⎠</p> 
 <p>⎮ ℯ     dx</p> 
 <p>⌡</p> 
 <p>In [2]: risch_in­te­grate(x*<em>100</em>ex­p(x), x).d­if­f(x)</p> 
 <p>Out­[2]:
 100  x
x   ⋅ℯ</p> 
 <p>In [3]: %timeit risch_in­te­grate(x*<em>100</em>ex­p(x), x).d­if­f(x)</p> 
 <p>1 loop­s, best of 3: 270 ms per loop</p> 
 <p>In [4]: in­te­grate(x*<em>100</em>ex­p(x), x)</p> 
 <p>... hangs ...</p> 
 <p>In [5]: risch_in­te­grate(x/log(x), x)</p> 
 <p>Out­[5]:</p> 
 <p>⌠</p> 
 <p>⎮   x</p> 
 <p>⎮ ────── dx</p> 
 <p>⎮ log(x)</p> 
 <p>⌡</p> 
 <p>In [6]: risch_in­te­grate(log(x)**10, x).d­if­f(x)</p> 
 <p>Out­[6]:
   10
log  (x)</p> 
 <p>In [7]: in­te­grate(log(x)**10, x).d­if­f(x)</p> 
 <p>Out­[7]:
   10
log  (x)</p> 
 <p>In [8]: %timeit risch_in­te­grate(log(x)**10, x).d­if­f(x)</p> 
 <p>10 loop­s, best of 3: 159 ms per loop</p> 
 <p>In [9]: %timeit in­te­grate(log(x)**10, x).d­if­f(x)</p> 
 <p>1 loop­s, best of 3: 2.35 s per loop</p> 
 <p>[/­code]</p> 
 <p>Be warned that things are still very bug­gy and you should al­ways ver­i­fy</p> 
 <p>re­sults by dif­fer­en­ti­at­ing.  Usu­al­ly, can­cel(d­if­f(re­sult, x) - re­sult)</p> 
 <p>should be enough.  This should go to 0.</p> 
 <p>So please, please, PLEASE, try out this func­tion and re­port any bugs that you find.  It is not nec­es­sary to re­port NotIm­ple­ment­ed­Er­ror bugs, be­cause I al­ready know about those (I put them in there), and as I men­tioned above, they are all planned to dis­ap­pear.  Al­so, I am con­tin­u­al­ly up­dat­ing my branch with fix­es, so you should do a "git pul­l" and try again be­fore you re­port any­thing.</p> 
 <p>Al­so, I am aware that there are test fail­ures.  This is be­cause I had to hack ex­p._e­val_­sub­s() to on­ly do ex­act sub­sti­tu­tion (no al­ge­bra­ic sub­sti­tu­tion).  It's just a quick hack workaround, and I should even­tu­al­ly get a re­al fix.   </p> 
 <p>Fi­nal­ly, I'm think­ing there needs to be a way to dif­fer­en­ti­ate be­tween an un­eval­u­at­ed In­te­gral be­cause the in­te­gra­tor failed and an un­eval­u­at­ed In­te­gral be­cause it has proven the in­te­gral to be non-ele­men­tary.  Any ideas?</p> 
 </blockquote>

<p>Al­so, look­ing at the in­te­gral from the pre­vi­ous blog post, you can get the dif­fer­ent re­sults by us­ing the  <code>han­dle_log</code>  ar­gu­ment to  <code>risch_in­te­grate()</code>:</p> 
 <p>If  <code>han­dle_­first == 'log'</code>  (the de­fault right now), then it will gath­er all log­a­rithms first, and then ex­po­nen­tials (in­so­much as it can do it in that or­der).  If  <code>han­dle_­first='­ex­p'</code>, it gath­ers ex­po­nen­tials first.  The dif­fer­ence is that the Risch Al­go­rithm in­te­grates re­cur­sive­ly, one ex­ten­sion at a time, start­ing with the out­er-­most one. So if you have an ex­pres­sion with both log­a­rithms and ex­po­nen­tial­s, such that they do not de­pend on each oth­er,  <code>han­dle_­first == 'log'</code>  will in­te­grate the ex­po­nen­tials first, be­cause they will be gath­ered last (be at the top of the tow­er of ex­ten­sion­s), and  <code>han­dle_­first == 'ex­p'</code>  will in­te­grate the log­a­rithms first.  Right now, I have de­fault­ed to 'log' be­cause the ex­po­nen­tial in­te­gra­tion al­go­rithm is slight­ly more com­plete.  If you get  <code>NotIm­ple­ment­ed­Er­ror</code>  with one, it is pos­si­ble (though I don't know for sure yet) that you might get an an­swer with the oth­er.   </p> 
 <p>Al­so, they can give dif­fer­ent look­ing re­sult­s, and at dif­fer­ent speed­s.  For ex­am­ple:</p> 
 <p><strong>Hov­er over the code and click on the left­-­most, "view source" icon (a pa­per icon with  <tt>&lt; &gt;</tt>  over it) to view with­out break­s.  Opens in a new win­dow.</strong></p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: f = (x<em>(x + 1)</em>((x<strong>2<em>ex­p(2</em>x</strong>2) - log(x + 1)<strong>2)</strong>2 +
   ...: 2<em>x</em>ex­p(3<em>x<strong>2)<em>(x - (2</em>x</strong>3 + 2</em>x<strong>2 + x + 1)<em>log(x + 1))))/((x +
   ...: 1)</em>log(x + 1)</strong>2 - (x<strong>3 + x</strong>2)<em>ex­p(2</em>x<strong>2))</strong>2</p> 
 <p>In [2]: f</p> 
 <p>Out­[2]: 
          ⎛                          2                                                   ⎞
          ⎜⎛                       2⎞                                                   2⎟
          ⎜⎜     2           2  2⋅x ⎟        ⎛    ⎛           2      3⎞           ⎞  3⋅x ⎟
x⋅(1 + x)⋅⎝⎝- log (1 + x) + x ⋅ℯ    ⎠  + 2⋅x⋅⎝x - ⎝1 + x + 2⋅x  + 2⋅x ⎠⋅log(1 + x)⎠⋅ℯ    ⎠</p> 
 <p>──────────────────────────────────────────────────────────────────────────────────────────
                                                                2                        <br> 
                         ⎛                                    2⎞                         <br> 
                         ⎜   2                  ⎛ 2    3⎞  2⋅x ⎟                         <br> 
                         ⎝log (1 + x)⋅(1 + x) - ⎝x  + x ⎠⋅ℯ    ⎠                           </p> 
 <p>In [3]: risch_in­te­grate(f, x, han­dle_­first='log')</p> 
 <p>Out­[3]: 
       ⎛              ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞                            <br> 
       ⎜log(1 + x)    ⎝x ⎠⎟                   ⎜  log(1 + x)    ⎝x ⎠⎟          ⎛ 2⎞              <br> 
    log⎜────────── + ℯ    ⎟                log⎜- ────────── + ℯ    ⎟       2  ⎝x ⎠              <br> 
       ⎝    x             ⎠                   ⎝      x             ⎠      x ⋅ℯ    ⋅log(1 + x)   <br> 
x + ─────────────────────── - log(1 + x) - ───────────────────────── + ──────────────────────────
               2                                       2                                        2
                                                                              2           3  2⋅x 
                                                                       - x⋅log (1 + x) + x ⋅ℯ     </p> 
 <p>In [4]: risch_in­te­grate(f, x, han­dle_­first='­ex­p')</p> 
 <p>Out­[4]: 
       ⎛                ⎛ 2⎞⎞                   ⎛                ⎛ 2⎞⎞        ⎛ 2⎞            <br> 
       ⎜                ⎝x ⎠⎟                   ⎜                ⎝x ⎠⎟        ⎝x ⎠            <br> 
    log⎝log(1 + x) + x⋅ℯ    ⎠                log⎝log(1 + x) - x⋅ℯ    ⎠     x⋅ℯ    ⋅log(1 + x)<br> 
x + ───────────────────────── - log(1 + x) - ───────────────────────── - ──────────────────────
                2                                        2                                    2
                                                                            2           2  2⋅x 
                                                                         log (1 + x) - x ⋅ℯ     </p> 
 <p>In [5]: %timeit risch_in­te­grate(f, x, han­dle_­first='log')</p> 
 <p>1 loop­s, best of 3: 1.49 s per loop</p> 
 <p>In [6]: %timeit risch_in­te­grate(f, x, han­dle_­first='­ex­p')</p> 
 <p>1 loop­s, best of 3: 1.21 s per loop</p> 
 <p>In [7]: can­cel(risch_in­te­grate(f, x, han­dle_­first='log').d­if­f(x) - f)</p> 
 <p>Out­[7]: 0</p> 
 <p>In [8]: can­cel(risch_in­te­grate(f, x, han­dle_­first='­ex­p').d­if­f(x) - f)</p> 
 <p>Out­[8]: 0</p> 
 <p>[/­code]</p> 
 <p>So go now, and pull my  <a href="//github.com/asmeurer/sympy/tree/integration3">branch</a>, and try this func­tion out.  And re­port any prob­lems that you have back to me, ei­ther through the mail­ing list, IR­C, is­sue 2010, or as a com­ment to this blog post (I don't re­al­ly care how).</p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/08/05/prototype-risch_integrate-function-ready-for-testing/#disqus_thread" data-disqus-identifier="cache/posts/2010/08/05/prototype-risch_integrate-function-ready-for-testing.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/07/31/integration-of-primitive-functions/" class="u-url">Integration of primitive functions</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-07-31T06:44:31-05:00">2010-07-31 06:44</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><strong>Integration of Primitive Functions</strong>
<p>So this past week, I had an­oth­er break through in my projec­t.  The  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">first break through</a>, as you may re­cal­l, was the com­ple­tion of the  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  func­tion, which al­lowed for the in­te­gra­tion in hy­per­ex­po­nen­tial ex­ten­sion­s, in­clud­ing prov­ing the nonex­is­tence of el­e­men­tary in­te­gral­s.  Now I have worked my way up to this lev­el on the oth­er ma­jor half of the in­te­gra­tion al­go­rithm (ac­tu­al­ly, ma­jor third; more on that lat­er): in­te­gra­tion of prim­i­tive el­e­ments.   </p> 
 <p>This time, I can re­fer you to my  <a href="http://asmeurersympy.wordpress.com/2010/07/24/the-risch-algorithm-part-2-elementary-functions/">pre­vi­ous blog post</a>  for def­i­ni­tion­s.  The chief thing here is that there is now a func­tion in my  <tt>in­te­gra­tion3</tt>  branch called  <code>in­te­grate_prim­i­tive()</code>, and it is used pri­mar­i­ly for in­te­grat­ing func­tions with log­a­rithm­s.</p> 
 <p>So, how about some ex­am­ples?  The first one comes from  <a href="http://">Al­go­rithms for com­put­er al­ge­bra By Kei­th O. Ged­des, Stephen R. Cza­por, George Labahn</a>  (ex­am­ple 12.8).  I like it be­cause it con­tains both ex­po­nen­tials and log­a­rithm­s, in a way that they do not de­pend on each oth­er, so it can be in­te­grat­ed with ei­ther  <code>in­te­grate_prim­i­tive()</code>  or  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>.  In ei­ther case, the poly­no­mi­al part is $la­tex \frac{x}{x + 1}$, so re­cur­sive­ly call­ing the oth­er func­tion is not re­quired.  (for those of you who have been fol­low­ing my  <tt>in­te­gra­tion3</tt>  branch, you may no­tice that this is bla­tant­ly tak­en from the com­mit his­to­ry).</p> 
 <p><strong>Hov­er over the code and click on the left­-­most, "view source" icon (a pa­per icon with  <tt>&lt; &gt;</tt>  over it) to view with­out break­s.  Opens in a new win­dow.</strong></p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port in­te­grate_prim­i­tive,</p> 
 <p>in­te­grate_hy­per­ex­po­nen­tial</p> 
 <p>In [2]: f = (x<em>(x + 1)</em>((x<strong>2<em>ex­p(2</em>x</strong>2) - log(x + 1)<strong>2)</strong>2 +</p> 
 <p>2<em>x</em>ex­p(3<em>x<strong>2)<em>(x - (2</em>x</strong>3 + 2</em>x*<em>2 + x + 1)</em>log(x + 1))))/((x +</p> 
 <p>1)<em>log(x + 1)<strong>2 - (x</strong>3 + x<strong>2)<em>ex­p(2</em>x</strong>2))</em>*2</p> 
 <p>In [3]: f</p> 
 <p>Out­[3]:
          ⎛                          2                                                   ⎞
          ⎜⎛                       2⎞                                                   2⎟
          ⎜⎜     2           2  2⋅x ⎟        ⎛    ⎛           2      3⎞           ⎞  3⋅x ⎟
x⋅(1 + x)⋅⎝⎝- log (1 + x) + x ⋅ℯ    ⎠  + 2⋅x⋅⎝x - ⎝1 + x + 2⋅x  + 2⋅x ⎠⋅log(1 + x)⎠⋅ℯ    ⎠</p> 
 <p>──────────────────────────────────────────────────────────────────────────────────────────
                                                                2
                         ⎛                                    2⎞
                         ⎜   2                  ⎛ 2    3⎞  2⋅x ⎟
                         ⎝log (1 + x)⋅(1 + x) - ⎝x  + x ⎠⋅ℯ    ⎠</p> 
 <p>In [4]: var('t0, t1')</p> 
 <p>Out­[4]: (t₀, t₁)</p> 
 <p>In [5]: a, d = map(lamb­da i: Poly(i, t1), f.­sub­s(­ex­p(x**2),</p> 
 <p>t0).­sub­s(log(x + 1), t1).as_nu­mer_­de­nom())</p> 
 <p>In [6]: a</p> 
 <p>Out­[6]:</p> 
 <p>Poly((x + x<strong>2)*t1</strong>4 + (-2<em>t0<strong>2*x</strong>3 - 2</em>t0<strong>2*x</strong>4)<em>t1</em>*2 +</p> 
 <p>(-2<em>t0<strong>3*x</strong>2 - 4</em>t0<strong>3*x</strong>3 - 6<em>t0<strong>3*x</strong>4 - 8</em>t0<strong>3*x</strong>5 -</p> 
 <p>4<em>t0<strong>3*x</strong>6)</em>t1 + 2<em>t0<strong>3*x</strong>3 + 2</em>t0<strong>3*x</strong>4 + t0<em>   </em>4<em>x</em>*5 +</p> 
 <p>t0<strong>4*x</strong>6, t1, do­main='Z­Z[x,t0]')</p> 
 <p>In [7]: d</p> 
 <p>Out­[7]: Poly((1 + 2<em>x + x<strong>2)*t1</strong>4 + (-2</em>t0<strong>2*x</strong>2 - 4*t0<strong>2*x</strong>3 -</p> 
 <p>2<em>t0<strong>2*x</strong>4)</em>t1<strong>2 + t0</strong>4<em>x<strong>4 + 2*t0</strong>4</em>x<strong>5 + t0</strong>4<em>x</em>*6, t1,</p> 
 <p>do­main='Z­Z[x,t0]')</p> 
 <p>In [8]: D = [Poly(1, x), Poly(2<em>x</em>t0, t0), Poly(1/(x + 1), t1)]</p> 
 <p>In [9]: r = in­te­grate_prim­i­tive(a, d, D, [x, t0, t1], [lamb­da x: log(x +</p> 
 <p>1), lamb­da x: ex­p(x**2)])</p> 
 <p>In [10]: r</p> 
 <p>Out­[10]:</p> 
 <p>⎛   ⎛                ⎛ 2⎞⎞      ⎛                ⎛ 2⎞⎞        ⎛ 2⎞                                ⎞</p> 
 <p>⎜   ⎜                ⎝x ⎠⎟      ⎜                ⎝x ⎠⎟        ⎝x ⎠                ⌠               ⎟</p> 
 <p>⎜log⎝log(1 + x) + x⋅ℯ    ⎠   log⎝log(1 + x) - x⋅ℯ    ⎠     x⋅ℯ    ⋅log(1 + x)     ⎮   x           ⎟</p> 
 <p>⎜───────────────────────── - ───────────────────────── - ────────────────────── + ⎮ ───── dx, True⎟</p> 
 <p>⎜            2                           2                                    2   ⎮ 1 + x         ⎟</p> 
 <p>⎜                                                           2           2  2⋅x    ⌡               ⎟</p> 
 <p>⎝                                                        log (1 + x) - x ⋅ℯ                       ⎠</p> 
 <p>[/­code]</p> 
 <p>An ex­pla­na­tion:   <code>f</code>  is the func­tion we are in­te­grat­ing.  Prepars­ing is not im­ple­ment­ed yet, so we have to do it man­u­al­ly in  <tt>[5]</tt>.   <tt>[8]</tt>  is the list of deriva­tions of the mono­mi­als we are work­ing with,  <code>[x, t0, t1]</code>, which rep­re­sent $la­tex x$, $la­tex e^{x^2}$, and $la­tex \log{(x + 1)}$, re­spec­tive­ly. Be­cause the out­er­most mono­mi­al is a log­a­rithm (prim­i­tive), we call  <code>in­te­grate_prim­i­tive()</code>  on it.  The last ar­gu­ment of the func­tion is the back sub­sti­tu­tion list, in re­verse or­der be­cause that is the or­der we have to back sub­sti­tute in.  We can see the re­sult con­tains an un­eval­u­at­ed In­te­gral.  This is be­cause the re­cur­sive calls to in­te­grate over the small­er ex­ten­sions have not yet been im­ple­ment­ed.  In the fi­nal ver­sion,  <code>in­te­grate()</code>  will au­to­mat­i­cal­ly call  <code>rat­in­t()</code>  in this case on it to give the com­plete an­swer.  The sec­ond ar­gu­ment of the re­sult, True, in­di­cates that the in­te­gral was el­e­men­tary and that this is the com­plete in­te­gral.</p> 
 <p>Be­cause the ex­ten­sions did not de­pend on each oth­er, we could have al­so in­te­grat­ed in $la­tex \math­b­b{Q}(x, \log{(x + 1)}, e^{x^2})$ in­stead of $la­tex \math­b­b{Q}(x, e^{x^2}, \log{(x + 1)})$:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [11]: a1, d1 = map(lamb­da i: Poly(i, t0), f.­sub­s(­ex­p(x**2), t0).­sub­s(log(x + 1), t1).as_nu­mer_­de­nom())</p> 
 <p>In [12]: D1 = [Poly(1, x), Poly(1/(x + 1), t1), Poly(2<em>x</em>t0, t0)]</p> 
 <p>In [13]: r1 = in­te­grate_hy­per­ex­po­nen­tial(a1, d1, D1, [x, t1, t0], [lamb­da x: ex­p(x**2), lamb­da x: log(x + 1)])</p> 
 <p>In [14]: r1</p> 
 <p>Out­[14]:</p> 
 <p>⎛   ⎛              ⎛ 2⎞⎞      ⎛                ⎛ 2⎞⎞                                                ⎞</p> 
 <p>⎜   ⎜log(1 + x)    ⎝x ⎠⎟      ⎜  log(1 + x)    ⎝x ⎠⎟          ⎛ 2⎞                                  ⎟</p> 
 <p>⎜log⎜────────── + ℯ    ⎟   log⎜- ────────── + ℯ    ⎟       2  ⎝x ⎠                  ⌠               ⎟</p> 
 <p>⎜   ⎝    x             ⎠      ⎝      x             ⎠      x ⋅ℯ    ⋅log(1 + x)       ⎮   x           ⎟</p> 
 <p>⎜─────────────────────── - ───────────────────────── + ────────────────────────── + ⎮ ───── dx, True⎟</p> 
 <p>⎜           2                          2                                        2   ⎮ 1 + x         ⎟</p> 
 <p>⎜                                                             2           3  2⋅x    ⌡               ⎟</p> 
 <p>⎝                                                      - x⋅log (1 + x) + x ⋅ℯ                       ⎠</p> 
 <p>[/­code]</p> 
 <p>We can ver­i­fy by tak­ing the de­riv­a­tive that the re­sults in each case are anti­deriv­a­tives of the orig­i­nal func­tion,  <code>f</code>, even though they ap­pear dif­fer­en­t.</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [15]: can­cel(r[0].d­if­f(x) - f)</p> 
 <p>Out­[15]: 0</p> 
 <p>In [16]: can­cel(r1[0].d­if­f(x) - f)</p> 
 <p>Out­[16]: 0</p> 
 <p>[/­code]</p> 
 <p>We can see in each case, the re­main­ing un­eval­u­at­ed  <code>In­te­gral</code>  was in $la­tex \math­b­b{Q}(x)$ on­ly, mean­ing that the re­cur­sive call to  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  or  <code>in­te­grate_prim­i­tive()</code>, re­spec­tive­ly, would not have been nec­es­sary. Fi­nal­ly, we can see that choos­ing the cor­rect ex­ten­sion to in­te­grate over can make a dif­fer­ence, time wise:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [17]: %timeit in­te­grate_prim­i­tive(a, d, D, [x, t0, t1], [lamb­da x: log(x + 1), lamb­da x: ex­p(x**2)])</p> 
 <p>1 loop­s, best of 3: 1.91 s per loop</p> 
 <p>In [18]: %timeit in­te­grate_hy­per­ex­po­nen­tial(a1, d1, D1, [x, t1, t0], [lamb­da x: ex­p(x**2), lamb­da x: log(x + 1)])</p> 
 <p>1 loop­s, best of 3: 2.63 s per loop</p> 
 <p>[/­code]</p> 
 <p>Just as with the ex­po­nen­tial case, the func­tion can prove the in­te­grals are non-ele­men­tary. This is the so-­called  <a href="http://en.wikipedia.org/wiki/Logarithmic_integral">log­a­rith­mic in­te­gral</a>:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [19]: f1 = 1/log(x)</p> 
 <p>In [20]: a, d = map(lamb­da i: Poly(i, t1), f1.­sub­s(log(x), t1).as_nu­mer_­de­nom())</p> 
 <p>In [21]: a</p> 
 <p>Out­[21]: Poly(1, t1, do­main='Z­Z')</p> 
 <p>In [22]: d</p> 
 <p>Out­[22]: Poly(t1, t1, do­main='Z­Z')</p> 
 <p>In [23]: in­te­grate_prim­i­tive(a, d, [Poly(1, x), Poly(1/x, t1)], [x, t1], [log])</p> 
 <p>Out­[23]: (0, False)</p> 
 <p>[/­code]</p> 
 <p>The sec­ond ar­gu­men­t,  <code>False</code>, in­di­cates that the in­te­gral was non-ele­men­tary.  Name­ly, the func­tion has proven that the func­tion $la­tex f - D(0) = \frac{1}{\log{(x)}}$ does not have an el­e­men­tary an­ti-deriva­tive over $la­tex \math­b­b{Q}(x, \log{(x)})$ (see the  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">pre­vi­ous post</a>  for more in­for­ma­tion).</p> 
 <p>Fi­nal­ly, be aware that, just as with  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  many in­te­grals will  raise  <code>NotIm­ple­ment­ed­Er­ror</code>, be­cause the sub­rou­tines nec­es­sary to solve them have not yet been fin­ished.</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [25]: f = log(log(x))**2</p> 
 <p>In [26]: f.d­if­f(x)</p> 
 <p>Out­[26]:</p> 
 <p>2⋅log(log(x))</p> 
 <p>─────────────
   x⋅log(x)</p> 
 <p>In [27]: a, d = map(lamb­da i: Poly(i, t1),</p> 
 <p>can­cel(f.d­if­f(x)).­sub­s(log(x), t0).­sub­s(log(t0), t1).as_nu­mer_­de­nom())</p> 
 <p>In [28]: a</p> 
 <p>Out­[28]: Poly(2*t1, t1, do­main='Z­Z')</p> 
 <p>In [29]: d</p> 
 <p>Out­[29]: Poly(t0*x, t1, do­main='Z­Z[x,t0]')</p> 
 <p>In [30]: D = [Poly(1, x), Poly(1/x, t0), Poly(1/(x*t0), t1)]</p> 
 <p>In [31]: in­te­grate_prim­i­tive(a, d, D, [x, t0, t1], [lamb­da x: log(log(x)), log])</p> 
 <hr>
<p>NotIm­ple­ment­ed­Er­ror: Re­main­ing cas­es for Poly RDE not yet im­ple­ment­ed.</p> 
 <p>[/­code]</p> 
 <p>Now one thing that I want to add from the above ex­am­ples tak­en from the com­mit mes­sage is that log­a­rithms are not the on­ly func­tion that are prim­i­tive.  The Li func­tion (the log­a­rith­mic in­te­gral, as above), con­sid­ered as an el­e­men­tary ex­ten­sion of $la­tex \math­b­b{Q}(x, \log{(x)})$ is al­so prim­i­tive.  But even among the com­mon­ly de­fined el­e­men­tary func­tion­s, there is one oth­er, acr­tan­gents.   </p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [32]: dif­f(atan(x)**2, x)</p> 
 <p>Out­[32]:  </p> 
 <p>2⋅atan(x)</p> 
 <p>─────────
       2 
  1 + x   </p> 
 <p>In [33]: in­te­grate_prim­i­tive(Poly(2*t, t), Poly(1 + x<strong>2, t), [Poly(1, x), Poly(1/(1 + x</strong>2), t)], [x, t], [atan])</p> 
 <p>Out­[33]:  </p> 
 <p>⎛    2         ⎞</p> 
 <p>⎝atan (x), True⎠</p> 
 <p>In [34]: in­te­grate_prim­i­tive(Poly(t, t), Poly(x, t), [Poly(1, x), Poly(1/(1 + x**2), t)], [x, t], [atan])</p> 
 <p>Out­[34]:  </p> 
 <p>⎛⌠                  ⎞</p> 
 <p>⎜⎮ atan(x)          ⎟</p> 
 <p>⎜⎮ ─────── dx, False⎟</p> 
 <p>⎜⎮    x             ⎟</p> 
 <p>⎝⌡                  ⎠</p> 
 <p>[/­code]</p> 
 <p>Due to a bug in the code right now, the fi­nal ver­sion re­turns the non-ele­men­tary in­te­gral in the fi­nal re­sult.  Suf­fice it to say that it has proven that $la­tex \int {\frac{\arc­tan{(x)}}{x} dx}$ is non-ele­men­tary. As far as I know, this is­n't any spe­cial func­tion.  Ac­tu­al­ly, it's just a ran­dom func­tion con­tain­ing arc­tan that looked non-ele­men­tary to me that I plugged in and found out that I was cor­rec­t.  It's very sim­i­lar in form to the  <a href="http://en.wikipedia.org/wiki/Exponential_integral">ex­po­nen­tial in­te­gral</a>  (Ei) or the  <a href="http://en.wikipedia.org/wiki/Sine_integral#Sine_integral">Sine/­Co­sine In­te­gral</a>  (Si/­Ci), which is how I guessed that it would be non-ele­men­tary.  Maybe it should be called ATi().</p> 
 <p><strong>Sta­tus Up­date</strong></p> 
 <p>So it has come to my at­ten­tion that the sug­gest­ed "pen­cils down" date is one week from Mon­day, and the hard "pen­cils down" date is two weeks from Mon­day (see the  <a href="http://socghop.appspot.com/document/show/gsoc_program/google/gsoc2010/timeline">Google Sum­mer of Code Time­line</a>).  Now, no mat­ter how fast I work, my work can­not be pushed in un­til Ma­teusz's lat­est polys branch gets pushed in, be­cause my work is based on top of it.  I plan on con­tin­u­ing work on the in­te­gra­tion al­go­rithm be­yond the sum­mer un­til I fin­ish the tran­scen­den­tal part of the al­go­rith­m, and even af­ter that, I want to look in­to im­ple­ment­ing oth­er in­te­gra­tion re­lat­ed things, like def­i­nite in­te­gra­tion us­ing  <a href="http://en.wikipedia.org/wiki/Meijer-G">Mei­jer G-­func­tion­s,</a>  and the al­ge­bra­ic part of the al­go­rith­m.  But for now, these are the things that I need to do for the tran­scen­den­tal part, which is this sum­mer's work:</p> 
 <p><em>1. Im­ple­ment the prepars­ing al­go­rithm­s.  </em>  This part is two-­fold.  First, I need to im­ple­ment al­go­rithms based on the Risch Struc­ture The­o­rem­s, which al­low me to de­ter­mine if an ex­ten­sion is al­ge­bra­ic or not (if it is al­ge­braic, we can­not in­te­grate it be­cause on­ly the tran­scen­den­tal part is im­ple­ment­ed).  The oth­er part will be the func­tion that ac­tu­al­ly goes through an ex­pres­sion and tries to build up a dif­fer­en­tial ex­ten­sion from it so it can be in­te­grat­ed.  This can be a tricky part. For ex­am­ple, if we want to in­te­grate $la­tex f = e^x + e^{\frac{x}{2}}$, we want to first choose $la­tex t_1=e^{\frac{x}{2}}$ so that $la­tex f = t_1^2 + t_1$, be­cause if we choose $la­tex t_1=e^x$, then $la­tex t_2=e^{\frac{x}{2}}=\sqrt{t_1}$ will be al­ge­bra­ic over $la­tex \math­b­b{Q}(x, t_1)$.  This is one case where we might try adding an al­ge­bra­ic ex­ten­sions but where it can be avoid­ed.  The so­lu­tion will have to be to go through and find the com­mon de­nom­i­na­tors of the ex­po­nen­tial­s.  I'm al­so con­sid­er­ing that this might hap­pen in more ad­vanced ways, so it could be nec­es­sary for the func­tion to back­track in the ex­ten­sion tree to see if it can do it in an en­tire­ly tran­scen­den­tal way.  For­tu­nate­ly, the Risch Struc­ture The­o­rems give us a de­ci­sion pro­ce­dure for de­ter­min­ing if an ex­ten­sion can be writ­ten in terms of the pre­vi­ous ex­ten­sions (is al­ge­bra­ic over it), but this will still be a very hard func­tion to get right.</p> 
 <p><em>2. Fin­ish the re­main­ing cas­es for  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  and  <code>in­te­grate_prim­i­tive()</code>.</em>  As you could see in this post, as well as in the  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">pre­vi­ous one</a>, there are many in­te­grals that can­not yet be in­te­grat­ed be­cause the spe­cial cas­es for them have not been im­ple­ment­ed yet.  Most of these ac­tu­al­ly re­ly on im­ple­ment­ing the struc­ture the­o­rem al­go­rithms from  <strong>1</strong>, and im­ple­ment­ing them once that is fin­ished will not take long, be­cause they will just be straight copy­ing of the pseu­docode from Bron­stein's book.  But some of them, par­tic­u­lar­ly ones from the prim­i­tive case, are not spelt out so well in Bron­stein's book, and will re­quire more think­ing (and thus time) on my part.  I should note that the Struc­ture The­o­rem al­go­rithms are al­so this way.</p> 
 <p><em>  3. Im­ple­ment the hy­per­tan­gent case.  </em>  The abil­i­ty to in­te­grate in tan­gent ex­ten­sions is the oth­er  <em>third</em>  I men­tioned above.  Since tan­gents re­quire more spe­cial cas­ing, I plan on do­ing this on­ly af­ter I have fin­ished  <strong>1</strong>  and  <strong>2</strong>.  This is ac­tu­al­ly not much work, be­cause most of the al­go­rithms for solv­ing the par­tic­u­lar sub­prob­lem for tan­gents (called the  <em>Cou­pled Risch Dif­fer­en­tial Equa­tion</em>) are ex­act­ly the same as those for solv­ing the sub­prob­lem for hy­per­ex­po­nen­tials (the  <em>Risch Dif­fer­en­tial Equa­tion</em>), which are al­ready (most­ly) im­ple­ment­ed in the hy­per­ex­po­nen­tial part.  There are on­ly a few ex­tra func­tions that need to be writ­ten for it.  Al­so, you will still be able to in­te­grate func­tions that con­tain tan­gents, such as $la­tex e^{\­tan{(x)}}$ (re­call  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">last time</a>  that we showed that  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  can prove that this does not have an el­e­men­tary in­te­gral).  It just won't be able to in­te­grate when the top-­most ex­ten­sion is a tan­gen­t.</p> 
 <p>So here is what I plan on do­ing.  Right now, I am go­ing to fo­cus my work on  <strong>1</strong>, since most of  <strong>2</strong>  can't be done un­til it is any­way.  But more im­por­tant­ly, I want to have a pro­to­type user-lev­el func­tion for the Risch Al­go­rith­m.  The rea­son I want this is so that peo­ple can try it out, with­out hav­ing to do the prepars­ing like I did above, but rather they can just call  <code>risch_in­te­grate(f, x)</code>, and it will re­turn the in­te­gral of  <code>f</code>, prove that it is non-ele­men­tary and re­duce it in­to the el­e­men­tary and non-ele­men­tary part­s, or ex­plain why it can­not do it (ei­ther be­cause the func­tion is not tran­scen­den­tal or be­cause some­thing is not im­ple­ment­ed yet).  My chief de­sire for do­ing this is so that peo­ple can try out my code and find the bugs in it for me.  I have al­ready found many crit­i­cal er­rors in the code (re­turns a wrong re­sult), and I want to iron these out be­fore any­thing goes in.  The best way to do this will be to re­lease a work­ing user-lev­el func­tion and hope that peo­ple try it out for me.   </p> 
 <p>Al­so, even if  <strong>2</strong>  and  <strong>3</strong>  are not fin­ished, if I have  <strong>1</strong>, I can in­te­grate it with  <code>in­te­grate()</code>  (no pun in­tend­ed) and just have it bail if it rais­es  <code>NotIm­ple­ment­ed­Er­ror</code>  I will need to come up with a way to dif­fer­en­ti­ate be­tween this and the case where it re­turns an un­eval­u­at­ed  <code>In­te­gral</code>  be­cause it has proven that an el­e­men­tary anti­deriv­a­tive does not ex­ist.  Any sug­ges­tion­s?</p> 
 <p>I plan on con­tin­u­ing work af­ter the sum­mer un­til I fin­ish  <strong>1</strong>  through  <strong>3</strong>, though I won't pre­tend that my work won't slow down con­sid­er­ably when I start class­es in Au­gust.  I al­so prom­ise to fin­ish the  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">Risch Al­go­rithm posts</a>  that I promised.</p> 
 <p>And for what it's worth, I plan on work­ing my ass off this next two week­s.</p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/07/31/integration-of-primitive-functions/#disqus_thread" data-disqus-identifier="cache/posts/2010/07/31/integration-of-primitive-functions.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/07/24/the-risch-algorithm-part-2-elementary-functions/" class="u-url">The Risch Algorithm: Part 2, Elementary Functions</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-07-24T03:32:57-05:00">2010-07-24 03:32</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>In  <a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/">Part 1</a>  of this se­ries of blog post­s, I gave what I be­lieved to be the pre­req­ui­sites to un­der­stand­ing the math­e­mat­ics be­hind the Risch Al­go­rithm (a­side from a ba­sic un­der­stand­ing of de­riv­a­tives and in­te­grals from cal­cu­lus).  In this post, I will elab­o­rate on what is meant by "ele­men­tary func­tion," a term that is thrown around a lot when talk­ing about Risch in­te­gra­tion.</p> 
 <p>The usu­al def­i­ni­tion of el­e­men­tary func­tion giv­en in cal­cu­lus is any func­tion that is a con­stan­t, a poly­no­mi­al, an ex­po­nen­tial ($la­tex e^x$, $la­tex 2^x$), a log­a­rithm ($la­tex \l­n({x})$, $la­tex \log_{10}({x})$), one of the stan­dard trig func­tions or their in­vers­es (s­in, cos, tan, arc­sin, ar­c­cos, arc­tan, etc.), and any com­bi­na­tion of these func­tions via ad­di­tion, sub­trac­tion, mul­ti­pli­ca­tion, di­vi­sion, tak­ing pow­er­s, and com­po­si­tion.  Thus, even a func­tion as crazy as  <a href="2010/07/crazy-function.png"><img src="2010/07/crazy-function.png" alt="" title="crazy function" width="193" height="41" class="alignnone size-full wp-image-632"></a>  is el­e­men­tary, by this def­i­ni­tion.   </p> 
 <p>But for the rig­or­ous def­i­ni­tion of an el­e­men­tary func­tion, we must take in­to con­sid­er­a­tion what field we are work­ing over.  Be­fore I get in­to that, I need some def­i­ni­tion­s.  Sup­pose that $la­tex k$ is the field we are work­ing over.  You can imag­ine that $la­tex k=\­math­b­b{Q}(x)$, the field of ra­tio­nal func­tions in x with ra­tio­nal num­ber co­ef­fi­cients.  As with the pre­vi­ous post, imag­ine $la­tex t$ as a func­tion, for ex­am­ple, $la­tex t = f(x)$.  Let $la­tex K$ be a dif­fer­en­tial ex­ten­sion of $la­tex k$.  We have not de­fined this, but it ba­si­cal­ly means that our deriva­tion $la­tex D$ works the same in $la­tex K$ as it does in $la­tex k$.  You can imag­ine here that $la­tex K=k[t]$.   </p> 
 <p>We say that $la­tex t \in K$ is a  <strong>prim­i­tive</strong>  over $la­tex k$ if $la­tex Dt \in k$.  In oth­er word­s, the de­riv­a­tive of $la­tex t$ is does not con­tain $la­tex t$, on­ly el­e­ments of $la­tex k$.  Ob­vi­ous­ly, by the def­i­ni­tion of a deriva­tion (see the  <a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/">last post</a>  in the se­ries), any el­e­ment of $la­tex k$ is a prim­i­tive over $la­tex K$, be­cause the de­riv­a­tive of any el­e­ment of a field is again an el­e­ment of that field (y­ou can see this by the def­i­ni­tion of a deriva­tion, al­so giv­en in the last post).  But al­so if $la­tex t=log(a)$ for some $la­tex a \in k$, then $la­tex t$ is a prim­i­tive over $la­tex k$, be­cause $la­tex Dt=\frac{­Da}{a}\in k$.   </p> 
 <p>We say that $la­tex t \in K^*$ is a  <strong>hy­per­ex­po­nen­tial</strong>  over $la­tex k$ if $la­tex \frac{Dt}{t}\in k$.  Writ­ten an­oth­er way, $la­tex Dt=at$ for some $la­tex a\in k$.  We know from cal­cu­lus that the func­tions that sat­is­fy dif­fer­en­tial equa­tions of the type $la­tex \frac{dy}{dx}=ay$ are ex­act­ly the ex­po­nen­tial func­tion­s, i.e., $la­tex y=e^{\in­t{a\ dx}}$.   </p> 
 <p>The last class of func­tions that needs to be con­sid­ered is  <strong><a href="http://en.wikipedia.org/wiki/Algebraic_function">al­ge­bra­ic func­tions</a></strong>.  I will not go in­to depth on al­ge­bra­ic func­tion­s, be­cause my work this sum­mer is on­ly on in­te­grat­ing pure­ly tran­scen­den­tal func­tion­s.  There­fore, the on­ly con­cern we shall have with al­ge­bra­ic func­tions in re­la­tion to the in­te­gra­tion al­go­rithm is to make sure that what­ev­er func­tion we are in­te­grat­ing is  <em>not</em>  al­ge­braic, be­cause the tran­scen­den­tal al­go­rithms will not be valid if they are.  Hope­ful­ly in a fu­ture post I will be able to dis­cuss the Risch Struc­ture The­o­rem­s, which give nec­es­sary and suf­fi­cient con­di­tions for de­terming if a Li­ou­vil­lian func­tion (see next para­graph) is al­ge­bra­ic.   </p> 
 <p>Now, we say that a func­tion $la­tex t \in K$ is  <strong>Li­ou­vil­lian</strong>  over $la­tex k$ if $la­tex t$ is al­ge­braic, a prim­i­tive, or a hy­per­ex­po­nen­tial over $la­tex k$.  For $la­tex t\in K$ to be a  <strong>Li­ou­vil­lian mono­mi­al</strong>  over $la­tex k$, we have the ad­di­tion­al con­di­tion that $la­tex \math­rm{­Con­st}(k) = \math­rm{­Con­st}(k(t))$. This just means that we can­not con­sid­er some­thing like $la­tex \log({2})$ over $la­tex \math­b­b{Q}$ as a Li­ou­vil­lian mono­mi­al.  Oth­er­wise (I be­lieve) we could run in­to un­de­cid­abil­i­ty prob­lem­s.   </p> 
 <p>We call $la­tex t \in K$ a  <strong>log­a­rithm</strong>  over $la­tex k$ if $la­tex Dt=\frac{D­b}{b}$ for some $la­tex b \in k^<em>$, i.e., $la­tex t=\log({b})$.  We call $la­tex t \in K^</em>$ an  <strong>ex­po­nen­tial</strong>  over $la­tex k$ if $la­tex \frac{Dt}{t}=D­b$ (or $la­tex Dt=t­D­b$) for some $la­tex b \in k$, i.e., $la­tex t=e^b$.  Note the dif­fer­ence be­tween an  <em>ex­po­nen­tial</em>  mono­mi­al and a  <em>hy­per­ex­po­nen­tial</em>  mono­mi­al.   </p> 
 <p>We can fi­nal­ly give the rig­or­ous def­i­ni­tion of an el­e­men­tary ex­ten­sion.  $la­tex K$ is an  <strong>el­e­men­tary ex­ten­sion</strong>  of $la­tex k$ if there are $la­tex t_1, \dot­s, t_n \in K$ such that $la­tex K=k(t_1,\­dot­s,t_n)$ and $la­tex t_i$ is el­e­men­tary over $la­tex k(t_1, \dot­s, t_{i-1})$ for all $la­tex i \in {1,\­dot­s,n}$.  An  <strong>el­e­men­tary func­tion  </strong>  is any el­e­ment of an el­e­men­tary ex­ten­sion of $la­tex \math­b­b{C}(x)$ with the deriva­tion $la­tex D=\frac{d}{dx}$.  A func­tion $la­tex f\in k$ has an  <strong>el­e­men­tary in­te­gral</strong>  over $la­tex k$ if there ex­ists an el­e­men­tary ex­ten­sion $la­tex K$ of $la­tex k$ and $la­tex g\in K$ such that $la­tex Dg=f$, i.e., $la­tex f=\in­t{g}$.   </p> 
 <p>Usu­al­ly, we start with $la­tex \math­b­b{Q}(x)$, the field of ra­tio­nal func­tions in x with ra­tio­nal num­ber co­ef­fi­cients. We then build up an el­e­men­tary ex­ten­sion one func­tion at a time, with each func­tion ei­ther be­ing a log­a­rithm or ex­po­nen­tial of what we have al­ready built up, or al­ge­bra­ic over it.  As I not­ed above, we will ig­nore al­ge­bra­ic func­tions here.  We gen­er­al­ly start with $la­tex \math­b­b{Q}$ be­cause it is com­putable (im­por­tant prob­lems such as the ze­ro equiv­a­lence prob­lem or the prob­lem of de­ter­min­ing cer­tain field iso­mor­phisms are de­cid­able), but the above def­i­ni­tion lets us start with any sub­field of $la­tex \math­b­b{C}$.   </p> 
 <p>Now you may be won­der­ing: we've cov­ered al­ge­bra­ic func­tion­s, ex­po­nen­tials and log­a­rithm­s, and ob­vi­ous­ly ra­tio­nal func­tions are el­e­ments of $la­tex \math­b­b{Q}(x)$, but what about trigono­met­ric func­tion­s?  Well, from a the­o­ret­i­cal stand point, we can make our lives eas­i­er by notic­ing that all the com­mon trigono­met­ric func­tions can be rep­re­sent­ed as ex­po­nen­tials and log­a­rithms over $la­tex \math­b­b{Q}(i)$.  For ex­am­ple, $la­tex \cos{x} = \frac{e^{ix} + e^{-ix}}{2}$.  You can see  <a href="http://en.wikipedia.org/wiki/Trig_identities#Exponential_definitions">here</a>  that all the com­mon trig func­tions can be rep­re­sent­ed as com­plex ex­po­nen­tials or log­a­rithms like this.  How­ev­er, from an al­go­rith­mic stand­point, we don't want do con­vert all trig ex­pres­sions in­to com­plex ex­po­nen­tials and log­a­rithms in or­der to in­te­grate them.  For one thing, our fi­nal re­sult will be in terms of com­plex ex­po­nen­tials and log­a­rithm­s, not the orig­i­nal func­tions we start­ed with, and con­vert­ing them back may or may not be an easy thing to do.  Al­so, aside from the fact that we have dif­fer­ent func­tions than we were ex­pect­ing, we al­so will end up with an an­swer con­tain­ing $la­tex \sqrt{-1}$, even if our orig­i­nal in­te­grand did not.   </p> 
 <p>For­tu­nate­ly, the in­te­grat­ing tan­gents di­rect­ly is a solved prob­lem, just like in­te­grat­ing al­ge­braic, ex­po­nen­tial, or log­a­rith­mic func­tions is solved.  We can't in­te­grate func­tions like $la­tex \s­in{x}$ or $la­tex \cos{x}$ di­rect­ly as mono­mi­als like we can with $la­tex \tan{x}$ or $la­tex e^x$, be­cause the de­riv­a­tives of sin and cos are not poly­no­mi­als in their re­spec­tive selves with co­ef­fi­cients in $la­tex \math­b­b{C}(x)$.  How­ev­er, we can use a trick or two to in­te­grate them.  One way is to re­write $la­tex \cos{x}=\frac{1 - \tan^2{\frac{x}{2}}}{1 + \tan^2{\frac{x}{2}}}$ and pro­ceed to in­te­grate it as a tan­gen­t.  An­oth­er al­ter­na­tive is to write $la­tex \cos{x}=\frac{1}{\sec{x}}=\sqrt{\frac{1}{\sec^2{x}}}=\sqrt{\frac{1}{\­tan^2{x} + 1}}$.  This func­tion is al­ge­bra­ic over $la­tex \math­b­b{Q}(x, \tan{(x)})$, but if we do not al­ready have $la­tex \tan{x}$ in our dif­fer­en­tial ex­ten­sion, it is tran­scen­den­tal, and we can re­write it as $la­tex e^{-\frac{\log{(1 + \tan^2{x})}}{2}}$ (this is used in Bron­stein's tex­t, so I be­lieve what I just said is cor­rec­t, though I haven't ver­i­fied it with the struc­ture the­o­rems just yet).   These both work us­ing the rel­e­vant iden­ti­ties for sin too.  Of course, there is still the prob­lem of rewrit­ing the fi­nal in­te­grand back in terms of sin or cos.  Oth­er­wise, you will get some­thing like $la­tex \frac{2e^x\­tan({\frac{x}{2}}) - \tan^2({\frac{x}{2}})e^x + e^x}{2 + 2\­tan^2({\frac{x}{2}})}$ in­stead of $la­tex \frac{e^x(\s­in{(x)} + \cos{(x)})}{2}$ for $la­tex \in­t{\­cos{(x)}e^xdx}$.  Bron­stein does­n't elab­o­rate on this too much in his book, so it is some­thing that I will have to fig­ure out on my own.</p> 
 <p>The sec­ond op­tion I gave above leads nice­ly in­to the main point I want­ed to make here about el­e­men­tary func­tion­s.  No­tice that ev­ery­where in the def­i­ni­tions above, things de­pend on the field we are work­ing in.  There­fore, $la­tex e^{\­tan{x}}$ can­not be an el­e­men­tary ex­ten­sion over $la­tex \math­b­b{Q}(x)$, but it can be over $la­tex \math­b­b{Q}(x, \tan{x})$.  Al­so, the  <a href="http://en.wikipedia.org/wiki/Error_function">er­ror func­tion</a>, de­fined as $la­tex \math­rm{er­f}{(x)} = \frac{2}{\sqrt{\pi}}\in­t{e^{-x^2}dx}$ can­not be an el­e­men­tary ex­ten­sion over $la­tex \math­b­b{Q}(x)$, but it can over $la­tex \math­b­b{Q}(x, e^{-x^2})$. In fact this is how we can in­te­grate in terms of some spe­cial func­tion­s, in­clud­ing the er­ror func­tion: by man­u­al­ly adding $la­tex e^{-x^2}$ (or what­ev­er) to our dif­fer­en­tial ex­ten­sion.   There­fore, the usu­al def­i­ni­tion of an el­e­men­tary an­ti-derivaitve and the above Risch Al­go­rithm def­i­ni­tion of an el­e­men­tary in­te­gral co­in­cide on­ly when the ex­ten­sion con­sists on­ly of el­e­men­tary func­tions of the form of the usu­al def­i­ni­tion (note that above, our fi­nal fields are $la­tex \math­b­b{Q}(x, \tan{x}, e^{\­tan{x}})$ and $la­tex \math­b­b{Q}(x, e^{-x^2}, \math­rm{er­f}{(x)})$, re­spec­tive­ly).   </p> 
 <p>Orig­i­nal­ly, I was al­so go­ing to talk about Li­ou­ville's The­o­rem in this blog post, but I think it has al­ready got­ten long enough (read "I'm get­ting tired"), so I'll put that off un­til next time.   </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/07/24/the-risch-algorithm-part-2-elementary-functions/#disqus_thread" data-disqus-identifier="cache/posts/2010/07/24/the-risch-algorithm-part-2-elementary-functions.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/07/17/a-hard-week/" class="u-url">A hard week</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-07-17T04:38:22-05:00">2010-07-17 04:38</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>Af­ter last week's  <a href="http://asmeurersympy.wordpress.com/2010/07/12/integration-of-exponential-functions/">break­through</a>, work this week has been very slow.  I start­ed work­ing on the Para­met­ric Risch Dif­fer­en­tial Equa­tion Prob­lem, which is al­most iden­ti­cal to the Risch Dif­fer­en­tial Equa­tion Prob­lem in how it is solved, ex­cept there are a few ex­tra step­s.  Un­for­tu­nate­ly, be­cause it is so sim­i­lar, Bron­stein breezes through the de­scrip­tion.  This is fine for the parts that are the same, but he is a lit­tle un­clear on how some of the new parts fit in.  Al­so, his pseu­docode has a line more or less say­ing  </p> 
 <p>[code]</p> 
 <p>if r1 = … = rn = 0 then
    N = -1
else
    N = max(deg(r1), …, deg(rn))</p> 
 <p>for i from 0 to N
    for j from 1 to m
        Mij = co­ef­fi­cien­t(r­j, t^i)</p> 
 <p>[/­code]</p> 
 <p>where M is a ma­trix.  It is not very clear what this is sup­posed to mean in the case where N = -1.  Ob­vi­ous­ly, you can't have a a ma­trix with neg­a­tive di­men­sion­s.  Clear­ly, this means that this par­tic­u­lar func­tion does­n't ap­ply some­how in this case, but I am not re­al­ly even sure where it fits in to the whole al­go­rithm at this point in read­ing.  Af­ter read­ing a few more pages in, it gives a few hints here and there on how it is to be used, but nev­er is it ex­plic­it­ly shown, in pseu­docode or oth­er­wise.  So for now, I think my best bet is to read ahead and get a fuller un­der­stand­ing of the com­plete func­tion be­fore I try im­ple­ment­ing any­thing (this is what I had been do­ing be­fore, but I caught up to my­self).   </p> 
 <p>Al­so, on an un­re­lat­ed note, I just found out to­day that I passed my  <a href="http://socghop.appspot.com/document/show/gsoc_program/google/gsoc2010/faqs#evaluations">Google Sum­mer of Code midterm eval­u­a­tion</a>.  This means that I will re­ceive half of my stipend for the pro­gram (the oth­er half comes af­ter pass­ing the fi­nal eval­u­a­tion at the end of the sum­mer), and that I can con­tin­ue work­ing on my project in the pro­gram.   </p> 
 <p>ED­IT:</p> 
 <p>Lat­er in the tex­t, it runs through an ex­am­ple and says "… $la­tex dc = -1$, hence M and A are 0 by 0 ma­tri­ces."  So ob­vi­ous­ly, that is what was mean­t.   </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/07/17/a-hard-week/#disqus_thread" data-disqus-identifier="cache/posts/2010/07/17/a-hard-week.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/07/12/integration-of-exponential-functions/" class="u-url">Integration of exponential functions</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-07-12T06:22:06-05:00">2010-07-12 06:22</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>So for the first time this sum­mer, I missed my blog­ging dead­line.  I have been on va­ca­tion for the past few week­s, and have spent a good bit of the last week in the car, driv­ing home. But that's not my ex­cuse.  I was on va­ca­tion the week be­fore, when I wrote up my  <a href="http://asmeurersympy.wordpress.com/2010/06/30/the-risch-algorithm-part-1/">lengthy blog post on the Risch Al­go­rithm</a>.  My ex­cuse is that I want­ed to fin­ish up my  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  func­tion be­fore I post­ed, so I could write about it.  Well, I fin­ished it on Thurs­day (to­day is Sun­day, the post was due Fri­day), but I ran in­to un­ex­pect­ed bugs (imag­ine that) that has post­poned it ac­tu­al­ly work­ing un­til now. I al­so end­ed up do­ing API changes 3 dif­fer­ent times (they are ba­si­cal­ly in­cre­men­tal­ly all one change, from sup­port­ing on­ly one ex­ten­sion to prop­er­ly sup­port­ing mul­ti­ple ex­ten­sion­s.  Look for long com­mits in my re­cent com­mit his­to­ry in my branch if you are in­ter­est­ed).   </p> 
 <p>So here is the func­tion.  It in­te­grates ex­po­nen­tial func­tion­s.  You still have to man­u­al­ly cre­ate the dif­fer­en­tial ex­ten­sion, as be­fore.  Here are some ex­am­ples.  You can try them in my  <a href="http://github.com/asmeurer/sympy/tree/integration2">in­te­gra­tion2</a>  branch (I have re­based over Ma­teusz's lat­est polys9up­date.  The lat­est branch is al­ways in­te­gra­tion<code>n</code>, where  <code>n</code>  is the largest in­te­ger avail­able).   </p> 
 <p><strong>Hov­er over the code and click on the left­-­most, "view source" icon (a pa­per icon with  <tt>&lt; &gt;</tt>  over it) to view with­out break­s.  Opens in a new win­dow.</strong></p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t1, t')</p> 
 <p>Out­[2]: (t₁, t)</p> 
 <p>In [3]: r = ex­p(2<em>tan(x))</em>tan(x) + tan(x) + ex­p(­tan(x))</p> 
 <p>In [4]: r</p> 
 <p>Out­[4]: 
 2⋅­tan(x)                    tan(x)
ℯ        ⋅tan(x) + tan(x) + ℯ       </p> 
 <p>In [5]: rd = r.d­if­f(x)</p> 
 <p>In [6]: rd</p> 
 <p>Out­[6]: 
    ⎛         2   ⎞  2⋅­tan(x)             2      ⎛       2   ⎞  2⋅­tan(x)   ⎛       2   ⎞  tan(x)
1 + ⎝2 + 2⋅­tan (x)⎠⋅ℯ        ⋅tan(x) + tan (x) + ⎝1 + tan (x)⎠⋅ℯ         + ⎝1 + tan (x)⎠⋅ℯ       </p> 
 <p>In [7]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­tan(x), t1).­sub­s(­ex­p(t1), t).as_nu­mer_­de­nom()) # Man­u­al­ly cre­ate the ex­ten­sion</p> 
 <p>In [8]: a</p> 
 <p>Out­[8]: Poly((1 + 2<em>t1 + t1<strong>2 + 2*t1</strong>3)</em>t<strong>2 + (1 + t1</strong>2)<em>t + 1 + t1</em>*2, t, do­main='Z­Z[t1]')</p> 
 <p>In [9]: d</p> 
 <p>Out­[9]: Poly(1, t, do­main='Z­Z')</p> 
 <p>In [10]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(1 + t1<strong>2, t1), Poly((1 + t1</strong>2)*t, t)], [x, t1, t], [lamb­da x: ex­p(­tan(x)), tan])</p> 
 <p>Out­[10]:  </p> 
 <p>⎛                   ⌠                                 ⎞</p> 
 <p>⎜ 2⋅­tan(x)          ⎮ ⎛       2   ⎞       tan(x)      ⎟</p> 
 <p>⎜ℯ        ⋅tan(x) + ⎮ ⎝1 + tan (x)⎠ dx + ℯ      , True⎟</p> 
 <p>⎝                   ⌡                                 ⎠</p> 
 <p>[/­code]</p> 
 <p>We have to man­u­al­ly build up the dif­fer­en­tial ex­ten­sion (<code>[7]</code>).  The first el­e­ment is $la­tex x$, which is al­ready there.  Nex­t, we add $la­tex t_1 = \tan{x}$, and fi­nal­ly $la­tex t = e^{\­tan{x}} = e^{t_1}$.  The third ar­gu­ment of  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>  is what gives these vari­ables their iden­ti­ties: their de­riv­a­tives.  The fourth ar­gu­ment is the list of the ex­ten­sion sym­bol­s, and the last ar­gu­ment is a list of the func­tions for which the sym­bols stand for, in re­verse or­der (be­cause we have to back sub­sti­tute in the so­lu­tion in re­verse or­der).   </p> 
 <p>The un­eval­u­at­ed In­te­gral in the so­lu­tion is due to the re­cur­sive na­ture of the Risch al­go­rith­m.  Even­tu­al­ly, an out­er func­tion in the al­go­rithm will re­cur­sive­ly in­te­grate un­til it reach­es the ground field, $la­tex \math­b­b{Q}$.  It will al­so do the prop­er prepars­ing au­to­mat­i­cal­ly as well.  The sec­ond el­e­ment of the so­lu­tion,  <code>True</code>, in­di­cates that the in­te­gral is el­e­men­tary, and thus the giv­en so­lu­tion is the com­plete in­te­gral of the orig­i­nal in­te­grand, which we can see ($la­tex \int (1 + \tan^2{x})dx=\­tan{x}$).   </p> 
 <p>An­oth­er ex­am­ple:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t')</p> 
 <p>Out­[2]: (t,)</p> 
 <p>In [3]: rd = ex­p(-x**2)</p> 
 <p>In [4]: rd</p> 
 <p>Out­[4]: 
   2
 -x 
ℯ    </p> 
 <p>In [5]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­ex­p(x**2), t).as_nu­mer_­de­nom())</p> 
 <p>In [6]: a</p> 
 <p>Out­[6]: Poly(1, t, do­main='Z­Z')</p> 
 <p>In [7]: d</p> 
 <p>Out­[7]: Poly(t, t, do­main='Z­Z')</p> 
 <p>In [8]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(2<em>x</em>t, t)], [x, t], [lamb­da x: ex­p(x**2)])</p> 
 <p>Out­[8]: (0, False)</p> 
 <p>[/­code]</p> 
 <p>Here the sec­ond ar­gu­ment of the so­lu­tion is  <code>False</code>, which in­di­cates that the al­go­rithm has proven that the in­te­gral of $la­tex e^{-x^2}$ is not el­e­men­tary!   The first ar­gu­ment 0 in­di­cates that ac­tu­al­ly it is the in­te­gral of $la­tex e^{-x^2} - \frac{d}{dx}(0)$ that is not el­e­men­tary, i.e., the Risch al­go­rithm will re­duce an in­te­grand in­to an in­te­grat­ed func­tion part and non-ele­men­tary part.  For ex­am­ple:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t1, t')</p> 
 <p>Out­[2]: (t₁, t)</p> 
 <p>In [3]: rd = ex­p(x)/­tan(x) + ex­p(x)/(1 + ex­p(x))</p> 
 <p>In [4]: rd</p> 
 <p>Out­[4]: 
   x        x<br> 
  ℯ        ℯ  <br> 
────── + ──────
     x   tan(x)
1 + ℯ           </p> 
 <p>In [5]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­ex­p(x), t).­sub­s(­tan(x), t1).as_nu­mer_­de­nom())</p> 
 <p>In [6]: a</p> 
 <p>Out­[6]: Poly(t*<em>2 + (1 + t1)</em>t, t, do­main='Z­Z[t1]')</p> 
 <p>In [7]: d</p> 
 <p>Out­[7]: Poly(t1*t + t1, t, do­main='Z­Z[t1]')</p> 
 <p>In [8]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(1 + t1**2, t1), Poly(t, t)], [x, t1, t], [ex­p, tan])</p> 
 <p>Out­[8]:  </p> 
 <p>⎛   ⎛     x⎞       ⎞</p> 
 <p>⎝log⎝1 + ℯ ⎠, False⎠</p> 
 <p>[/­code]</p> 
 <p>This in­di­cates that the in­te­gral of $la­tex (\frac{e^x}{\­tan{x}} + \frac{e^x}{1 + e^x}) - \frac{d}{dx}(\log{(1 + e^x)}) = \frac{e^x}{\­tan{x}}$ is not el­e­men­tary.  That is one ad­van­tage that the new al­go­rithm will have over the present one.  Cur­rent­ly, the present al­go­rithm just re­turns an un­eval­u­at­ed In­te­gral for the above  <code>rd</code>, but the new one will be able to re­turn $la­tex \log{(1 + e^x)} + \in­t{\frac{e^x}{\­tan{x}}dx}$.  It will be able to do this even if rd were rewrit­ten as $la­tex \frac{e^x \tan{x} + e^x + e^{2x}}{e^x \tan{x} + \tan{x}}$ (no­tice that this is ex­act­ly what  <code>.as_nu­mer_­de­nom()</code>  is do­ing any­way in  <code>[5]</code>, as you can see in  <code>[6]</code>  and  <code>[7]</code>).  Fur­ther­more, it will have ac­tu­al­ly  <em>proven</em>  that the re­main­ing $la­tex \in­t{\frac{e^x}{\­tan{x}}dx}$ is non-ele­men­tary.  I plan on hav­ing some kind of mark­er in the pret­ty print­ed un­eval­u­at­ed  <code>In­te­gral</code>  to in­di­cate this.  Sug­ges­tions on what this should be are wel­come.   </p> 
 <p>Fi­nal­ly, the full al­go­rithm ap­pears to be faster (prob­a­bly asymp­tot­i­cal­ly faster) than the cur­rent im­ple­men­ta­tion:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t1, t')</p> 
 <p>Out­[2]: (t₁, t)</p> 
 <p>In [3]: rd = ex­p(x)<em>x</em>*4</p> 
 <p>In [4]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­ex­p(x), t).as_nu­mer_­de­nom())</p> 
 <p>In [5]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(t, t)], [x, t], [lamb­da x: ex­p(x)])</p> 
 <p>Out­[5]:  </p> 
 <p>⎛    x    4  x         x      3  x       2  x      ⎞</p> 
 <p>⎝24⋅ℯ  + x ⋅ℯ  - 24⋅x⋅ℯ  - 4⋅x ⋅ℯ  + 12⋅x ⋅ℯ , True⎠</p> 
 <p>In [6]: %timeit in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(t, t)], [x, t], [ex­p])</p> 
 <p>10 loop­s, best of 3: 28 ms per loop</p> 
 <p>In [7]: in­te­grate(rd, x)</p> 
 <p>Out­[7]: 
    x    4  x         x      3  x       2  x
24⋅ℯ  + x ⋅ℯ  - 24⋅x⋅ℯ  - 4⋅x ⋅ℯ  + 12⋅x ⋅ℯ  </p> 
 <p>In [8]: %timeit in­te­grate(rd, x)</p> 
 <p>1 loop­s, best of 3: 218 ms per loop</p> 
 <p>[/­code]</p> 
 <p>Of course, keep in mind that I am tim­ing what will be an in­ter­nal func­tion against a full func­tion.  But if you in­crease the ex­po­nent on x, you find that there is no way the ad­di­tion of prepars­ing time (which should­n't be af­fect­ed by such a change) will cause it to be­come as slow as the cur­rent  <code>in­te­grate()</code>.  Like I said, I am pret­ty sure that it is as­ymp­tot­ic.  For ex­am­ple:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t1, t')</p> 
 <p>Out­[2]: (t₁, t)</p> 
 <p>In [3]: rd = ex­p(x)<em>x</em>*10</p> 
 <p>In [4]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­ex­p(x), t).as_nu­mer_­de­nom())</p> 
 <p>In [5]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(t, t)], [x, t], [lamb­da x: ex­p(x)])</p> 
 <p>Out­[5]:  </p> 
 <p>⎛         x    10  x              x           3  x          5  x        7  x       9  x       8  x         6  x           4  x            2  x      ⎞</p> 
 <p>⎝3628800⋅ℯ  + x  ⋅ℯ  - 3628800⋅x⋅ℯ  - 604800⋅x ⋅ℯ  - 30240⋅x ⋅ℯ  - 720⋅x ⋅ℯ  - 10⋅x ⋅ℯ  + 90⋅x ⋅ℯ  + 5040⋅x ⋅ℯ  + 151200⋅x ⋅ℯ  + 1814400⋅x ⋅ℯ , True⎠</p> 
 <p>In [6]: %timeit in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(t, t)], [x, t], [ex­p])</p> 
 <p>10 loop­s, best of 3: 42 ms per loop</p> 
 <p>In [7]: in­te­grate(rd, x)</p> 
 <p>Out­[7]: 
         x    10  x              x           3  x          5  x        7  x       9  x       8  x         6  x           4  x            2  x
3628800⋅ℯ  + x  ⋅ℯ  - 3628800⋅x⋅ℯ  - 604800⋅x ⋅ℯ  - 30240⋅x ⋅ℯ  - 720⋅x ⋅ℯ  - 10⋅x ⋅ℯ  + 90⋅x ⋅ℯ  + 5040⋅x ⋅ℯ  + 151200⋅x ⋅ℯ  + 1814400⋅x ⋅ℯ  </p> 
 <p>In [8]: %timeit in­te­grate(rd, x)</p> 
 <p>1 loop­s, best of 3: 2.78 s per loop</p> 
 <p>[/­code]</p> 
 <p>There is one thing I should men­tion.  I haven't im­ple­ment­ed all the cas­es in  <code>rischDE()</code>, which is the sub­prob­lem for ex­po­nen­tial func­tions (more on this in a fu­ture "The Risch Al­go­rith­m" post).  So some in­te­grals will fail with a  <code>NotIm­ple­ment­ed­Er­ror</code>, in­di­cat­ing that there is a func­tion that I still need to im­ple­ment to solve the in­te­gral:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>In [1]: from sympy.in­te­gral­s.risch im­port *</p> 
 <p>In [2]: var('t1, t')</p> 
 <p>Out­[2]: (t₁, t)</p> 
 <p>In [3]: rd = (ex­p(x) - x<em>ex­p(2</em>x)*­tan(x))/­tan(x)</p> 
 <p>In [4]: a, d = map(lamb­da i: Poly(i, t), rd.­sub­s(­ex­p(x), t).­sub­s(­tan(x), t1).as_nu­mer_­de­nom())</p> 
 <p>In [5]: a</p> 
 <p>Out­[5]: Poly(-t1<em>x</em>t**2 + t, t, do­main='Z­Z[x,t1]')</p> 
 <p>In [6]: d</p> 
 <p>Out­[6]: Poly(t1, t, do­main='Z­Z[t1]')</p> 
 <p>In [7]: in­te­grate_hy­per­ex­po­nen­tial(a, d, [Poly(1, x), Poly(1 + t1**2, t1), Poly(t, t)], [x, t1, t], [ex­p, tan])</p> 
 <hr>
<p>...</p> 
 <p>NotIm­ple­ment­ed­Er­ror: The abil­i­ty to solve the para­met­ric log­a­rith­mic de­riv­a­tive prob­lem is re­quired to solve this RDE</p> 
 <p>[/­code]</p> 
 <p>So feel free to give this a try and let me know what you think.  You will have to do the prepars­ing as I have done above, which means that you al­so have to be care­ful that any ex­ten­sion that you make is not the de­riv­a­tive or log­a­rith­mic de­riv­a­tive of an el­e­ment of the field you have al­ready built up.  You al­so can­not use al­ge­bra­ic func­tion­s, as I men­tioned be­fore, in­clud­ing things like $la­tex e^\frac{\log{x}}{2}$ (func­tions like these are called the log­a­rith­mic de­riv­a­tives of k(t)-rad­i­cal­s, which I will al­so dis­cuss in a fu­ture "The Risch Al­go­rith­m" post).  If you just use sim­ple ex­ten­sions like  <code>t1 = tan(x);t=­ex­p(x)</code>  like I have above, you won't need to wor­ry about this.  Each de­riv­a­tive Poly should be in the vari­able that it is the de­riv­a­tive of (e.g., start with  <code>Poly(1, x)</code>, then add  <code>Poly(1 + t1<strong>2, t1)</strong></code>,  <code>Poly(t2*(1 + t12), t2)</code>, etc.).  Ev­ery­thing else should be a Poly in  <code>t</code>, the last el­e­ment of the ex­ten­sion.  And in cause you did­n't get it, the last ex­ten­sion must be an ex­po­nen­tial func­tion.   </p> 
 <p>Al­so, I did­n't have to do it in any of the above ex­am­ples, but the first and sec­ond ar­gu­ments to  <code>in­te­grate_hy­per­ex­po­nen­tial()</code>   <em>must</em>  be can­celed (<code>a, d = a.­can­cel(d, in­clude=True)</code>  will do this for you), or you will get a wrong re­sult!  I spent a good day of de­bug­ging un­til I fig­ured this out.  The ex­is­tence of oth­er bugs did­n't help.</p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/07/12/integration-of-exponential-functions/#disqus_thread" data-disqus-identifier="cache/posts/2010/07/12/integration-of-exponential-functions.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/06/30/the-risch-algorithm-part-1/" class="u-url">The Risch Algorithm: Part 1</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-06-30T03:43:00-05:00">2010-06-30 03:43</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>My work this week is­n't very in­ter­est­ing, even in­so­much as my work any week is in­ter­est­ing, so this week I have elect­ed to start a se­ries of blog posts about the Risch Al­go­rithm in gen­er­al.  I will start out with the ba­sics in this post.</p> 
 <p>Any­one who has tak­en Cal­cu­lus knows a hand­ful of heuris­tics to cal­cu­late in­te­gral­s.  u-­sub­sti­tu­tion, par­tial frac­tion­s, in­te­gra­tion by part­s, trigono­met­ric sub­sti­tu­tion, and ta­ble in­te­gra­tion are a few of the more pop­u­lar ones.  These are gen­er­al enough to work for most in­te­grals that are en­coun­tered in prob­lems in Physic­s, En­gi­neer­ing, and so on, as well as most of those gen­er­at­ed by solv­ing dif­fer­en­tial equa­tions from the same field­s.  But these fall short in a cou­ple of ways.  First of­f, they are just heuris­tic­s.  If they fail, it does not mean that no in­te­gral ex­ist­s.  This means that they are use­less for prov­ing that cer­tain func­tion­s, such as $la­tex e^{-x^2}$ do not have in­te­gral­s, no mat­ter how hard you try to find them.  Sec­ond, they work for on­ly rel­a­tive­ly sim­ple func­tion­s.  For ex­am­ple, sup­pose you have a ra­tio­nal func­tion in $la­tex \log{x}$ and $la­tex x$.  An ex­am­ple would be $la­tex \frac{(\log{x})^2 + 2\log{x} + x^2 + 1}{x\log{x} + 2x^3}$.  We are not in­ter­est­ed in in­te­grat­ing this func­tion, but rather in find­ing it back giv­en its deriva­tive, $la­tex - \frac{1 + 7 x^{2} \log{x} + \log{x} + (\log{x})^2 + 3 x^{2} + 6 x^{2} (\log{x})^2 + (\log{x})^3 + 2 x^{4}}{4 x^{4} \log{x} + x^{2} (\log{x})^2 + 4 x^{6}}$.  The on­ly method I named above that would come even close to be­ing ap­pli­ca­ble to this in­te­grand is par­tial frac­tion­s.  This re­quires mul­ti­vari­ate par­tial frac­tion de­com­po­si­tion (with re­spect to $la­tex x$ and $la­tex \log{x}$), and gives $la­tex -{\frac {2\,{x}^{2}+1+\log{x} }{{x}^{2}}}+{\frac {-1+8\,{x}^{4}-16\,{x}^{6}-{x}^{2}}{ \left( \log{x} +2\,{x}^{2} \right) ^{2}{x}^{2}}}+{\frac {-3\,{x}^{2}+12\,{x}^{4}-1}{ \left( \log{x} +2\,{x}^{2} \right) {x}^{2}}}$, which brings us no clos­er to a so­lu­tion.  </p> 
 <p>The rea­son that I start­ed with a func­tion and then com­put­ed its de­riv­a­tive was to show how easy it is to come up with a very com­pli­cat­ed func­tion that has an el­e­men­tary an­ti-deriva­tive.  There­fore, we see that the meth­ods from cal­cu­lus are not the ones to use if we want an in­te­gra­tion al­go­rithm that is com­plete.  The Risch In­te­gra­tion Al­go­rithm is based on a com­plete­ly dif­fer­ent ap­proach.  At its core lies Li­ou­ville's The­o­rem, which gives us the form of any el­e­men­tary an­ti-deriva­tive.   (I wish to point out at this point that heuris­tics like this are still use­ful in a com­put­er al­ge­bra sys­tem such as SymPy as fast pre­proces­sors to the full in­te­gra­tion al­go­rith­m).</p> 
 <p>The Risch Al­go­rithm works by do­ing poly­no­mi­al ma­nip­u­la­tions on the in­te­grand, which is en­tire­ly de­ter­min­is­tic (non-heuris­tic), and gives us the pow­er of all the the­o­rems of al­ge­bra, al­low­ing us to ac­tu­al­ly prove that an­ti-deriva­tives can­not ex­ist when they don't.  To start of­f, we have to look at deriva­tion­s.  As I said, ev­ery­thing with the Risch Al­go­rithm is looked at al­ge­bra­i­cal­ly (as op­posed to an­a­lyt­i­cal­ly).  The first thing to look at is the de­riv­a­tive it­self.  We de­fine a deriva­tion as any func­tion $la­tex D$ on a ring $la­tex R$ that sat­is­fies two prop­er­ties:</p> 
 <ol>
<li> 
 <p>$la­­tex D(a + b) = Da + Db$ (Sum Rule),</p>  
  </li> 
 <li> 
 <p>$la­­tex D(ab) = aDb + bDa$ (Prod­uct Rule)</p>  
  </li> 
 </ol>
<p>for any $la­tex a, b \in R$.  Fur­ther­more, de­fine the set of con­stant el­e­ments as $la­tex Con­st_D(R) = {a \in R\­tex­tr­m{ such that }Da = 0}$.  From just these two rules, you can prove all the rules from cal­cu­lus such as the pow­er rule and the quo­tient rule.  Defin­ing things al­ge­bra­i­cal­ly lets us avoid an­a­lyt­ic prob­lem­s, such as dis­con­ti­nu­ities and the need to prove con­ver­gence all the time.  An­oth­er prob­lem from anal­y­sis is the mul­ti­val­ue na­ture of cer­tain func­tion­s, name­ly the nat­u­ral log­a­rith­m.  We get around this by defin­ing $la­tex \log{a}$ as the unique func­tion sat­is­fy­ing $la­tex D\log{a} = \frac{­Da}{a}$, for $la­tex a \neq 0$.   From this def­i­ni­tion we can prove the fa­mous log­a­rith­mic iden­ti­ties $la­tex \log{ab} = \log{a} + \log{b}$ and $la­tex \log{a^n} = n\log{a}$ for log­a­rith­mic deriva­tives, again us­ing on­ly the two rules for a deriva­tion giv­en above.  For ex­am­ple, $la­tex D\log{ab}=\frac{D­ab}{ab}=\frac{aDb + bDa}{ab} = \frac{b­Da}{ab} + \frac{aD­b}{ab} = $$la­tex \frac{­Da}{a} + \frac{D­b}{b}=D\log{a} + D\log{b}=D(\log{a} + \log{b})$.   </p> 
 <p>The above def­i­ni­tion for the nat­u­ral log­a­rithm gives the first in­sight in­to how the in­te­gra­tion al­go­rithm work­s.  We de­fine tran­scen­den­tal func­tions in terms of their de­riv­a­tives.  So if $la­tex t = e^x$, then $la­tex Dt/t = 1$.  We can de­fine all of the trigono­met­ric func­tions in terms of $la­tex e^x$ and $la­tex \log{x}$ if we use $la­tex \sqrt{-1}$, but we can al­so avoid this.  For ex­am­ple, if $la­tex t = \tan{x}$, then $la­tex Dt = 1 + t^2$ be­cause $la­tex \frac{d}{dx}\­tan{x} = \sec^2{x} = 1 + \tan^2{x}$.   </p> 
 <p>We say that $la­tex t\in K$ is a  <em>mono­mi­al</em>  over the field $la­tex k$ with re­spect to a deriva­tion $la­tex D$ if it sat­is­fies</p> 
 <ol>
<li> 
 <p>$la­­tex t$ is tran­s­cen­­den­­tal over $la­­tex k$,</p>  
  </li> 
 <li> 
 <p>$la­­tex D[t]\in k[t]$.</p>  
  </li> 
 </ol>
<p>The first con­di­tion is nec­es­sary be­cause the we are on­ly go­ing to deal with the trance­nen­tal ver­sion of the Risch Al­go­rithm (the al­ge­bra­ic case is solved too, but the so­lu­tion method is quite dif­fer­en­t, and I am not im­ple­ment­ing it this sum­mer).  The sec­ond con­di­tion just says that the de­riv­a­tive of t is a poly­no­mi­al in t and a ra­tio­nal func­tion in x.  The func­tions I men­tioned above all sat­is­fy these prop­er­ties for $la­tex K = \math­b­b{Q}$.  The­o­rems in anal­y­sis show that $la­tex \log{x}$, $la­tex e^x$, and $la­tex \tan{x}$ are all tran­scen­den­tal over $la­tex \math­b­b{Q}[x]$.  This is ac­tu­al­ly the on­ly use of anal­y­sis that we make in the in­te­gra­tion al­go­rith­m.  Al­so, we see that if $la­tex t_1=\log{x}$, $la­tex t_2=e^x$, and $la­tex t_3=\­tan{x}$, then $la­tex Dt_1=\frac{1}{x}$, $la­tex Dt_2=t_2$, and $la­tex Dt_3=1 + t_3^2$, which are all poly­no­mi­als in their re­spec­tive $la­tex t_i$ and ra­tio­nal func­tions in $la­tex x$.  In the al­go­rith­m, $la­tex K$ is ac­tu­al­ly a tow­er of mono­mi­al ex­ten­sions of $la­tex \math­b­b{Q}$, so $la­tex t_n$ is a mono­mi­al over $la­tex \math­b­b{Q}(x, t_1, \dot­s, t_{n-1})$.   This al­lows us to work with func­tions like $la­tex e^{\­tan{x}}$.  We can't make $la­tex t=e^{\­tan{x}}$ di­rect­ly be­cause $la­tex \frac{d}{dx}e^{\­tan{x}} = (1 + \tan^2{x})e^{\­tan{x}}$ is not a poly­no­mi­al in $la­tex t$ (it al­so con­tains $la­tex \tan{x}$) .  But if we let $la­tex t_1$ be such that $la­tex Dt_1=1 + t_1^2$, i.e., $la­tex t_1=\­tan{x}$, then we can let $la­tex t_2$ be such that $la­tex Dt_2=(1 + t_1^2)t_2$, i.e., $la­tex t_2=e^{\­tan{x}}$.  Re­mem­ber that the $la­tex t_i$ are all "func­tion­s" of x, but there is no need to write $la­tex t=t(x)$ as long as we re­mem­ber that $la­tex Dt\neq 0$, i.e., $la­tex t\not \in Con­st_D(K)$.  This is an­oth­er ad­van­tage of us­ing al­ge­bra­ic over an­a­lyt­ic meth­od­s; it al­lows us to re­duce an in­te­gral down to a ra­tio­nal func­tion in the "sym­bol­s" $la­tex x$ and $la­tex t_1, t_2, \dot­s, t_n$.  By con­ven­tion, we make the first ex­ten­sion $la­tex t_0$ such that $la­tex Dt_0=1$, i.e., $la­tex t_0=x$.  I will just call it $la­tex x$ here in­stead of $la­tex t_0$, to avoid con­fu­sion.   </p> 
 <p>This is the prepars­ing that I al­lud­ed to in an  <a href="http://asmeurersympy.wordpress.com/2010/06/26/quick-update/">ear­li­er post</a>  that I have not im­ple­ment­ed yet.  The rea­son that I haven't im­ple­ment­ed it yet is not just be­cause I haven't got­ten around to it.  We have to be care­ful when we build up the ex­ten­sion that each el­e­ment is in­deed tran­scen­den­tal over the al­ready built-up field $la­tex k$.  For ex­am­ple, al­though it ap­pears tran­scen­den­tal, the func­tion $la­tex e^{\frac{1}{2}\log{(1 + x^2)}}$ is re­al­ly al­ge­bra­ic be­cause it equals $la­tex \sqrt{1 + x^2}$.  There are ad­di­tion­al re­quire­ments, such that each ex­ten­sion is not the de­riv­a­tive of log­a­rith­mic de­riv­a­tive of an el­e­ment of $la­tex k$ (see al­so the ex­am­ple I gave in the pre­vi­ous post).  This is the part that I was talk­ing about in my  <a href="http://asmeurersympy.wordpress.com/2010/06/26/quick-update/">pre­vi­ous post</a>  that is not writ­ten out as much as the oth­er al­go­rithms in Bron­stein's book.  So this is al­go­rith­mi­cal­ly solved, just like the rest of the Al­go­rith­m, but it is non-triv­ial and may end up be­ing the hard­est part of the al­go­rithm for me to im­ple­men­t, just be­cause it will prob­a­bly re­quire the most fig­ur­ing out on my part.   </p> 
 <p>So we can see that we can con­vert a tran­scen­den­tal in­te­gral, such as the one above, in­to a ra­tio­nal func­tion in x and mono­mi­al ex­ten­sions $la­tex t_1, t_2, \dot­s, t_n$.  For ex­am­ple, the above in­te­grand would be­come $la­tex - \frac{1 + t + t^{2} + 3 x^{2} + 6 t^{2} x^{2} + 7 t x^{2} + t^{3} + 2 x^{4}}{t^{2} x^{2} + 4 t x^{4} + 4 x^{6}}$.  We then per­form cer­tain poly­no­mi­al ma­nip­u­la­tions on this in­te­grand, us­ing the fact that $la­tex Dx=1$ and $la­tex Dt=\frac{1}{x}$.  For the tran­scen­den­tal case of the Risch Al­go­rith­m, this is sim­i­lar to the ra­tio­nal func­tion in­te­gra­tion that I out­lined in  <a href="http://asmeurersympy.wordpress.com/2010/06/11/integration-of-rational-functions/">this post</a>, and has Li­ou­ville's The­o­rem at its core.  This is where I will start off next time.   </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/06/30/the-risch-algorithm-part-1/#disqus_thread" data-disqus-identifier="cache/posts/2010/06/30/the-risch-algorithm-part-1.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/06/26/quick-update/" class="u-url">Quick Update</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-06-26T03:16:52-05:00">2010-06-26 03:16</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>I've spend most of this week sit­ting in a car, so while I have been able to do some work, I haven't had much time to write up a blog post.  So, to com­ply with  <a href="http://groups.google.com/group/sympy/browse_thread/thread/7d7dceb34db45302">On­drej's rule</a>, here is a quick up­date.</p> 
 <p>I have been work­ing my way through Bron­stein's book.  I fin­ished the out­er al­go­rith­mic lay­er of the im­plan­ta­tion.  Ba­si­cal­ly, the al­go­rithm does poly­no­mi­als ma­nip­u­la­tion on the in­te­grand.  It first re­duces the in­te­grand in­to small­er in­te­gral­s, un­til it gets to an in­te­gral where a sub­prob­lem must be solved to solve it.  The sub­prob­lem that must be solved dif­fers de­pend­ing on the type of the in­te­gral.  The first one that comes up in Bron­stein's text is the Risch Dif­fer­en­tial Equa­tion, which aris­es from the in­te­gra­tion of ex­po­nen­tial func­tion­s.  (I will ex­plain all of these thing in more de­tail in a fu­ture blog post).  At this point, the al­go­rithms be­gin to re­cur­sive­ly de­pend on each oth­er, re­quir­ing me to im­ple­ment more and more al­go­rithms at a time in or­der for each to work.  To make things worse, a very fun­da­men­tal set of al­go­rithms are on­ly de­scribed in the tex­t, not giv­en in pseu­do-­code, so I have had to fig­ure those things out.   These are al­go­rithms to de­ter­mine if a dif­fer­en­tial ex­ten­sion is a de­riv­a­tive or log­a­rith­mic de­riv­a­tive of el­e­ments that have al­ready been ex­tend­ed.  Again, I will ex­plain bet­ter in a fu­ture post, but the idea is that you re­place el­e­ments in an in­te­grand with dum­my vari­ables, but each el­e­ment has to be tran­scen­den­tal over the pre­vi­ous el­e­ments.  So if you have $la­tex \int (e^x + e^{x^2} + e^{x + x*^2})dx$, and you set $la­tex t_1 = e^x$ and $la­tex t_2 = e^{x^2}$ ($la­tex Dt_1 = t_1$ and $la­tex Dt_2 = 2x­t_2$), then you can­not make $la­tex t_3 = e^{x + x^2}$ be­cause $la­tex e^{x + x^2} = t_1t_2$.  The abil­i­ty to de­ter­mine if an el­e­ment is a de­riv­a­tive or a log­a­rith­mic de­riv­a­tive of an el­e­ment of the al­ready build dif­fer­en­tial ex­ten­sion is im­por­tant not on­ly for build­ing up the ex­ten­sion for the in­te­grand (ba­si­cal­ly the prepars­ing), but al­so for solv­ing some of the cas­es of the sub­prob­lems such as the Risch Dif­fer­en­tial Equa­tion prob­lem.</p> 
 <p>So I am still fig­ur­ing out some of the de­tails on that one.  The de­scrip­tion in the book is pret­ty good (this is prob­a­bly the best writ­ten math text­book I have ev­er seen), but I still have had to fig­ure out some of the math­e­mat­i­cal de­tails on pa­per (which is some­thing I en­joy any­way, but it can be more stress­ful).  Hope­ful­ly by the next time I can have some code that is work­ing enough to ac­tu­al­ly demon­strate solv­ing some com­plex in­te­gral­s, (with man­u­al prepars­ing), and even more ex­cit­ing­ly, prove that some non-ele­men­tary in­te­gral­s, such as the clas­sic $la­tex \int e^{-x^2}dx$, are in­deed so.   And I al­so hope to have some more ex­pla­na­tions on how the Risch al­go­rithm works in fu­ture post­s.   </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/06/26/quick-update/#disqus_thread" data-disqus-identifier="cache/posts/2010/06/26/quick-update.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/06/16/strange-python-behavior-can-someone-please-explain-to-me-what-is-going-on-here/" class="u-url">Strange Python Behavior (can someone please explain to me what is going on here?)</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-06-16T04:49:06-05:00">2010-06-16 04:49</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><a href="http://asmeurersympy.wordpress.com/2009/07/20/modifying-a-list-while-looping-through-it-in-python/">Every once in a while</a>, seemingly really simple Python code does something completely unexpected for me. Look at the following snippet of Python code.  This is run straight from the 2.6.5 interpreter, with no other commands executed.  Do you notice anything strange?
<p>[code lan­guage="py"]</p> 
 <p>$python</p> 
 <p>Python 2.6.5 (r265:79359, Mar 24 2010, 01:32:55)  </p> 
 <p>[GCC 4.0.1 (Ap­ple In­c. build 5493)] on dar­win</p> 
 <p>Type "help", "copy­right", "cred­it­s" or "li­cense" for more in­for­ma­tion.</p> 
 <p>&gt;&gt;&gt;; l = lamb­da i: a[i]</p> 
 <p>&gt;&gt;&gt; l</p> 
 <p>&lt;func­tion at="" 0x39e7f0=""&gt;</p> 
 <p>&gt;&gt;&gt; H = [(1, 2), (3, 4)]</p> 
 <p>&gt;&gt;&gt; [l(0) + l(1) for a in H]</p> 
 <p>[3, 7]</p> 
 <p>[/­code]</p> 
 <p>Did you spot it?  Here is a hin­t. Run­ning a dif­fer­ent but sim­i­lar ses­sion:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>$python</p> 
 <p>Python 2.6.5 (r265:79359, Mar 24 2010, 01:32:55)  </p> 
 <p>[GCC 4.0.1 (Ap­ple In­c. build 5493)] on dar­win</p> 
 <p>Type "help", "copy­right", "cred­it­s" or "li­cense" for more in­for­ma­tion.</p> 
 <p>&gt;&gt;&gt; l = lamb­da i: a[i]</p> 
 <p>&gt;&gt;&gt; l</p> 
 <p>&lt;func­tion at="" 0x39e7f0=""&gt;</p> 
 <p>&gt;&gt;&gt; l(0)</p> 
 <p>Trace­back (most re­cent call last):
  File "", line 1, in 
  File "", line 1, in 
NameEr­ror: glob­al name 'a' is not de­fined</p> 
 <p>[/­code]</p> 
 <p>Do you see it now?  I de­fined the lamb­da func­tion  <code>l</code>  in terms of  <code>a</code>  with­out defin­ing first defin­ing  <code>a</code>!  And fur­ther­more, it just works when  <code>a</code>  is de­fined.  This is ac­tu­al­ly in­de­pen­dent of the fact that we are work­ing in a list com­pre­hen­sion, as this con­tin­u­a­tion of the pre­vi­ous ses­sion shows:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>&gt;&gt;&gt; a = [3, 4, 5]</p> 
 <p>&gt;&gt;&gt; l(0)</p> 
 <p>3</p> 
 <p>[/­code]</p> 
 <p>But I want to ex­pand on the list com­pre­hen­sion ex­am­ple, be­cause there even more biz­zare things go­ing on here.  Restart­ing a new ses­sion again:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>$python</p> 
 <p>Python 2.6.5 (r265:79359, Mar 24 2010, 01:32:55)  </p> 
 <p>[GCC 4.0.1 (Ap­ple In­c. build 5493)] on dar­win</p> 
 <p>Type "help", "copy­right", "cred­it­s" or "li­cense" for more in­for­ma­tion.</p> 
 <p>&gt;&gt;&gt; l = lamb­da i: a[i]</p> 
 <p>&gt;&gt;&gt; H = [(1, 2), (3, 4)]</p> 
 <p>&gt;&gt;&gt; [l(0) + l(1) for a in H]</p> 
 <p>[3, 7]</p> 
 <p>&gt;&gt;&gt; (l(0) + l(1) for a in H)</p> 
 <p>&lt;gen­er­a­tor ob­jec­t="" at="" 0x3a4350=""&gt;</p> 
 <p>&gt;&gt;&gt; list((l(0) + l(1) for a in H))</p> 
 <p>[7, 7]</p> 
 <p>[/­code]</p> 
 <p>So, if you are as­tute and have been us­ing Python for long enough, you should be able to catch what is go­ing on here.  If you don't know, here is a hint (con­tin­u­a­tion of pre­vi­ous ses­sion):</p> 
 <p>[code lan­guage="py"]</p> 
 <p>&gt;&gt;&gt; a</p> 
 <p>(3, 4)</p> 
 <p>[/­code]</p> 
 <p>So, as you may know, in Python 2.6 and ear­lier, list com­pre­hen­sion in­dex vari­ables "leek" in­to the lo­cal names­pace.  The strange thing here is that al­though the list com­pre­hen­sion would re­set it, the gen­er­a­tor ver­sion does not.  Well, nor­mal­ly, it does do this:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>&gt;&gt;&gt; x = 1</p> 
 <p>&gt;&gt;&gt; [x for x in range(10)]</p> 
 <p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p> 
 <p>&gt;&gt;&gt; x</p> 
 <p>9</p> 
 <p>&gt;&gt;&gt; del x</p> 
 <p>&gt;&gt;&gt; list((x for x in range(10)))</p> 
 <p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p> 
 <p>&gt;&gt;&gt; x</p> 
 <p>Trace­back (most re­cent call last):
  File "", line 1, in 
NameEr­ror: name 'x' is not de­fined</p> 
 <p>&gt;&gt;&gt; x = 1</p> 
 <p>&gt;&gt;&gt; list((x for x in range(10)))</p> 
 <p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p> 
 <p>&gt;&gt;&gt; x</p> 
 <p>1</p> 
 <p>[/­code]</p> 
 <p>So the above bit has some­thing to do with the way the  <code>lamb­da</code>  func­tion was de­fined with the  <code>a</code>.  By the way, here is what hap­pens with the gen­er­a­tor com­pre­hen­sion (is that what these are called?) if  <code>a</code>  is not de­fined:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>&gt;&gt;&gt; del a</p> 
 <p>&gt;&gt;&gt; list((l(0) + l(1) for a in H))</p> 
 <p>Trace­back (most re­cent call last):
  File "", line 1, in 
  File "", line 1, in 
  File "", line 1, in 
NameEr­ror: glob­al name 'a' is not de­fined</p> 
 <p>[/­code]</p> 
 <p>This is how I dis­cov­ered this.  I had de­fined a lamb­da func­tion us­ing an vari­able that was then passed to a list com­pre­hen­sion that used this vari­able as the in­dex with­out re­al­iz­ing it. But then I tried con­vert­ing this in­to a gen­er­a­tor com­pre­hen­sion to see if it would be faster, and got the above er­ror.   </p> 
 <p>Fi­nal­ly, since the "fea­ture" of leak­ing list com­pre­hen­sion loop vari­ables in­to the lo­cal names­pace is  <a href="http://docs.python.org/release/3.0.1/whatsnew/3.0.html#changed-syntax">go­ing away</a>  in Python 3, I ex­pect­ed things to be­have at least a lit­tle dif­fer­ent­ly in Python 3.  I tried the above in a Python 3.1.2 in­ter­preter and got the fol­low­ing:</p> 
 <p>[code lan­guage="py"]</p> 
 <p>$python3</p> 
 <p>Python 3.1.2 (r312:79147, Mar 23 2010, 22:02:05)  </p> 
 <p>[GCC 4.2.1 (Ap­ple In­c. build 5646) (dot 1)] on dar­win</p> 
 <p>Type "help", "copy­right", "cred­it­s" or "li­cense" for more in­for­ma­tion.</p> 
 <p>&gt;&gt;&gt; l = lamb­da i: a[i]</p> 
 <p>&gt;&gt;&gt; l</p> 
 <p>&lt;func­tion at="" 0x100585a68=""&gt;</p> 
 <p>&gt;&gt;&gt; H = [(1, 2), (3, 4)]</p> 
 <p>&gt;&gt;&gt; [l(0) + l(1) for a in H]</p> 
 <p>Trace­back (most re­cent call last):
  File "", line 1, in 
  File "", line 1, in 
  File "", line 1, in 
NameEr­ror: glob­al name 'a' is not de­fined</p> 
 <p>&gt;&gt;&gt; list((l(0) + l(1) for a in H))</p> 
 <p>Trace­back (most re­cent call last):
  File "", line 1, in 
  File "", line 1, in 
  File "", line 1, in 
NameEr­ror: glob­al name 'a' is not de­fined</p> 
 <p>[/­code]</p> 
 <p>So in Python 3, both the list com­pre­hen­sion and the gen­er­a­tor com­pre­hen­sions act the same, which is not too sur­pris­ing.  I guess I should re­code that piece of code to make it fu­ture proof, al­though this does­n't seem easy at the mo­men­t, and it may re­quire con­vert­ing a one-­lin­er in­to a six-­lin­er.  If you are in­ter­est­ed, the piece of code is  <a href="http://github.com/asmeurer/sympy/blob/15c3675ff67be854c12c349ed9034f12bb2f5247/sympy/integrals/risch.py#L297">here</a>.</p> 
 <p>So can any­one pro­vide any in­sight in­to what is go­ing on with that lamb­da func­tion?  Run­ning it with the  <code>-3</code>  switch to  <code>python2.6</code>  did­n't give any warn­ings re­lat­ed to it.   </p> 
 <p><strong>Up­date:</strong>  As I not­ed in a  <a href="http://asmeurersympy.wordpress.com/2010/06/16/strange-python-behavior-can-someone-please-explain-to-me-what-is-going-on-here/#comment-121">com­ment</a>, I fig­ured out how to make this fu­ture-proof.  I need to con­vert it from  </p> 
 <p>[code lan­guage="py"]</p> 
 <p>def residue_re­duce_deriva­tion(H, D, x, t, z):
    lambda­func = lamb­da i: i*deriva­tion(a[1], D, x, t).as_ba­sic().­sub­s(z, i)/ \
         a[1].as_ba­sic().­sub­s(z, i)
    re­turn S(­sum([­Root­Sum(a[0].as_poly(z), lambda­func) for a in H]))
[/­code]</p> 
 <p>to</p> 
 <p>[code lan­guage="py"]</p> 
 <p>def residue_re­duce_deriva­tion(H, D, x, t, z):
    re­turn S(­sum((­Root­Sum(a[0].as_poly(z), lamb­da i: i*deriva­tion(a[1], D, x, t).as_ba­sic().­sub­s(z, i)/ \
        a[1].as_ba­sic().­sub­s(z, i)) for a in H)))
[/­code]</p> 
 <p>Thanks to all the com­menters for the ex­pla­na­tion­s.   </p> 
 <p>Al­so, you may have no­ticed that I dis­cov­ered that if you use  <tt>[code]</tt>  in­stead of  <tt>&lt;code&gt;</tt>, you get these nicer code blocks that  <em>ac­tu­al­ly re­spect in­den­ta­tion!</em>   Now I just need to fig­ure out how to make them syn­tax high­light Python code.</p> 
 <p><strong>Up­date 2:</strong>   <tt>[code='py']</tt>  col­ors it!  Sweet!</p> 
 <p><strong>Up­date 3:</strong>  I just dis­cov­ered that SymPy has a  <code>Lamb­da()</code>  ob­ject that han­dles this bet­ter.  In par­tic­u­lar, it pret­ty prints the code, and is what is al­ready be­ing used for  <code>Root­Sum()</code>  in the ra­tio­nal func­tion in­te­gra­tor, at least in Ma­teusz's polys9.   </p> 
 <p>[code lan­guage="py"]</p> 
 <p>&gt;&gt;&gt; in­te­grate(1/(x**5 + 1), x)</p> 
 <p>log(1 + x)/5 + Root­Sum(625<em>_t<strong>4 + 125*_t</strong>3 + 25</em>_t<em><em>2 + 5</em>_t + 1, Lamb­da(_t, _t</em>log(x + 5*_t)))                                                    </p> 
 <p>[/­code]</p> 
 <p>Stil­l, this has been a very good learn­ing ex­pe­ri­ence.   </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/06/16/strange-python-behavior-can-someone-please-explain-to-me-what-is-going-on-here/#disqus_thread" data-disqus-identifier="cache/posts/2010/06/16/strange-python-behavior-can-someone-please-explain-to-me-what-is-going-on-here.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/06/15/a-weeklog/" class="u-url">A Weeklog</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-06-15T21:45:22-05:00">2010-06-15 21:45</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><a href="http://ondrejcertik.blogspot.com/2010/06/week-may-30-june-4.html">These</a> <a href="http://ondrejcertik.blogspot.com/2010/06/week-june-5-june-11.html">seem</a> <a href="http://haz-tech.blogspot.com/2010/06/plowing-forward.html">to</a> <a href="http://ojensen.wordpress.com/2010/06/15/array-arguments/">be</a> all the rave these days, so I figured, why not jump on the bandwagon:
<p><code></code></p> 
 <p>Aaron-Meur­er:­doc aaron­meur­er20100615153531(in­te­gra­tion$)$git weekre­port  </p> 
 <p>Aaron Meur­er (20):
      Fix some bugs in Poly
      Make Poly(s­in(x)/x*t, t, do­main='EX').­clear_­de­nom­s() work
      Fix in­te­grate to work cor­rect­ly with heurisch.py
      Use more ef­fi­cient gcdex­dio­phan­tine() al­go­rith­m
      Add sup­port for tak­ing the deriva­tion over the co­ef­fi­cient do­main in risch.py
      Add (but do not yet use) split­fac­tor_sqf() in risch.py
      Add poly­no­mi­al_re­duce() to risch.py
      Add tests for al­go­rithms in risch.py in a new test_risch.py file
      On­ly al­low co­er­cion to larg­er do­main­s
      Al­low co­er­cion from ZZ(a) to ZZ(a, b)
      Fix doctest in new heurisch.py file
      Add residue_re­duce()
      For­mat­ting fix­es in doc­strings in sympy/polys/al­ge­bra­tool­s.py
      Add in­clude­PRS op­tion to re­sul­tant func­tion­s
      Add per­mute method to DM­P
      Add a test for the in­clude­PRS op­tion of re­sul­tan­t()
      Have residue_re­duce() make S_i mon­ic
      Re­write poly­no­mi­al_re­duce() non-re­cur­sive­ly
      Add in­te­grate_hy­per­tan­gen­t_poly­no­mi­al()
      Add in­te­grate_non­lin­ear_no_spe­cial­s()
 </p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/06/15/a-weeklog/#disqus_thread" data-disqus-identifier="cache/posts/2010/06/15/a-weeklog.html">Comments</a>


        </p></article>
        <article class="postbox h-entry post-text">
        <h1 class="p-name"><a href="posts/2010/06/11/integration-of-rational-functions/" class="u-url">Integration of rational functions</a>
        <small>  
             Posted: <time class="published dt-published" datetime="2010-06-11T19:39:58-05:00">2010-06-11 19:39</time>
        </small></h1>
        <hr>
        <div class="e-content">
        <div><p></p><p>So for this week's blog post I will try to ex­plain how the gen­er­al al­go­rithm for in­te­grat­ing ra­tio­nal func­tions work­s.  Re­call that a  <a href="http://en.wikipedia.org/wiki/Rational_function">ra­tio­nal func­tion</a>  is the quo­tient of two poly­no­mi­al­s.  We know that us­ing com­mon de­nom­i­na­tors, we can con­vert the sum of any num­ber of ra­tio­nal func­tions in­to a sin­gle quo­tien­t, $la­tex \frac{a_nx^n + a_{n-1}x^{n-1} + \c­dots + a_2x^2 + a_1x + a_0}{b_nx^n + b_{n-1}x^{n-1} + \c­dots + b_2x^2 + a_1x + a_0}$.  Al­so, us­ing  <a href="http://en.wikipedia.org/wiki/Polynomial_division">poly­no­mi­al di­vi­sion</a>  we can re­write any ra­tio­nal func­tion as the sum of a poly­no­mi­al and the quo­tient of two poly­no­mi­als such that the de­gree of the nu­mer­a­tor is less than the de­gree of the de­nom­i­na­tor ($la­tex F(x) = \frac{b(x)}{c(x)} = p(x) + \frac{r(x)}{g(x)}$, with $la­tex deg(r) &lt; deg(g)$).  Fur­ther­more, we know that the rep­re­sen­ta­tion of a ra­tio­nal func­tion is not unique.  For ex­am­ple, $la­tex \frac{(x + 1)(x - 1)}{(x + 2)(x - 1)}$ is the same as $la­tex \frac{x + 1}{x + 2}$ ex­cept at the point $la­tex x = 1$, and $la­tex \frac{(x - 1)^2}{x - 1}$ is the same as $la­tex x - 1$ ev­ery­where.  But by us­ing  <a href="http://en.wikipedia.org/wiki/Euclid%27s_algorithm_for_polynomials#Polynomials">Eu­clid's al­go­rithm</a>  for find­ing the GCD of poly­no­mi­als on the nu­mer­a­tor and the de­nom­i­na­tor, along with poly­no­mi­al di­vi­sion on each,  we can can­cel all com­mon fac­tors to get a rep­re­sen­ta­tion that is unique (as­sum­ing we ex­pand all fac­tors in­to one poly­no­mi­al).  Fi­nal­ly, us­ing poly­no­mi­al di­vi­sion with re­main­der, we can re­write any ra­tio­nal func­tion $la­tex F(x)$ as $la­tex \frac{a(x)}{b(x)} = p(x) + \frac{a(x)}{d(x)}$, where $la­tex a(x)$, $la­tex b(x)$, $la­tex c(x)$, $la­tex d(x)$, and $la­tex p(x)$ are all poly­no­mi­al­s, and the de­gree of $la­tex a$ is less than the de­gree of $la­tex d$.   </p> 
 <p>We know from cal­cu­lus that the in­te­gral of any ra­tio­nal func­tion con­sists of three part­s: the poly­no­mi­al part, the ra­tio­nal part, and the log­a­rith­mic part (con­sid­er arc­tan­gents as com­plex log­a­rithm­s).  The poly­no­mi­al part is just the in­te­gral of $la­tex p(x)$ above.  The ra­tio­nal part is an­oth­er ra­tio­nal func­tion, and the log­a­rith­mic part, which is a sum of log­a­rithms of the form $la­tex a\log{s(x)}$, where $la­tex a$ is an al­ge­bra­ic con­stant and $la­tex s(x)$ is a poly­no­mi­al (note that if $la­tex s(x)$ is a ra­tio­nal func­tion, we can split it in­to two log­a­rithms of poly­no­mi­als us­ing the log iden­ti­ties).   </p> 
 <p>To find the ra­tio­nal part, we first need to know about square-free fac­tor­iza­tion­s.  An im­por­tant re­sult in al­ge­bra is that any poly­no­mi­al with ra­tio­nal co­ef­fi­cients can be fac­tored unique­ly in­to ir­re­duc­ible poly­no­mi­als with ra­tio­nal co­ef­fi­cients, up to mul­ti­pli­ca­tion of a non-ze­ro con­stant and re­order­ing of fac­tors, sim­i­lar to how any in­te­ger can be fac­tored unique­ly in­to primes up to mul­ti­pli­ca­tion of 1 and -1 and re­order­ing of fac­tors (tech­ni­cal­ly, it is with co­ef­fi­cients from a unique fac­tor­iza­tion do­main, for which the ra­tio­nals is a spe­cial case, and up to mul­ti­pli­ca­tion of a unit, which for ra­tio­nals is ev­ery non-ze­ro con­stan­t).  A poly­no­mi­al is square-free if this unique fac­tor­iza­tion does not have any poly­no­mi­als with pow­ers greater than 1.  An­oth­er the­o­rem from al­ge­bra tells us that ir­re­duc­ible poly­no­mi­als over the ra­tio­nals do not have any re­peat­ed root­s, and so giv­en this, it is not hard to see that a poly­no­mi­al be­ing square-free is equiv­a­lent to it not hav­ing re­peat­ed root­s.   </p> 
 <p>A  <a href="http://en.wikipedia.org/wiki/Square-free_factorization">square-free fac­tor­iza­tion</a>  of a poly­no­mi­al is a list of poly­no­mi­al­s, $la­tex P_1P_2^2 \c­dots P_n^n$, where each $la­tex P_i$ is square-free (in oth­er word­s, $la­tex P_1$ is the prod­uct of all the fac­tors of de­gree 1, $la­tex P_2$ is the prod­uct of all the fac­tors of de­gree 2, and so on).  There is a rel­a­tive­ly sim­ple al­go­rithm to com­pute the square-free fac­tor­iza­tion of a poly­no­mi­al, which is based on the fact that $la­tex gcd(P, \frac{d­p}{dx})$ re­duces the pow­er of each ir­re­duc­ible fac­tor by 1.  For ex­am­ple:</p> 
 <p><a href="2010/06/gcd.png"><img src="2010/06/gcd.png" alt="" title="gcd" width="246" height="175" class="alignnone size-full wp-image-420"></a></p> 
 <p>(Sor­ry for the pic­ture.   Word­Press code blocks do not work)</p> 
 <p>It is not too hard to prove this us­ing the prod­uct rule on the fac­tor­iza­tion of P.  So you can see that by com­put­ing $la­tex \frac{P}{gcd(P, \frac{d­P}{dx})}$, you can ob­tain $la­tex P_1P_2\c­dots P_n$.  Then, by re­cur­sive­ly com­put­ing $la­tex A_0 = P$, $la­tex A_1 = gcd(A_0, \frac{­dA_0}{dx})$, $la­tex A2 = gcd(A_1, \frac{­dA_1}{dx})$, … and tak­ing the quo­tient each time as above, we can find the square-free fac­tors of P.   </p> 
 <p>OK, so we know from par­tial frac­tion de­com­po­si­tions we learned in cal­cu­lus that if we have a ra­tio­nal func­tion of the form $la­tex \frac{Q(x)}{V(x)^n}$ , where $la­tex V(x)$ is square-free, the in­te­gral will be a ra­tio­nal func­tion if $la­tex n &gt; 1$ and a log­a­rithm if $la­tex n = 1$.  We can use the par­tial frac­tion de­com­po­si­tion that is easy to find once we have the square-free fac­tor­iza­tion of the de­nom­i­na­tor to re­write the re­main­ing ra­tio­nal func­tion as a sum of terms of the form $la­tex \frac{Q}{V_k^k}$, where $la­tex V_i$ is square-free.  Be­cause $la­tex V$ is square-free, $la­tex gcd(V, V')=1$, so the  <a href="http://en.wikipedia.org/wiki/Extended_Euclidean_algorithm">Ex­tend­ed Eu­clidean Al­go­rithm</a>  gives us $la­tex B_0$ and $la­tex C_0$ such that $la­tex B_0V + C_0V'=1$ (re­call that $la­tex g$ is the gcd of $la­tex p$ and $la­tex q$ if and on­ly if there ex­ist $la­tex a$ and $la­tex b$ rel­a­tive­ly prime to $la­tex g$ such that $la­tex ap+bq=g$.  This holds true for in­te­gers as well as poly­no­mi­al­s). Thus we can find $la­tex B$ and $la­tex C$ such that $la­tex BV + CV'= \frac{Q}{1-k}$.  Mul­ti­ply­ing through by $la­tex \frac{1-k}{V^k}$, $la­tex \frac{Q}{V^k}=-\frac{(k-1)B­V'}{V^k} + \frac{(1-k)C}{V^{k-1}}$, which is equal to $la­tex \frac{Q}{V^k} = (\frac{B'}{V^{k-1}} - \frac{(k-1)B­V'}{V^k}) + \frac{(1-k)C-B'}{V^{k-1}}$.  You may no­tice that the term in the paren­the­sis is just the de­riv­a­tive of  $la­tex \frac{B}{V^{k-1}}$, so we get $la­tex \in­t\frac{Q}{V^k}=\frac{B}{V^{k-1}} + \in­t\frac{(1-k)C - B'}{V^{k-1}}$.  This is called Her­mite Re­duc­tion.  We can re­cur­sive­ly re­duce the in­te­gral on the right hand side un­til the $la­tex k=1$. Note that there are more ef­fi­cient ways of do­ing this that do not ac­tu­al­ly re­quire us to com­pute the par­tial frac­tion de­com­po­si­tion, and there is al­so a lin­ear ver­sion due to Mack (this one is quadrat­ic), and an even more ef­fi­cient al­go­rithm called the Horow­itz-Ostro­grad­sky Al­go­rith­m, that does­n't even re­quire a square-free de­com­po­si­tion.   </p> 
 <p>So when we have fin­ished the Her­mite Re­duc­tion, we are left with in­te­grat­ing ra­tio­nal func­tions with pure­ly square-free de­nom­i­na­tors.  We know from cal­cu­lus that these will have log­a­rith­mic in­te­gral­s, so this is the log­a­rith­mic part.   </p> 
 <p>First, we need to look at re­sul­tants and PRSs. The  <a href="http://en.wikipedia.org/wiki/Resultant">re­sul­tant</a>  of two poly­no­mi­als is de­fined as dif­fer­ences of the roots of the two poly­no­mi­al­s, i.e., $la­tex re­sul­tan­t(A, B) = \prod_{i=1}^n\prod_{j=1}^m (\al­pha_i - \be­ta_j)$, where $la­tex A = (x - \al­pha_1)\c­dot­s(x - \al­pha_n)$ and $la­tex B = (x - \be­ta_1)\c­dot­s(x - \be­ta_m)$ are mon­ic poly­no­mi­als split in­to lin­ear fac­tors.  Clear­ly, the re­sul­tant of two poly­no­mi­als is 0 if and on­ly if the two poly­no­mi­als share a root. It is an im­por­tant re­sult that the re­sul­tant of two poly­no­mi­als can be com­put­ed from on­ly their co­ef­fi­cients by tak­ing the de­ter­mi­nant of the  <a href="http://en.wikipedia.org/wiki/Sylvester_matrix">Sylvester Ma­trix</a>  of the two poly­no­mi­al­s.  How­ev­er, it is more ef­fi­cient­ly cal­cu­lat­ed us­ing a poly­no­mi­al re­main­der se­quence  (PRS) (sor­ry, there does­n't seem to be a Wikipedia ar­ti­cle), which in ad­di­tion to giv­ing the re­sul­tant of A and B, al­so gives a se­quence of poly­no­mi­als with some use­ful prop­er­ties that I will dis­cuss be­low.  A poly­no­mi­al re­main­der se­quence is a gen­er­al­iza­tion of the Eu­clid­i­an al­go­rithm where in each step, the re­main­der $la­tex R_i$ is mul­ti­plied by a con­stant $la­tex \be­ta_i$.  The Fun­da­men­tal PRS The­o­rem shows how to com­pute spe­cif­ic $la­tex \be­ta_i$ such that the re­sul­tant can be cal­cu­lat­ed from the poly­no­mi­als in the se­quence.  </p> 
 <p>Then, if we have $la­tex \frac{A}{D}$, left over from the Her­mite Re­duc­tion (so $la­tex D$ square-free), let $la­tex R=re­sul­tan­t_t(A-t\frac{d­D}{dx}, D)$, where $la­tex t$ is a new vari­able, and $la­tex \al­pha_i$ be the dis­tinct roots of R.  Let $la­tex p_i=\gcd(A - \al­pha_i\frac{d­D}{dx}, D)$.  Then it turns out that the log­a­rith­mic part of the in­te­gral is just $la­tex \al­pha_1\log{p_1} + \al­pha_2\log{p_2} + \c­dots \al­pha_n\log{p_n}$.  This is called the Roth­stein-­Trager Al­go­rith­m.</p> 
 <p>How­ev­er, this re­quires find­ing the prime fac­tor­iza­tion of the re­sul­tan­t, which can be avoid­ed if a more ef­fi­cient al­go­rithm called the Lazard-Ri­o­boo-­Trager Al­go­rithm is used. I will talk a lit­tle bit about it.  It works by us­ing sub­re­sul­tant poly­no­mi­al re­minder se­quences.  </p> 
 <p>It turns out that the above $la­tex gcd(A-\al­pha\frac{d­D}{dx}, D)$ will ap­pear in the PRS of $la­tex D$ and $la­tex A-t\frac{d­D}{dx}$.  Fur­ther­more, we can use the PRS to im­me­di­ate­ly find the re­sul­tant $la­tex R=re­sul­tan­t_t(A-t\frac{d­D}{dx}, D)$, which as we saw, is all we need to com­pute the log­a­rith­mic part.   </p> 
 <p>So that's ra­tio­nal in­te­gra­tion.  I hope I haven't bored you too much, and that this made at least a lit­tle sense.  I al­so hope that it was all cor­rec­t.  Note that this en­tire al­go­rithm has al­ready been im­ple­ment­ed in SymPy, so if you plug a ra­tio­nal func­tion in to  <code>in­te­grate()</code>, you should get back a so­lu­tion.  How­ev­er, I de­scribe it here be­cause the tran­scen­den­tal case of the Risch Al­go­rithm is just a gen­er­al­iza­tion of ra­tio­nal func­tion in­te­gra­tion.</p> 
 <p>As for work up­dates, I found that the Poly ver­sion of the heur­sitic Risch al­go­rithm was con­sid­er­ably slow­er than the orig­i­nal ver­sion, due to in­ef­fi­cien­cies in the way the poly­no­mi­als are cur­rent­ly rep­re­sent­ed in SymPy.  So I have put that aside, and I have start­ed im­ple­ment­ing al­go­rithms from the full al­go­rith­m.  There's not much to say on that fron­t.  It's te­dious work.  I copy the al­go­rithm from Bron­stein's book, then try make sure that it is cor­rect based on the few ex­am­ples giv­en and from the math­e­mat­i­cal back­ground given, and when I'm sat­is­fied, I move on to the next one.  Fol­low my  <a href="http://github.com/asmeurer/sympy/tree/integration">in­te­gra­tion</a>  branch if you are in­ter­est­ed.</p> 
 <p>In my next post, I'll try to de­fine some terms, like "ele­men­tary func­tion," and in­tro­duce a lit­tle dif­fer­en­tial al­ge­bra, so you can un­der­stand a lit­tle bit of the na­ture of the gen­er­al in­te­gra­tion al­go­rith­m.</p></div>
        </div>
            
        
    <p>
        <a href="posts/2010/06/11/integration-of-rational-functions/#disqus_thread" data-disqus-identifier="cache/posts/2010/06/11/integration-of-rational-functions.html">Comments</a>


        </p></article>
    
        <div>
        <ul class="pager">
            <li class="previous">
                <a href="index-5.html" rel="prev">← Newer posts</a>
            </li>
            <li class="next">
                <a href="index-3.html" rel="next">Older posts →</a>
            </li>
        </ul>
        </div>

    
        
       <script>var disqus_shortname="asmeurer";(function(){var a=document.createElement("script");a.async=true;a.src="//"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>


	
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}
        });
        </script>
        <script src="assets/js/mathjax.js"></script>


        </div>
        <!--End of body content-->

        <footer>
            Contents © 2014         <a href="mailto:asmeurer@gmail.com">Aaron Meurer</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         
<p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
  <a rel="license" href="http://creativecommons.org/publicdomain/zero/1.0/">
    <img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0">
  </a>

        </p></footer>
    </div>
</div>


            <script src="assets/js/all-nocdn.js" type="text/javascript"></script>


    
<!-- Social buttons -->
<div id="addthisbox" class="addthis_toolbox addthis_peekaboo_style addthis_default_style addthis_label_style addthis_32x32_style">
<a class="addthis_button_more">Share</a>
<ul><li><a class="addthis_button_facebook"></a>
</li><li><a class="addthis_button_google_plusone_share"></a>
</li><li><a class="addthis_button_linkedin"></a>
</li><li><a class="addthis_button_twitter"></a>
</li></ul>
</div>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4f7088a56bb93798"></script>
<!-- End of social buttons -->


    <script type="text/javascript">jQuery("a.image-reference").colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script>
    

<script type="text/javascript" src="assets/js/tipuesearch_set.js"></script>
<script type="text/javascript" src="assets/js/tipuesearch.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('#tipue_search_input').tipuesearch({
        'mode': 'json',
        'contentLocation': '/assets/js/tipuesearch_content.json',
        'showUrl': false
    });
});
</script>


</body></html>